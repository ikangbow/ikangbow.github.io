<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CTP的OrderRef/OrderActionRef字段规则]]></title>
    <url>%2F2022%2F01%2F22%2FCTP%E7%9A%84OrderRef-OrderActionRef%E5%AD%97%E6%AE%B5%E8%A7%84%E5%88%99%2F</url>
    <content type="text"><![CDATA[记录学习CTP的OrderRef/OrderActionRef字段规则，原文参考https://zhuanlan.zhihu.com/p/89602892 规则OrderRef用来标识报单，OrderActionRef用来标识标撤单。 CTP量化投资API要求报单的OrderRef/OrderActionRef字段在同一线程内必须是递增的，长度不超过13的数字字符串。 如果包含非数字字符，或者非递增关系，都会触发以下的错误： {&apos;ErrorID&apos;: 22, &apos;ErrorMsg&apos;: &apos;CTP:报单错误：不允许重复报单&apos;} 设计方案如果为每个策略开启一个线程，即一个TraderApi管理一个策略，则指定策略ID，为每个策略分配一个报单区间，据此在回报/通知中分辨出对应策略。 如果一个TraderApi作为主引擎管理多个策略，则需要在引擎层面管理OrderRef/OrderActionRef，然后维护策略与OrderRef/OrderActionRef的关系表，或者制定一个可标识策略的递增规则，据此在回报/通知中分辨出对应策略。]]></content>
      <categories>
        <category>algoplus</category>
      </categories>
      <tags>
        <tag>algoplus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CTP出现4097、8193报错]]></title>
    <url>%2F2022%2F01%2F22%2FCTP%E5%87%BA%E7%8E%B04097%E3%80%818193%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[记录学习CTP出现4097、8193报错，原文参考https://zhuanlan.zhihu.com/p/89263750 追根溯源在客户端程序与期货公司行情、交易前置建立连接之后，服务器会定时发送心跳包确认连接是否正常。当网络连接出现异常，客户端可以在回调函数OnFrontDisconnected中收到通知： ///当客户端与交易后台通信连接断开时，该方法被调用。当发生这个情况后，API会自动重新连接，客户端可不做处理。 ///@param nReason 错误原因 /// 0x1001 网络读失败 4097 /// 0x1002 网络写失败 4098 /// 0x2001 接收心跳超时 8193 /// 0x2002 发送心跳失败 8194 /// 0x2003 收到错误报文 8195 void OnFrontDisconnected(int nReason) {}; 交易者最常遇到的就是4097（十六进制：0x1001）和8193（十六进制：0x2001）报错。 如果网络异常，很容易发现问题。 但是初次使用CTP的交易者会发现，在网络正常情况下，也会报这个错误。究竟是哪里出了问题呢？ 其实，这是不熟悉CTP异步执行特性导致的。 CTP所有的方法都是异步执行的，也就是说，调用一个方法返回时，该方法并没有执行完成，而是刚开始执行。如果主线程没有等待子线程执行完成就结束了，会触发OnFrontDisconnected错误。 网络正常情况下使用AlgoPlus重现该问题from AlgoPlus.CTP.MdApi import MdApi class TickEngine(MdApi): #5-8行 # def __init__(self, md_server, broker_id, investor_id, password, app_id, auth_code # , instrument_id_list, md_queue_list=None # , page_dir=&apos;&apos;, using_udp=False, multicast=False): # self.Join() # ///深度行情通知 def OnRtnDepthMarketData(self, pDepthMarketData): print(pDepthMarketData) if __name__ == &apos;__main__&apos;: import sys sys.path.append(&quot;..&quot;) from account_info import my_future_account_info_dict future_account = my_future_account_info_dict[&apos;SimNow&apos;] tick_engine = TickEngine(future_account.server_dict[&apos;MDServer&apos;] , future_account.broker_id , future_account.investor_id , future_account.password , future_account.app_id , future_account.auth_code , future_account.instrument_id_list , None , future_account.md_page_dir) # 30行 # tick_engine.Join() 网络正常情况下的解决方案解决办法很简单，就是在主线程结束之前调用Join方法，等待子线程执行。 将以上5-8行代码的注释取消，或者在30行之后调用tick_engine的Join方法，就正常了。 网络异常情况下的解决方案 无需做其他处理，等待CTP自动重连，重连成功后自动登录账户，然后就可以正常使用了。 MdApi同样会自动重连，且重连成功后自动登录账户，但是需要重新订阅行情，否则不会收到断开前订阅的行情数据。 AlgoPlus已封装了MdApi重连成功时订阅合约的功能。]]></content>
      <categories>
        <category>algoplus</category>
      </categories>
      <tags>
        <tag>algoplus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[algoplus期货量化（5）]]></title>
    <url>%2F2022%2F01%2F19%2Falgoplus%E6%9C%9F%E8%B4%A7%E9%87%8F%E5%8C%96%EF%BC%885%EF%BC%89%2F</url>
    <content type="text"><![CDATA[记录学习用AlgoPlus构建自己的交易盈亏风控系统，原文参考https://zhuanlan.zhihu.com/p/88655638 止盈止损方法常用的止盈止损方案有：固定止损、固定止盈、跟踪止损、阶梯止损、保本、时间止损。 撤单次数数据结构# {&quot;InstrumentID&quot;: 0} self.order_action_num_dict = {} 撤单次数是一个以合约名为键值的字典。收到撤单通知后，对撤单次数计算增加。 收到撤单通知时增加撤单次数计数 def OnRtnOrder(self, pOrder): &quot;&quot;&quot; 当收到订单状态变化时，可以在本方法中获得通知。不适宜在回调函数里做比较耗时的操作。 :param pOrder: AlgoPlus.CTP.ApiStruct中OrderField的实例。 :return: &quot;&quot;&quot; if pOrder.OrderStatus == b&quot;5&quot;: if pOrder.InstrumentID in self.action_num_dict.keys(): self.action_num_dict[pOrder.InstrumentID] += 1 else: self.action_num_dict[pOrder.InstrumentID] = 1 止盈止损参数数据结构`pl_parameter = { &apos;StrategyID&apos;: 9, # 盈损参数，&apos;0&apos;代表止盈, &apos;1&apos;代表止损，绝对价差 &apos;ProfitLossParameter&apos;: { b&apos;rb2010&apos;: {&apos;0&apos;: [2], &apos;1&apos;: [2]}, b&apos;ni2007&apos;: {&apos;0&apos;: [20], &apos;1&apos;: [20]}, }, }` # {&quot;InstrumentID&quot;: {&quot;Type&quot;: []}} self.pl_parameter_dict = {} 止损参数是一个以合约名为键值的字典，根据不同的止损止盈类型存储价差/时间差等参数。其中，止损类型参数取值： Type止损逻辑b”0″固定止盈b”1″固定止损 价差/时间差等参数以列表形式存储，有些止损逻辑可能需要多个参数。 持仓数据结构# {&quot;InstrumentID&quot;: {&quot;LongVolume&quot;: 0, &quot;LongPositionList&quot;: [], &quot;ShortVolume&quot;: 0, &quot;ShortPositionList&quot;: []}} self.local_position_dict = {} 账户持仓以合约名为键值存入字典中，LongVolume统计合约总多头持仓，ShortVolume统计合约总空头持仓，LongPositionList、ShortPositionList以OrderRef为单位存储成交明细。 成交明细是在AlgoPlus.CTP.ApiStruct中TradeField基础上附加IsLock、AnchorTime、StopProfitDict、StopLossDict、MaxProfitPrice字段。 rtn_trade[&quot;IsLock&quot;] = False # 平仓状态 rtn_trade[&quot;AnchorTime&quot;] = timer() # 成交发生时间 rtn_trade[&quot;StopProfitDict&quot;] = {} # 止盈触发价格，持仓期间实时更新 rtn_trade[&quot;StopLossDict&quot;] = {} # 止损触发价格，持仓期间实时更新 成交通知 收到成交通知时放入一个列表中，等待后续处理，避免在此设计复杂的耗时操作。 def OnRtnTrade(self, pTrade): &quot;&quot;&quot; 当报单成交时，可以在本方法中获得通知。不适宜在回调函数里做比较耗时的操作。 :param pTrade: AlgoPlus.CTP.ApiStruct中的TradeField实例。 :return: &quot;&quot;&quot; self.local_rtn_trade_list.append(pTrade.to_dict_raw()) 处理成交通知根据开买卖开平字段将成交成交信息放入持仓数据结构中。 def process_rtn_trade(self): &quot;&quot;&quot; 从上次订单ID位置开始处理订单数据。 :return: &quot;&quot;&quot; last_rtn_trade_id = len(self.local_rtn_trade_list) for rtn_trade in self.local_rtn_trade_list[self.last_rtn_trade_id:last_rtn_trade_id]: if rtn_trade[&quot;InstrumentID&quot;] not in self.instrument_id_registered: self.instrument_id_registered.append(rtn_trade[&quot;InstrumentID&quot;]) rtn_trade[&quot;IsLock&quot;] = False rtn_trade[&quot;AnchorTime&quot;] = timer() rtn_trade[&quot;StopProfitDict&quot;] = {} rtn_trade[&quot;StopLossDict&quot;] = {} if rtn_trade[&quot;InstrumentID&quot;] not in self.local_position_dict.keys(): self.local_position_dict[rtn_trade[&quot;InstrumentID&quot;]] = {&quot;LongVolume&quot;: 0, &quot;LongPositionList&quot;: [], &quot;ShortVolume&quot;: 0, &quot;ShortPositionList&quot;: []} local_position_info = self.local_position_dict[rtn_trade[&quot;InstrumentID&quot;]] # 开仓 if rtn_trade[&quot;OffsetFlag&quot;] == b&apos;0&apos;: self.update_stop_price(rtn_trade) if rtn_trade[&quot;Direction&quot;] == b&apos;0&apos;: local_position_info[&quot;LongVolume&quot;] += rtn_trade[&quot;Volume&quot;] local_position_info[&quot;LongPositionList&quot;].append(rtn_trade) elif rtn_trade[&quot;Direction&quot;] == b&apos;1&apos;: local_position_info[&quot;ShortVolume&quot;] += rtn_trade[&quot;Volume&quot;] local_position_info[&quot;ShortPositionList&quot;].append(rtn_trade) elif rtn_trade[&quot;Direction&quot;] == b&apos;0&apos;: local_position_info[&quot;ShortVolume&quot;] = max(local_position_info[&quot;ShortVolume&quot;] - rtn_trade[&quot;Volume&quot;], 0) elif rtn_trade[&quot;Direction&quot;] == b&apos;1&apos;: local_position_info[&quot;LongVolume&quot;] = max(local_position_info[&quot;LongVolume&quot;] - rtn_trade[&quot;Volume&quot;], 0) self.last_rtn_trade_id = last_rtn_trade_id 实时监控当前行情价格是否触及止盈止损价遍历持仓数据结构中的所有成交明细，判断最新行情是否触发某个止盈止损阈值，如果触发则录入平仓报单。 def check_position(self): &quot;&quot;&quot; 检查所有持仓是否触发持仓阈值。 &quot;&quot;&quot; try: for instrument_id, position_info in self.local_position_dict.items(): for long_position in position_info[&quot;LongPositionList&quot;]: if not long_position[&quot;IsLock&quot;]: trigger = False order_price = None for stop_profit in long_position[&quot;StopProfitDict&quot;].values(): if self.md_dict[instrument_id][&quot;LastPrice&quot;] &amp;gt; stop_profit: trigger = True order_price = self.get_stop_profit_price(instrument_id, long_position[&quot;Direction&quot;]) break if not trigger: for stop_loss in long_position[&quot;StopLossDict&quot;].values(): if self.md_dict[instrument_id][&quot;LastPrice&quot;] &amp;lt; stop_loss: trigger = True order_price = self.get_stop_loss_price(instrument_id, long_position[&quot;Direction&quot;]) break if trigger and order_price: self.order_ref += 1 self.sell_close(long_position[&quot;ExchangeID&quot;], instrument_id, order_price, long_position[&quot;Volume&quot;], self.order_ref) long_position[&quot;IsLock&quot;] = True for short_position in position_info[&quot;ShortPositionList&quot;]: if not short_position[&quot;IsLock&quot;]: trigger = False order_price = None for stop_profit in short_position[&quot;StopProfitDict&quot;].values(): if self.md_dict[instrument_id][&quot;LastPrice&quot;] &amp;lt; stop_profit: trigger = True order_price = self.get_stop_profit_price(instrument_id, short_position[&quot;Direction&quot;]) break if not trigger: for stop_loss in short_position[&quot;StopLossDict&quot;].values(): if self.md_dict[instrument_id][&quot;LastPrice&quot;] &amp;gt; stop_loss: trigger = True order_price = self.get_stop_loss_price(instrument_id, short_position[&quot;Direction&quot;]) break if trigger and order_price: self.order_ref += 1 self.buy_close(short_position[&quot;ExchangeID&quot;], instrument_id, order_price, short_position[&quot;Volume&quot;], self.order_ref) short_position[&quot;IsLock&quot;] = True except Exception as err: self._write_log(err) 止盈止损逻辑根据止盈止损（固定止盈、固定止损、跟踪止损、阶梯止损、保本止损）逻辑计算出触发价格，存入成交明细字典中。这里给出了固定止盈和固定止损的例子 def update_stop_price(self, position_info): &quot;&quot;&quot; 获取止盈止损阈值。止损类型参考https://7jia.com/1002.html :param position_info: 持仓信息 :return: &quot;&quot;&quot; for instrument_id, pl_dict in self.pl_parameter_dict.items(): if isinstance(pl_dict, dict): for pl_type, delta in pl_dict.items(): # 固定止盈 sgn = 1 if position_info[&quot;Direction&quot;] == b&apos;0&apos; else -1 if pl_type == b&quot;0&quot;: position_info[&quot;StopProfitDict&quot;][b&quot;0&quot;] = position_info[&quot;Price&quot;] + delta[0] * sgn # 固定止损 elif pl_type == b&quot;1&quot;: position_info[&quot;StopLossDict&quot;][b&quot;1&quot;] = position_info[&quot;Price&quot;] - delta[0] * sgn 行情，目前可获取CTP实时推送的TICK行情，并可合成1minK线 历史数据+实时行情组装策略计算所需要的数据，由实时行情驱动策略，产生交易信号 根据产生的交易信号触发交易 风控模块，即止盈止损，首先需要获取交易账户所有持仓及持仓成本，需要本地维护一个字典，用来记录持仓信息 通知模块，成交通知，目前采用钉钉进行交易信息推送]]></content>
      <categories>
        <category>algoplus</category>
      </categories>
      <tags>
        <tag>algoplus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[algoplus期货量化（4）]]></title>
    <url>%2F2022%2F01%2F19%2Falgoplus%E6%9C%9F%E8%B4%A7%E9%87%8F%E5%8C%96%EF%BC%884%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Python的multiprocessing,Queue，Process在多线程multiprocessing模块中，有两个类，Queue（队列）和Process（进程） 队列Queue： Queue是python中的标准库，可以直接import引用在队列中; Queue.Queue(maxsize)创建队列对象，如果不提供maxsize,则队列数无限制。 # _*_ encoding:utf-8 _*_ import Queue q = Queue.Queue(10) q.put(&apos;LOVE&apos;) q.put(&apos;You&apos;) print (q.get()) print (q.get()) 当一个队列为空的时候,用get取回堵塞,所以一般取队列的时候会用,get_nowait()方法,这个方法在向一个空队列取值的时候会抛一个Empty异常,所以一般会先判断队列是否为空,如果不为空则取值; 不阻塞的方式取队列 判断队列是否为空，为空返回True，不为空返回False 返回队列的长度 Queue.get([block[, timeout]]) 获取队列，timeout等待时间Queue.get_nowait() 相当Queue.get(False)非阻塞 Queue.put(item) 写入队列，timeout等待时间Queue.put_nowait(item) 相当Queue.put(item, False) Multiprocessing中使用子进程的概念Process： from multiprocessing import Process 可以通过Process来构造一个子进程 p=Process(target=fun,args=(args)) 再通过p.start()来启动子进程 再通过p.join()方法来使得子进程运行结束后再执行父进程 在multiprocessing中使用pool： 如果需要多个子进程时可以考虑使用进程池（pool）来管理 Pool创建子进程的方法与Process不同,是通过p.apply_async(func,args=(args))实现,一个池子里能同时运行的任务是取决你电脑CPU的数量,如果是4个CPU,那么会有task0,task1,task2,task3同时启动,task4需要在某个进程结束后才开始。 多个子进程间的通信: 多个子进程间的通信就要采用第一步中的队列Queue,比如,有以下需求,一个子进程向队列中写数据,另一个进程从队列中取数据; # _*_ encoding:utf-8 _*_ from multiprocessing import Process,Queue,Pool,Pipe import os,time,random #写数据进程执行的代码： def write(p): for value in [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;]: print (&apos;Write---Before Put value---Put %s to queue...&apos; % value) p.put(value) print (&apos;Write---After Put value&apos;) time.sleep(random.random()) print (&apos;Write---After sleep&apos;) #读数据进程执行的代码： def read(p): while True: print (&apos;Read---Before get value&apos;) value = p.get(True) print (&apos;Read---After get value---Get %s from queue.&apos; % value) if __name__ == &apos;__main__&apos;: #父进程创建Queue，并传给各个子进程： p = Queue() pw = Process(target=write,args=(p,)) pr = Process(target=read,args=(p,)) #启动子进程pw，写入： pw.start() #启动子进程pr，读取: pr.start() #等待pw结束： pw.join() #pr进程里是死循环，无法等待其结束，只能强行终止： pr.terminate()]]></content>
      <categories>
        <category>algoplus</category>
      </categories>
      <tags>
        <tag>algoplus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[algoplus期货量化（2）]]></title>
    <url>%2F2022%2F01%2F17%2Falgoplus%E6%9C%9F%E8%B4%A7%E9%87%8F%E5%8C%96%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[记录学习使用AlgoPlus接收期货实时行情，原文参考https://zhuanlan.zhihu.com/p/86082225 使用AlgoPlus接收期货实时行情关于CTP CTP是Comprehensive Transaction Platform的简称。CTP有MdApi和TraderApi两个独立的开放接口。 MdApi负责行情相关操作（订阅、接收）。 TraderApi负责交易相关的操作（买、卖、撤、查）。 MdApi与TraderApi方法的执行过程都是异步的，每一个请求都对应一个或多个负责接收执行结果的回调函数。例如，通过ReqOrderInsert方法向交易所发出买开仓指令，对应的回调方法OnRtnOrder可以实时接收交易所服务器发回来的执行通知。 AlgoPlus创建行情接口MdApi是行情接口，使用时只需要传递账户参数创建一个实例就可以了。示例： from AlgoPlus.CTP.MdApi import MdApi class TickEngine(MdApi): # 深度行情通知 def OnRtnDepthMarketData(self, pDepthMarketData): print(pDepthMarketData) # print(f&quot;{pDepthMarketData.InstrumentID}当前最新价：{pDepthMarketData.LastPrice}&quot;) if __name__ == &apos;__main__&apos;: from account_info import my_future_account_info_dict future_account = my_future_account_info_dict[&apos;SimNow&apos;] tick_engine = TickEngine(future_account.server_dict[&apos;MDServer&apos;] , future_account.broker_id , future_account.investor_id , future_account.password , future_account.app_id , future_account.auth_code , future_account.instrument_id_list , None , future_account.md_page_dir) tick_engine.Join() 1、从AlgoPlus.CTP.MdApi文件中导入MdApi类。MdApi已对工作流程的前六步进行了封装。 2、TickEngine是MdApi的子类。TickEngine类主要实现收到行情的数据处理算法，示例只将收到的行情打印出来。 3、创建行情接口实例前，需要导入账户信息。示例的账户信息存放在同一个目录下的account_info.py文件中。 4、交易时间运行以上代码就可以将接收到的实时期货行情打印出来。 5、回调函数OnRtnDepthMarketData接收到的pDepthMarketData行情是DepthMarketDataField结构体的实例，在AlgoPlus.CTP.ApiStruct中被定义。以调用属性的方式可以获取行情任意字段的数值，例如pDepthMarketData.LastPrice表示最新价。DepthMarketDataField包括以下字段： class DepthMarketDataField(BaseField): &quot;&quot;&quot;深度行情&quot;&quot;&quot; _fields_ = [ (&apos;TradingDay&apos;, c_char * 9) # ///交易日 , (&apos;InstrumentID&apos;, c_char * 31) # 合约代码 , (&apos;ExchangeID&apos;, c_char * 9) # 交易所代码 , (&apos;ExchangeInstID&apos;, c_char * 31) # 合约在交易所的代码 , (&apos;LastPrice&apos;, c_double) # 最新价 , (&apos;PreSettlementPrice&apos;, c_double) # 上次结算价 , (&apos;PreClosePrice&apos;, c_double) # 昨收盘 , (&apos;PreOpenInterest&apos;, c_double) # 昨持仓量 , (&apos;OpenPrice&apos;, c_double) # 今开盘 , (&apos;HighestPrice&apos;, c_double) # 最高价 , (&apos;LowestPrice&apos;, c_double) # 最低价 , (&apos;Volume&apos;, c_int) # 数量 , (&apos;Turnover&apos;, c_double) # 成交金额 , (&apos;OpenInterest&apos;, c_double) # 持仓量 , (&apos;ClosePrice&apos;, c_double) # 今收盘 , (&apos;SettlementPrice&apos;, c_double) # 本次结算价 , (&apos;UpperLimitPrice&apos;, c_double) # 涨停板价 , (&apos;LowerLimitPrice&apos;, c_double) # 跌停板价 , (&apos;PreDelta&apos;, c_double) # 昨虚实度 , (&apos;CurrDelta&apos;, c_double) # 今虚实度 , (&apos;UpdateTime&apos;, c_char * 9) # 最后修改时间 , (&apos;UpdateMillisec&apos;, c_int) # 最后修改毫秒 , (&apos;BidPrice1&apos;, c_double) # 申买价一 , (&apos;BidVolume1&apos;, c_int) # 申买量一 , (&apos;AskPrice1&apos;, c_double) # 申卖价一 , (&apos;AskVolume1&apos;, c_int) # 申卖量一 , (&apos;BidPrice2&apos;, c_double) # 申买价二 , (&apos;BidVolume2&apos;, c_int) # 申买量二 , (&apos;AskPrice2&apos;, c_double) # 申卖价二 , (&apos;AskVolume2&apos;, c_int) # 申卖量二 , (&apos;BidPrice3&apos;, c_double) # 申买价三 , (&apos;BidVolume3&apos;, c_int) # 申买量三 , (&apos;AskPrice3&apos;, c_double) # 申卖价三 , (&apos;AskVolume3&apos;, c_int) # 申卖量三 , (&apos;BidPrice4&apos;, c_double) # 申买价四 , (&apos;BidVolume4&apos;, c_int) # 申买量四 , (&apos;AskPrice4&apos;, c_double) # 申卖价四 , (&apos;AskVolume4&apos;, c_int) # 申卖量四 , (&apos;BidPrice5&apos;, c_double) # 申买价五 , (&apos;BidVolume5&apos;, c_int) # 申买量五 , (&apos;AskPrice5&apos;, c_double) # 申卖价五 , (&apos;AskVolume5&apos;, c_int) # 申卖量五 , (&apos;AveragePrice&apos;, c_double) # 当日均价 , (&apos;ActionDay&apos;, c_char * 9) # 业务日期 ] 说明： 1、队列是实现行情进程与策略进程之间共享数据的最简单有效的方案，也是AlgoPlus默认使用的方案。 2、每个策略对应一个队列，将这些队列的列表赋值给参数md_queue_list。 3、在OnRtnDepthMarketData中，将收到的行情放入所有队列。 策略接收行情import time from datetime import datetime, timedelta from multiprocessing import Process, Queue from AlgoPlus.CTP.TraderApi import TraderApi from AlgoPlus.CTP.ApiStruct import * from tick_engine import TickEngine class TraderEngine(TraderApi): def __init__(self, td_server, broker_id, investor_id, password, app_id, auth_code, md_queue=None , page_dir=&apos;&apos;, private_resume_type=2, public_resume_type=2): self.order_ref = 0 # 报单引用 self.order_time = None # 报单时间 self.order_status = b&quot;&quot; # 订单状态 self.Join() # 撤单 def req_order_action(self, exchange_id, instrument_id, order_ref, order_sysid=&apos;&apos;): input_order_action_field = InputOrderActionField( BrokerID=self.broker_id, InvestorID=self.investor_id, UserID=self.investor_id, ExchangeID=exchange_id, ActionFlag=&quot;0&quot;, InstrumentID=instrument_id, FrontID=self.front_id, SessionID=self.session_id, OrderSysID=order_sysid, OrderRef=str(order_ref), ) l_retVal = self.ReqOrderAction(input_order_action_field) # 报单 def req_order_insert(self, exchange_id, instrument_id, order_price, order_vol, order_ref, direction, offset_flag): input_order_field = InputOrderField( BrokerID=self.broker_id, InvestorID=self.investor_id, ExchangeID=exchange_id, InstrumentID=instrument_id, UserID=self.investor_id, OrderPriceType=&quot;2&quot;, Direction=direction, CombOffsetFlag=offset_flag, CombHedgeFlag=&quot;1&quot;, LimitPrice=order_price, VolumeTotalOriginal=order_vol, TimeCondition=&quot;3&quot;, VolumeCondition=&quot;1&quot;, MinVolume=1, ContingentCondition=&quot;1&quot;, StopPrice=0, ForceCloseReason=&quot;0&quot;, IsAutoSuspend=0, OrderRef=str(order_ref), ) l_retVal = self.ReqOrderInsert(input_order_field) # 买开仓 def buy_open(self, exchange_ID, instrument_id, order_price, order_vol, order_ref): self.req_order_insert(exchange_ID, instrument_id, order_price, order_vol, order_ref, &apos;0&apos;, &apos;0&apos;) # 卖开仓 def sell_open(self, exchange_ID, instrument_id, order_price, order_vol, order_ref): self.req_order_insert(exchange_ID, instrument_id, order_price, order_vol, order_ref, &apos;1&apos;, &apos;0&apos;) # 买平仓 def buy_close(self, exchange_ID, instrument_id, order_price, order_vol, order_ref): if exchange_ID == &quot;SHFE&quot; or exchange_ID == &quot;INE&quot;: self.req_order_insert(exchange_ID, instrument_id, order_price, order_vol, order_ref, &apos;0&apos;, &apos;3&apos;) else: self.req_order_insert(exchange_ID, instrument_id, order_price, order_vol, order_ref, &apos;0&apos;, &apos;1&apos;) # 卖平仓 def sell_close(self, exchange_ID, instrument_id, order_price, order_vol, order_ref): if exchange_ID == &quot;SHFE&quot; or exchange_ID == &quot;INE&quot;: self.req_order_insert(exchange_ID, instrument_id, order_price, order_vol, order_ref, &apos;1&apos;, &apos;3&apos;) else: self.req_order_insert(exchange_ID, instrument_id, order_price, order_vol, order_ref, &apos;1&apos;, &apos;1&apos;) # 报单通知 def OnRtnOrder(self, pOrder): self.order_status = pOrder.OrderStatus if pOrder.OrderStatus == b&quot;a&quot;: status_msg = &quot;未知状态！&quot; elif pOrder.OrderStatus == b&quot;0&quot;: if pOrder.Direction == b&quot;0&quot;: if pOrder.CombOffsetFlag == b&quot;0&quot;: status_msg = &quot;买开仓已全部成交！&quot; else: status_msg = &quot;买平仓已全部成交！&quot; else: if pOrder.CombOffsetFlag == b&quot;0&quot;: status_msg = &quot;卖开仓已全部成交！&quot; else: status_msg = &quot;卖平仓已全部成交！&quot; elif pOrder.OrderStatus == b&quot;1&quot;: status_msg = &quot;部分成交！&quot; elif pOrder.OrderStatus == b&quot;3&quot;: status_msg = &quot;未成交！&quot; elif pOrder.OrderStatus == b&quot;5&quot;: status_msg = &quot;已撤！&quot; else: status_msg = &quot;其他！&quot; self._write_log(f&quot;{status_msg}=&gt;{pOrder}&quot;) def Join(self): while True: if self.status == 0: last_md = None # 如果队列非空，从队列中取数据 while not self.md_queue.empty(): last_md = self.md_queue.get(block=False) if last_md: # ############################################################################# # if self.order_ref == 0: # 涨停买开仓 self.order_ref += 1 self.buy_open(test_exchange_id, test_instrument_id, last_md.BidPrice1, test_vol, self.order_ref) self.order_time = datetime.now() self._write_log(f&quot;=&gt;买开仓请求！&quot;) if self.order_ref == 1 and self.order_status == b&quot;3&quot; and datetime.now() - self.order_time &gt; timedelta(seconds=3): self.order_status = b&quot;&quot; self.req_order_action(test_exchange_id, test_instrument_id, self.order_ref) self._write_log(f&quot;=&gt;发出撤单请求！&quot;) if self.order_ref == 1 and self.order_status == b&quot;0&quot;: self.order_ref += 1 self.order_status = b&quot;&quot; self.sell_close(test_exchange_id, test_instrument_id, last_md.BidPrice1, test_vol, self.order_ref) self.order_time = datetime.now() self._write_log(f&quot;=&gt;买开仓已全部成交，发出卖平仓请求！&quot;) # ############################################################################# # if self.order_ref == 1: if self.order_status == b&quot;5&quot;: print(&quot;老爷，买开仓单超过3秒未成交，已撤销，这里的测试工作已经按照您的吩咐全部完成！&quot;) break elif datetime.now() - self.order_time &gt; timedelta(seconds=3): print(&quot;买开仓执行等待中！&quot;) elif self.order_ref == 2: if self.order_status == b&quot;0&quot;: print(&quot;老爷，卖平仓单已成交，这里的测试工作已经按照您的吩咐全部完成！&quot;) break elif datetime.now() - self.order_time &gt; timedelta(seconds=3): print(&quot;卖平仓执行等待中！&quot;) else: time.sleep(1) 说明： 1、直接在TraderApi的子类中编写策略是最简单的方案。但是，该方案不适合单账户策略比较多的情况，因为CTP支持同时在线的终端个数有限。如果策略比较多，则创建有限个TraderApi，在独立的Strategy类与MdApi和TraderApi之间实现共享数据。 2、在Join方法中实现了策略逻辑：登录成功之后，先以排队价发开仓委托，如果挂单超过3秒未成交，则撤单并退出策略。如果开仓全部成交，则以对手价发平仓委托，等待全部成交后退出策略。 3、Join方法中的策略每次执行时，从队列中取出所有数据，以最后一笔行情的盘口价格作为委托价。4、OnRtnOrder收到订单状态通知时更新本地订单状态、持仓手数，在策略中根据状态的变化进行后续操作。 ## 多进程 # 请在这里填写需要测试的合约数据 # 警告：该例子只支持上期所品种平今仓测试 test_exchange_id = &apos;SHFE&apos; # 交易所 test_instrument_id = &apos;ag1912&apos; # 合约代码 test_vol = 1 # 报单手数 share_queue = Queue(maxsize=100) # 队列 if __name__ == &quot;__main__&quot;: import sys sys.path.append(&quot;..&quot;) from account_info import my_future_account_info_dict future_account = my_future_account_info_dict[&apos;SimNow&apos;] # 行情进程 md_process = Process(target=TickEngine, args=(future_account.server_dict[&apos;MDServer&apos;] , future_account.broker_id , future_account.investor_id , future_account.password , future_account.app_id , future_account.auth_code , future_account.instrument_id_list , [share_queue] , future_account.md_page_dir) ) # 交易进程 trader_process = Process(target=TraderEngine, args=(future_account.server_dict[&apos;TDServer&apos;] , future_account.broker_id , future_account.investor_id , future_account.password , future_account.app_id , future_account.auth_code , share_queue , future_account.td_page_dir) ) md_process.start() trader_process.start() md_process.join() trader_process.join() 说明： 1、前几节的例子中需要手动设置涨跌停价作为报单价，这里我们以实时行情的盘口价作为报单价，所以不再需要设置涨跌停价参数。 2、share_queue是一个队列，在多进程中，队列数据可以实现共享。 3、md_process和trader_process分别是行情进程和交易进程。这两个进程通过队列share_queue共享数据。 4、所有进程的join方法须在start方法之后最后调用。5、这段代码放置在策略代码最后，执行即可看到执行结果。6、参考这个例子可以很方便的扩展一对多、多对多的进程间数据共享模式。 AlgoPlus的设计在登录时通过查询获取初始持仓、可用资金，成交发生时自动增减持仓数量，当平仓报单时自动增加冻结数量。当报撤单、出入金时自动增减可用资金。 ## 智能交易指令 买卖智能开平指令 只关注买卖，不关注平仓还是开仓，优先平仓，无持仓的情况下再开仓。 除了buyOpen、sellClose、sellOpen、buyClose、closeLong、closeShort这些指定了开平方向的指令，其他都是智能开平指令。]]></content>
      <categories>
        <category>algoplus</category>
      </categories>
      <tags>
        <tag>algoplus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[algoplus期货量化（3）]]></title>
    <url>%2F2022%2F01%2F16%2Falgoplus%E6%9C%9F%E8%B4%A7%E9%87%8F%E5%8C%96%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[买卖撤查请求买卖报单、撤单、查询概括了所有的交易业务。这些业务都是由交易者主动发起的，并且提供必要的信息。交易者只需要传递相应参数，AlgoPlus就可以按照CTP标准组织信息并发起请求。 b&quot;ExchangeID&quot;: b&quot;SHFE&quot;, b&quot;Direction&quot;: 1, b&quot;Volume&quot;: 1 exchange_id：b&quot;SHFE&quot; instrument_id：b&quot;rb2001&quot; order_vol：1 买开仓 buy_open(exchange_id, instrument_id, order_price, order_vol) 卖平仓 sell_close(exchange_id, instrument_id, order_price, order_vol, is_today) 卖开仓 sell_open(exchange_id, instrument_id, order_price, order_vol) 买平仓 buy_close(exchange_id, instrument_id, order_price, order_vol, is_today) 撤单 req_order_action(exchange_id, instrument_id, order_ref, order_sysid) 查成交 req_qry_trade() 查持仓 req_qry_investor_position() 查资金账户 req_qry_trading_account() 查合约 req_qry_instrument() 买卖撤查通知交易者发起的买卖报单、撤单请求后，期货公司柜台会推送一条响应信息，AlgoPlus的回调函数以此作为参数被调用。 OnRspOrderInsert(pInputOrder, pRspInfo, nRequestID, bIsLast) OnRspOrderAction(pInputOrderAction, pRspInfo, nRequestID, bIsLast) pRspInfo是一个python字典，内容如下 { &apos;ErrorID&apos;: 0, # 错误代码 &apos;ErrorMsg&apos;: &quot;&quot;, # 错误信息 } 买卖报单、撤单请求到达交易所被执行的过程中，交易所实时推送订单状态变化信息，AlgoPlus的回调函数OnRtnOrder(pOrder)以此作为参数被调用。pOrder是一个python字典，内容如下 { &apos;BrokerID&apos;: &quot;&quot;, # 经纪公司代码 &apos;InvestorID&apos;: &quot;&quot;, # 投资者代码 &apos;InstrumentID&apos;: &quot;&quot;, # 合约代码 &apos;OrderRef&apos;: &quot;&quot;, # 报单引用 &apos;UserID&apos;: &quot;&quot;, # 用户代码 &apos;OrderPriceType&apos;: &quot;&quot;, # 报单价格条件 &apos;Direction&apos;: &quot;&quot;, # 买卖方向 &apos;CombOffsetFlag&apos;: &quot;&quot;, # 组合开平标志 &apos;CombHedgeFlag&apos;: &quot;&quot;, # 组合投机套保标志 &apos;LimitPrice&apos;: 0.0, # 价格 &apos;VolumeTotalOriginal&apos;: 0, # 数量 &apos;TimeCondition&apos;: &quot;&quot;, # 有效期类型 &apos;GTDDate&apos;: &quot;&quot;, # GTD日期 &apos;VolumeCondition&apos;: &quot;&quot;, # 成交量类型 &apos;MinVolume&apos;: 0, # 最小成交量 &apos;ContingentCondition&apos;: &quot;&quot;, # 触发条件 &apos;StopPrice&apos;: 0.0, # 止损价 &apos;ForceCloseReason&apos;: &quot;&quot;, # 强平原因 &apos;IsAutoSuspend&apos;: 0, # 自动挂起标志 &apos;BusinessUnit&apos;: &quot;&quot;, # 业务单元 &apos;RequestID&apos;: 0, # 请求编号 &apos;OrderLocalID&apos;: &quot;&quot;, # 本地报单编号 &apos;ExchangeID&apos;: &quot;&quot;, # 交易所代码 &apos;ParticipantID&apos;: &quot;&quot;, # 会员代码 &apos;ClientID&apos;: &quot;&quot;, # 客户代码 &apos;ExchangeInstID&apos;: &quot;&quot;, # 合约在交易所的代码 &apos;TraderID&apos;: &quot;&quot;, # 交易所交易员代码 &apos;InstallID&apos;: 0, # 安装编号 &apos;OrderSubmitStatus&apos;: &quot;&quot;, # 报单提交状态 &apos;NotifySequence&apos;: 0, # 报单提示序号 &apos;TradingDay&apos;: &quot;&quot;, # 交易日 &apos;SettlementID&apos;: 0, # 结算编号 &apos;OrderSysID&apos;: &quot;&quot;, # 报单编号 &apos;OrderSource&apos;: &quot;&quot;, # 报单来源 &apos;OrderStatus&apos;: &quot;&quot;, # 报单状态 &apos;OrderType&apos;: &quot;&quot;, # 报单类型 &apos;VolumeTraded&apos;: 0, # 今成交数量 &apos;VolumeTotal&apos;: 0, # 剩余数量 &apos;InsertDate&apos;: &quot;&quot;, # 报单日期 &apos;InsertTime&apos;: &quot;&quot;, # 委托时间 &apos;ActiveTime&apos;: &quot;&quot;, # 激活时间 &apos;SuspendTime&apos;: &quot;&quot;, # 挂起时间 &apos;UpdateTime&apos;: &quot;&quot;, # 最后修改时间 &apos;CancelTime&apos;: &quot;&quot;, # 撤销时间 &apos;ActiveTraderID&apos;: &quot;&quot;, # 最后修改交易所交易员代码 &apos;ClearingPartID&apos;: &quot;&quot;, # 结算会员编号 &apos;SequenceNo&apos;: 0, # 序号 &apos;FrontID&apos;: 0, # 前置编号 &apos;SessionID&apos;: 0, # 会话编号 &apos;UserProductInfo&apos;: &quot;&quot;, # 用户端产品信息 &apos;StatusMsg&apos;: &quot;&quot;, # 状态信息 &apos;UserForceClose&apos;: 0, # 用户强平标志 &apos;ActiveUserID&apos;: &quot;&quot;, # 操作用户代码 &apos;BrokerOrderSeq&apos;: 0, # 经纪公司报单编号 &apos;RelativeOrderSysID&apos;: &quot;&quot;, # 相关报单 &apos;ZCETotalTradedVolume&apos;: 0, # 郑商所成交数量 &apos;IsSwapOrder&apos;: 0, # 互换单标志 &apos;BranchID&apos;: &quot;&quot;, # 营业部编号 &apos;InvestUnitID&apos;: &quot;&quot;, # 投资单元代码 &apos;AccountID&apos;: &quot;&quot;, # 资金账号 &apos;CurrencyID&apos;: &quot;&quot;, # 币种代码 &apos;IPAddress&apos;: &quot;&quot;, # IP地址 &apos;MacAddress&apos;: &quot;&quot;, # Mac地址 } OrderStatus取值及含义 全部成交 OrderStatus_AllTraded = b&apos;0&apos; #部分成交还在队列中 OrderStatus_PartTradedQueueing = b&apos;1&apos; #部分成交不在队列中 OrderStatus_PartTradedNotQueueing = b&apos;2&apos; #未成交还在队列中 OrderStatus_NoTradeQueueing = b&apos;3&apos; #未成交不在队列中 OrderStatus_NoTradeNotQueueing = b&apos;4&apos; #撤单 OrderStatus_Canceled = b&apos;5&apos; #未知 OrderStatus_Unknown = b&apos;a&apos; #尚未触发 OrderStatus_NotTouched = b&apos;b&apos; #已触发 OrderStatus_Touched = b&apos;c&apos; OrderSubmitStatus取值及含义 #已经提交 OrderSubmitStatus_InsertSubmitted = b&apos;0&apos; #撤单已经提交 OrderSubmitStatus_CancelSubmitted = b&apos;1&apos; #修改已经提交 OrderSubmitStatus_ModifySubmitted = b&apos;2&apos; #已经接受 OrderSubmitStatus_Accepted = b&apos;3&apos; #报单已经被拒绝 OrderSubmitStatus_InsertRejected = b&apos;4&apos; #撤单已经被拒绝 OrderSubmitStatus_CancelRejected = b&apos;5&apos; #改单已经被拒绝 OrderSubmitStatus_ModifyRejected = b&apos;6&apos; 当订单有成交发生时，交易所还会推送一条成交信息，AlgoPlus的回调函数OnRtnTrade(pTrade)以此作为参数被调用。除了成交价格之外，pTrade中的其他信息在pOrder中都有。 行情数据通知 创建行情接口实例时，AlgoPlus会根据交易者传递的合约列表参数自动订阅合约。在盘中，AlgoPlus的回调函数OnRtnDepthMarketData(pDepthMarketData)以实时行情数据为参数被调用。pDepthMarketData是一个python字典，内容如下： { &apos;TradingDay&apos;: b&apos;20200113&apos;, # 交易日 &apos;InstrumentID&apos;: b&apos;rb2005&apos;, # 合约代码 &apos;ExchangeID&apos;: b&apos;&apos;, # 交易所代码 &apos;ExchangeInstID&apos;: b&apos;&apos;, # 合约在交易所的代码 &apos;LastPrice&apos;: 3559.0, # 最新价 &apos;PreSettlementPrice&apos;: 3568.0, # 上次结算价 &apos;PreClosePrice&apos;: 3571.0, # 昨收盘 &apos;PreOpenInterest&apos;: 1357418.0, # 昨持仓量 &apos;OpenPrice&apos;: 3565.0, # 今开盘 &apos;HighestPrice&apos;: 3567.0, # 最高价 &apos;LowestPrice&apos;: 3544.0, # 最低价 &apos;Volume&apos;: 347796, # 数量 &apos;Turnover&apos;: 12361542250.0, # 成交金额 &apos;OpenInterest&apos;: 1345077.0, # 持仓量 &apos;ClosePrice&apos;: 1.7976931348623157e+308, # 今收盘 &apos;SettlementPrice&apos;: 1.7976931348623157e+308, # 本次结算价 &apos;UpperLimitPrice&apos;: 3782.0, # 涨停板价 &apos;LowerLimitPrice&apos;: 3353.0, # 跌停板价 &apos;PreDelta&apos;: 0.0, # 昨虚实度 &apos;CurrDelta&apos;: 1.7976931348623157e+308, # 今虚实度 &apos;UpdateTime&apos;: b&apos;23:00:01&apos;, # 最后修改时间 &apos;UpdateMillisec&apos;: 0, # 最后修改毫秒 &apos;BidPrice1&apos;: 3559.0, # 申买价一 &apos;BidVolume1&apos;: 158, # 申买量一 &apos;AskPrice1&apos;: 3560.0, # 申卖价一 &apos;AskVolume1&apos;: 18, # 申卖量一 &apos;BidPrice2&apos;: 1.7976931348623157e+308, # 申买价二 &apos;BidVolume2&apos;: 0, # 申买量二 &apos;AskPrice2&apos;: 1.7976931348623157e+308, # 申卖价二 &apos;AskVolume2&apos;: 0, # 申卖量二 &apos;BidPrice3&apos;: 1.7976931348623157e+308, # 申买价三 &apos;BidVolume3&apos;: 0, # 申买量三 &apos;AskPrice3&apos;: 1.7976931348623157e+308, # 申卖价三 &apos;AskVolume3&apos;: 0, # 申卖量三 &apos;BidPrice4&apos;: 1.7976931348623157e+308, # 申买价四 &apos;BidVolume4&apos;: 0, # 申买量四 &apos;AskPrice4&apos;: 1.7976931348623157e+308, # 申卖价四 &apos;AskVolume4&apos;: 0, # 申卖量四 &apos;BidPrice5&apos;: 1.7976931348623157e+308, # 申买价五 &apos;BidVolume5&apos;: 0, # 申买量五 &apos;AskPrice5&apos;: 1.7976931348623157e+308, # 申卖价五 &apos;AskVolume5&apos;: 0, # 申卖量五 &apos;AveragePrice&apos;: 35542.50839572623, # 当日均价 &apos;ActionDay&apos;: b&apos;20200110&apos; # 业务日期 } 技术分析体系]]></content>
      <categories>
        <category>algoplus</category>
      </categories>
      <tags>
        <tag>algoplus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[algoplus期货量化（1）]]></title>
    <url>%2F2022%2F01%2F14%2Falgoplus%E6%9C%9F%E8%B4%A7%E9%87%8F%E5%8C%96%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[记录学习基于AlgoPlus的量化交易开发准备工作，原文参考https://www.zhihu.com/column/AlgoPlus centos安装yumyum install -y git yum install -y vim yum -y install wget yum install -y bzip2 安装anocandawget --no-check-certificate https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2021.11-Linux-x86_64.sh bash Anaconda3-2021.11-Linux-x86_64.sh -u vim /etc/profile export PATH=/root/anaconda3/bin:$PATH source /etc/profile anocanda使用conda create -n foralgo python=3.7 conda env list source activate foralgo 添加镜像源conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 #显示检索路径 conda config --set show_channel_urls yes #显示镜像通道 conda config --show channels #更新pip源 pip install -i https://pypi.doubanio.com/simple pip -U --user 安装、启动mongovim /etc/yum.repos.d/mongodb-org-4.2.repo 编辑以下内容 [mongodb-enterprise] name=MongoDB Enterprise Repository baseurl=https://repo.mongodb.com/yum/redhat/$releasever/mongodb-enterprise/4.2/$basearch/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-4.2.asc 使用yum进行安装 yum install -y mongodb-org vim /etc/mongod.conf systemLog: destination: file #日志输出方式。file/syslog,如果是file，需指定path，默认是输出到标准输出流中 path: /var/log/mongodb/mongod/log #日志路径 logAppend: false #启动时，日志追加在已有日志文件内还是备份旧日志后，创建新文件记录日志, 默认false net: port: 27017 #监听端口，默认27017 bindIp: 127.0.0.1 #绑定监听的ip，设置为127.0.0.1时，只会监听本机 maxIncomingConnections: 65536 #最大连接数，可接受的连接数还受限于操作系统配置的最大连接数 wireObjectCheck: true #校验客户端的请求，防止错误的或无效BSON插入,多层文档嵌套的对象会有轻微性能影响,默认true processManagement: fork: true # 后台运行 security: authorization: enabled # enabled/disabled #开启客户端认证 storage: dbPath: /var/lib/mongodb # 数据库地址 journal: enabled: true #启动journal,64位系统默认开启，32位默认关闭 启动mongo mongod -f /etc/mongod.conf 安装algopluspip install AlgoPlus -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com]]></content>
      <categories>
        <category>algoplus</category>
      </categories>
      <tags>
        <tag>algoplus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[myQuant项目部署]]></title>
    <url>%2F2021%2F12%2F18%2FmyQuant%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[基于Backtrader的量化投资项目基于centos的docker镜像docker pull centos:7 centos安装yumyum install -y git yum install -y vim yum -y install wget 在新安装的Centos中安装python3.7 解决pip和yum问题yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tar.xz python环境创建tar xvf Python-3.7.0.tar.xz mv Python-3.7.0 python3 cd python3 ./configure --prefix=/usr/local/python3 make &amp;&amp; make install 检查python3.7的编译器： /usr/local/python3/bin/python3.7 建立Python3和pip3的软链 ln -s /usr/local/python3/bin/python3 /usr/bin/python3 并将/usr/local/python3/bin加入PATH （1）vim /etc/profile （2）按“I”，然后贴上下面内容： # vim ~/.bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/bin:/usr/local/python3/bin export PATH 安装mongo4vim /etc/yum.repos.d/mongodb-org-4.2.repo 编辑以下内容 [mongodb-enterprise] name=MongoDB Enterprise Repository baseurl=https://repo.mongodb.com/yum/redhat/$releasever/mongodb-enterprise/4.2/$basearch/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-4.2.asc 使用yum进行安装 yum install -y mongodb-org vim /etc/mongod.conf systemLog: destination: file #日志输出方式。file/syslog,如果是file，需指定path，默认是输出到标准输出流中 path: /var/log/mongodb/mongod/log #日志路径 logAppend: false #启动时，日志追加在已有日志文件内还是备份旧日志后，创建新文件记录日志, 默认false net: port: 27017 #监听端口，默认27017 bindIp: 127.0.0.1 #绑定监听的ip，设置为127.0.0.1时，只会监听本机 maxIncomingConnections: 65536 #最大连接数，可接受的连接数还受限于操作系统配置的最大连接数 wireObjectCheck: true #校验客户端的请求，防止错误的或无效BSON插入,多层文档嵌套的对象会有轻微性能影响,默认true processManagement: fork: true # 后台运行 security: authorization: enabled # enabled/disabled #开启客户端认证 storage: dbPath: /var/lib/mongodb # 数据库地址 journal: enabled: true #启动journal,64位系统默认开启，32位默认关闭 启动mongo mongod -f /etc/mongod.conf systemctl start 安装crontabyum install vixie-cron yum install crontabs service crond status 启动rsyslog&amp;crond服务 systemctl start rsyslog systemctl start crond systemctl restart crond tail -f /var/log/cron crontab -l 查看所有定时任务 crontab -e 编辑修改定时任务 更改时区 tzselect vim /etc/crontab 添加变量 CRON_TZ=Asia/Shanghai cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime service crond restart 示例 30 20 * * 1-5 python3 /home/stock/data_main.py 0 5 * * 2-6 python3 /home/stock/frm_main.py 30 7 * * 1-5 python3 /home/stock/stock_match.py 30 8 * * 1-5 python3 /home/stock/timer.py 将当前容器创建为镜像（id）docker commit -a &quot;ikangbow&quot; -m &quot;描述信心&quot; 730661ccf053 ikangbow/myquant:1.0.1 推送到dockerhubdocker push ikangbow/myquant:1.0.1 docker运行docker run -e TZ=&quot;Asia/Shanghai&quot; --privileged -it -d --name myquant ikangbow/myquant:1.0.3 安装talibwget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz cd ta-lib/ ./configure --prefix=/usr make &amp;&amp; make install cd /usr find -name libta_lib.so.0 vim /etc/profile export LD_LIBRARY_PATH=/usr/lib64 pip install ta-lib -U cp /usr/lib/libta_lib.* /usr/lib64/]]></content>
      <categories>
        <category>投资</category>
      </categories>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程]]></title>
    <url>%2F2021%2F10%2F26%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[为什么要用线程池？核心参数有哪些作用 降低资源消耗；提高线程利用率，降低创建和销毁线程的消耗。 提高响应速度；任务来了直接有线程可用可执行，而不是先创建线程，再执行。 提高线程的可管理；线程是稀缺资源，使用线程池可用统一分配调优监控。 线程池创建public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } 核心参数corePoolSize：核心线程最大数量，即线程池中常驻线程的最大数量。线程池新建线程的时候，如果当前线程总数小于corePoolSize，则新建的是核心线程；如果超过了corePoolSize则新建的是非核心线程。 maximumPoolSize：线程池中运行的最大线程数，包括核心线程和非核心线程。 keepAliveTime：线程池中空闲线程所存活的最长时间（仅适用于非核心线程）。 unit：表示超出核心线程数之外的线程的空闲存活时间，也就是核心线程不会消除，但是超出核心线程数的部分线程如果空闲一定的时间则会被消除，可以通过keepAliveTime来设置空闲时间。 workQueue：用来存放任务的阻塞队列，假设我们现在核心线程都被使用，还有任务进来则全部放入队列，直到整个队列被放满但再持续进入则会创建新的线程。 ThreadFactory：实际上是一个线程工厂，用来生产线程执行任务，我们可以选择使用默认的创建工厂，产生的线程都在同一个组内，拥有相同的优先级，且都不是守护线程。当然我们也可以选择自定义线程工厂，一般会根据业务来制定不同的线程工厂。 Handler：任务拒绝策略，有两种情况，第一种是当我们调用shutdown等方法关闭线程池后，这时候即使线程池内部还有没执行完的任务正在执行，但是由于线程池已经关闭，我们再继续想线程池提交任务就会遭到拒绝，另一种情况就是当达到最大线程数，线程池已经没有能力继续处理新提交的任务时，这时也就会拒绝。 执行流程图 判断线程池中的核心线程数是否已经到达corePoolSize，没有则创建一个核心线程执行任务。 若核心线程数已经到达corePoolSize，判断阻塞队列workQueque是否已满，如果没满，则将新任务加入阻塞队列。如果已满，判断线程池中的线程数是否到达maximumPoolSize，如果没有，则新建一个非核心线程执行任务，如果到达阀值，则执行线程池饱和策略。 线程池饱和策略 AbortPolicy：直接抛出一个异常阻止系统正常运行，默认策略 DiscardPolicy：直接丢弃任务，不予任何处理也不抛异常（如果允许任务丢失，建议此方案） DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前任务 CallerRunsPolicy：调用者运行一种调节机制，该策略既不会抛弃任务也不会抛异常，而是将某些任务回退到调用者，从而降低新任务的流量。]]></content>
      <categories>
        <category>面基</category>
      </categories>
      <tags>
        <tag>面基</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双亲委派]]></title>
    <url>%2F2021%2F10%2F23%2F%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%2F</url>
    <content type="text"><![CDATA[java中类加载器JDK自带有三个类加载器：BootStrapClassLoader，ExtClassLoader，AppClassLoaser BootStrapClassLoader是ExtClassLoader的父类加载器，默认加载%JAVA_HOME%/lib下的jar包和class文件。 ExtClassLoader是AppClassLoader的父类加载器，负责加载%JAVA_HOME%/jre/lib/ext目录下的一些扩展jar。 AppClassLoader是自定义类加载器的父类，负责加载classpath下的类文件，是系统类加载器，线程上下文加载器继承ClassLoader实现自定义类加载器。 双亲委派机制当某个类加载器需要加载.class文件时，它首先会把这个任务委托给他的上级类加载器，递归这个操作，如果上级的类加载器没有加载，自己才会去加载这个类。即向上委派，实际上就是查找缓存，看是否加载了该类，有则返回，没有继续向上；向下查找，查找加载的路径，有则加载，没有继续向下查找。 作用：主要为了安全性，避免用户自己编写的类动态替换java的一些核心类。同时也避免了类的重复加载，因为JVM中区分不同的类，不仅仅是根据类名，相同的class文件被不同的classLoader加载就是不同的两个类。 是什么Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象，加载某个类class文件时，java虚拟机采用的是双亲委派机制，即把请求交由父类处理。 工作原理如果一个类加载器收到了类加载请求，它并不会自己先加载，而是把这个请求委托给父类的加载器去执行 如果父类加载器还存在其他父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的引导类加载器 如果父类加载器可以完成类加载的任务，就成功返回，倘若父类加载器无法完成加载任务，子类加载器材会尝试自己去加载，这就是双亲委派机制 父类加载器一层一层往下分配任务，如果子类加载器能加载，则加载此类，如果一直向下查找也无法加载此类，则抛出异常。 作用避免类的重复加载。 保护程序安全，防止核心API被随意篡改。]]></content>
      <categories>
        <category>面基</category>
      </categories>
      <tags>
        <tag>面基</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试题（查漏补缺）]]></title>
    <url>%2F2021%2F10%2F18%2F%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA%EF%BC%89%2F</url>
    <content type="text"><![CDATA[List和Set的区别List:有序集合，可重复，按对象的插入顺序保存。允许多个Null元素对象，可以使用iterator取出所有元素，在逐一遍历，还可以使用get(index)获取指定下标的元素。 Set：无序，不可重复。最多只允许有一个Null元素对象，取元素时只能用iterator接口取得所有元素，在逐一遍历。 ArrayList和LinkedList区别ArrayList：基于动态数组实现，连续内存存储，适合下标访问（随机访问）。有扩容机制，因为数组长度固定，超出长度存数据时需要新建数组，然后将老数组拷贝到新的数组，如果不是尾部插入，还会涉及元素的移动。使用尾插法并指定初始容量可以极大提升性能。 LinkedList：基于链表实现，可以存储在分散的内存中，适合做数据的插入及删除操作，不适合查询。需要逐一遍历，遍历LinkedList必须使用iterator不能使用for循环，因为每次for循环体内通过get（i）取得某一元素时都需要对List重新遍历，性能消耗大。另外不要用indexOf等返回元素索引，并利用其进行遍历，使用indexOf对list进行遍历，当结果未空时会遍历整个列表。 ==和equals比较==比较的是栈中的值，基本数据类型是变量值，引用类型是堆中内存对象的地址。equals：object中默认也是采用==比较，但通常会重写。 接口和抽象类的区别一个类只能继承一个抽象类，但可以实现多个接口。继承抽象类用关键字extends，实现接口用implements。 抽象类中的成员变量可以是各种类型的，而接口中的成员变量默认是public static final类型，必须赋初始值，不能被修改。 抽象类可以存在普通成员函数，接口只有定义，在JDK8之前方法被指定为public abstract，JDK8之后允许添加非抽象的方法，但是必须用default关键字修饰；定义了default的方法可以不被实现子类所实现，但可以被实现子类的对象调用。 接口的设计目的是对类的行为进行约束，也就是提供一种机制，可以强制要求不同类具有相同的行为，它只约束了行为的有无，但不对如何实现行为进行限制。 而抽象类的设计目的，是为了代码复用，当不同的类具有某些相同的行为，且其中一部分行为的实现方式一致（A的非真子集，记为B），可以让这些类都派生与一个抽象类。这个抽象类中实现了B，避免让所有的子类来实现B，这样就达到了代码复用的目的。而A-B的部分，留给各个子类自己实现。正是因为A-B在这里没有实现，所有抽象类不允许实例化。 抽象类是对类本质的抽象，表达的是is a的关系。抽象类包含并实现子类的通用特性，将子类存在差异化的特性进行抽象，交由子类去实现。 接口的核心是定义行为，，即实现类可以做什么，至于实现类主体是谁，如何实现，接口并不关心。 重载和重写的区别重载：发生在同一个类中，方法名必须相同，参数类型不同，个数不同，顺序不同，方法返回值和访问修饰符可以不同。 重写：发生在父子类中，方法名，参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符的范围大于等于父类，如果父类方法访问修饰符为private则子类就不能重写该方法。]]></content>
      <categories>
        <category>面基</category>
      </categories>
      <tags>
        <tag>面基</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap相关]]></title>
    <url>%2F2021%2F10%2F18%2FHashMap%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[HashMap底层数据结构详解JDK1.7及之前：数组+链表 JDK1.8:数组+链表+红黑树 HashMap添加元素时，会根据哈希值和数组长度来计算该元素put的位置。put元素会判断当前位置是否已经有元素，这样在多线程环境下，会发生数据覆盖，导致数据丢失。通常为了使元素分布均匀会使用取模运算，计算出index后，就会将改元素添加进去，但是会出现两个key的哈希值相同，这时候会产生哈希冲突，当哈希冲突时会产生链表的数据结构，冲突的元素会在该索引出以链表的形式保存。 但是当链表的长度过长时，其固有弊端就显示出来了，即查询效率低。 此时就引入了第三种数据结构，红黑树，红黑树是一刻接近于平衡的二叉树，远远比链表的查询料率高。但是如果链表的长度不到一定阈值，直接使用红黑树也不行，因为红黑树的自身维护代价也比较高，每插入一个元素都可能打破红黑树的平衡，这时候就需要对红黑树再平衡（左旋，右旋，重新着色）。 HashMap中数组的初始长度是16，默认的加载因子是0.75 数组一单达到容量的阈值就需要对数组进行扩容。扩容就意味着要进行数组的移动，数组一旦移动，每移动一次就要重新计算索引，这个过程牵扯大量元素的迁移，会大大影响效率。如果直接使用与运算，这个效率远远高于取模运算。 为什么链表长度大于等于8时转成了红黑树 遵循概率论里的泊松分布。链表中元素个数为8的概率已经非常小，红黑树平均查找长度是log（n），当长度为8时，平均查找长度为3，如果继续使用链表，平均查找长度为8、2=4，这时候才有转为树的必要。 HashMap的遍历方式迭代器遍历（iterator） For Each方式遍历 Lambda表达式遍历 Streams Api遍历 迭代器entrySet @Test public void test001(){ Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;1&quot;,1); map.put(&quot;2&quot;,2); Iterator&lt;Map.Entry&lt;String,Integer&gt;&gt; iterator = map.entrySet().iterator(); while (iterator.hasNext()){ Map.Entry&lt;String,Integer&gt; entry = iterator.next(); System.out.println(&quot;key:&quot;+entry.getKey()+&quot;,value:&quot;+entry.getValue()); } } 当遍历时涉及到删除操作，建议使用iterator的remove方法。使用forEach会报错。 迭代器keySet @Test public void test002(){ Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;11&quot;,1); map.put(&quot;22&quot;,2); Iterator&lt;String&gt; iterator = map.keySet().iterator(); while (iterator.hasNext()){ String key = iterator.next(); System.out.println(&quot;key:&quot;+key+&quot;,value:&quot;+map.get(key)); } } ForEach EntrySet @Test public void test003(){ Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;111&quot;,1); map.put(&quot;222&quot;,2); for (Map.Entry&lt;String,Integer&gt; entry:map.entrySet() ) { System.out.println(&quot;key:&quot;+entry.getKey()+&quot;,value:&quot;+entry.getValue()); } } 通过Map.entrySet遍历key和value，代码简洁高效，推荐使用 ForEach keySet(如果只需要获取所有的key，推荐使用，比entrySet遍历要快，代码简洁) @Test public void test004(){ Map&lt;String,Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;1111&quot;,1); map.put(&quot;2222&quot;,2); for (String str: map.keySet()) { System.out.println(&quot;key:&quot;+str+&quot;,value:&quot;+map.get(str)); } } 根据键取值是耗时操作，不推荐使用 如果只需要获取所有的value，推荐使用，比entrySet快，简洁 @Test public void test004(){ Map&lt;String,Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;1111&quot;,1); map.put(&quot;2222&quot;,2); for (String str: map.values()) { System.out.println(&quot;value:&quot;+str); } } Lambda表达式 @Test public void test005(){ Map&lt;String,Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;11111&quot;,1); map.put(&quot;22222&quot;,2); map.forEach((key,value)-&gt;{ System.out.println(&quot;key:&quot;+key+&quot;,value:&quot;+value); }); } Streams API单线程 @Test public void test006(){ Map&lt;String,Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;111111&quot;,1); map.put(&quot;222222&quot;,2); map.entrySet().stream().forEach((entry)-&gt;{ System.out.println(&quot;key:&quot;+entry.getKey()+&quot;,value:&quot;+entry.getValue()); }); } Streams API多线程 @Test public void test007(){ Map&lt;String,Integer&gt; map = new HashMap&lt;&gt;(); map.put(&quot;1111111&quot;,1); map.put(&quot;2222222&quot;,2); map.entrySet().parallelStream().forEach((entry)-&gt;{ System.out.println(&quot;key:&quot;+entry.getKey()+&quot;,value:&quot;+entry.getValue()); }); }]]></content>
      <categories>
        <category>面基</category>
      </categories>
      <tags>
        <tag>面基</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TBQUANT]]></title>
    <url>%2F2021%2F05%2F22%2FTBQUANT%2F</url>
    <content type="text"><![CDATA[输出PlotString(&quot;BarStatus&quot;,Text(BarStatus)); 定义参数整个公式中只能出现一个 Params 宣告，并且要放到公式的开始部分，在变量定义之前 Params Bool bTest(False); //定义布尔型参数 bTest，默认值为 False; Numberic Length(10); //定义数值型参数 Length，默认值为 10； NumericSeries Price(0); //定义数值型序列参数 Price，默认值为 0； NumericRef output(0); //定义数值型引用参数 output，默认值为 0； String strTmp(&quot;Hello&quot;); //定义字符串参数 strTmp,默认值为 Hello 定义变量整个公式中只能出现一个 Vars 声明，并且要放到公式的开始部分，在参数定义之后，正文之前。 Vars NumericSeries MyVal1(0); //定义数值型序列变量 MyVal1，默认值为 0； Numeric MyVal2(0); //定义数值型变量 MyVal2，默认值为 0； Bool MyVal3(False); //定义布尔型变量 MyVal3，默认值为 False; String MyVal4(&quot;Test&quot;); //定义字符串变量 MyVal4，默认值为 Test。 用户函数编写示例Average 函数脚本如下： Params NumericSeries Price(1); Numeric Length(10); Vars Numeric AvgValue; Begin AvgValue = Summation(Price, Length) / Length; Return AvgValue; End 上面的例子，返回值只有一个，即最后求得的平均值，在函数中直接使用 Return 语句返回就可以了。 Params NumericSeries Price(1); Numeric Length(10); NumericRef HighestBar; //设置引用型的变量 Vars Numeric MyVal;TradeBlazer 公式开发指南 TradeBlazer 用户函数 - 69 - Numeric MyBar; Numeric i; Begin MyVal = Price; MyBar = 0; For i = 1 to Length – 1 { If ( Price[i] &gt; MyVal) { MyVal = Price[i]; MyBar = i; //记录最大值 Bar 与当前 Bar 的偏移量 } } HighestBar = MyBar; //通过将偏移量赋值给引用型变量，它可将该值传递回去 Return MyVal; End 函数参数声明类型 可传入的变量类型 Numeric== Numeric，NumericRef，NumericSeries NumericRe==f Numeric，NumericRef NumericSeries== Numeric，NumericRef，NumericSeries Bool== Bool，BoolRef，BoolSeriesTradeBlazer BoolRef== Bool，BoolRef BoolSeries== Bool，BoolRef，BoolSeries String== String，StringRef，StringSeries StringRef== String，StringRef StringSeries== String，StringRef，StringSeries 输出PlotNumeric-----在当前 Bar 输出一个数值； 语法：PlotNumeric(String Name,Numeric Number,Numeric Locator=0,Integer Color=-1,Integer BarsBack=0) Name 输出值的名称的字符串，可以为中、英文、数字或者其它字符； Number 输出的数值； Locator 输出值的定位点，默认时输出单点，否则输出连接两个值线段，用法请看例 2； Color 输出值的显示颜色，默认表示使用属性设置框中的颜色； BarsBack 从当前 Bar 向前回溯的 Bar 数，默认值为当前 Bar。 例 1：PlotNumeric (&quot;RSI&quot;,RSIValue); 输出 RSI 的值。 例 2：PlotNumeric (&quot;OpenToClose&quot;,Open,Close); 输出开盘价与收盘价的连线。（需要在公式属性的输出线形选择柱状图） 例 3：PlotNumeric (&quot;AvgValue&quot;,average(close,5),0,Blue); 输出一条蓝色的以收盘价计算的五日平均线。 注意：当后面的参数都使用默认值的情况下，可省略不写，如例 1。但如果后面还有其它参数要指明， 而只是中间某一个或者多个参数需要默认值的话，则中间参数不可省略，需将默认值一一填写，如例 PlotBool-----在当前 Bar 输出一个布尔值。 语法：PlotBool(String Name,Bool bPlot,Numeric Locator=0,Integer Color=-1,Integer BarsBack=0) Name 输出值的名称，不区分大小写； bPlot 输出的布尔值； Locator 输出值的定位点； Color 输出值的显示颜色，默认表示使用属性设置框中的颜色 BarsBack 从当前 Bar 向前回溯的 Bar 数，默认值为当前 Bar 示例 ：PlotBool (&quot;con&quot;,con,High); 在 Bar 的最高价位置输出条件 con 的布尔值。 PlotString-----在当前 Bar 输出一个字符串。 语法：PlotString(String Name,String str,Numeric Locator=0,Integer Color=-1,Integer BarsBack=0) Name 输出值的名称，不区分大小写； str 输出的字符串； Locator 输出值的定位点； Color 输出值的显示颜色，默认表示使用属性设置框中的颜色； BarsBack 从当前 Bar 向前回溯的 Bar 数，默认值为当前 Bar。 示例 ： PlotString (&quot;Bear Market&quot;,&quot;Bear Bear&quot;,High,Blue); 在 Bar 的最高价位置输出一个蓝色的字符串 Bear Bear 公式正文参数与变量的声明之后，在公式的正文部分，对所声明的变量进行赋值计算，必要的话还可以将所需的条件语句编写进公式里。最后则是将计算结果以数值、布尔或字符串的形式在图表上输出。现在我们来试着建立一个技术分析类的公式应用。首先，需要确立我们的指标里需要什么信息，以及需要通过何种计算从而得到我们想要的结果。比如，现在我们需要得到两条移动平均线，那么首先可以确定，要输出的是两个数值型的变量的值，然后我们就可以在公式中进行定义与赋值了。有如下代码： Params //参数的声明 numeric length1(10); numeric length2(20); Vars //变量的声明 numeric ma1; numeric ma2; Begin ma1 = averageFC(close,length1); ma2 = averageFC(close,length2); // 给变量的赋值 PlotNumeric(&quot;ma1&quot;,ma1,0,cyan); PlotNumeric(&quot;ma2&quot;,ma2,0,Magenta); //输出计算出来的平均值 End 以上公式先声明参数与变量，然后在 Begin 与 End 之间对公式的主体进行了赋值与输出。 交易指令函数交易开拓者提供 Buy，SellShort， BuyToCover，Sell，A_SendOrder 这五个函数来做下单的动作指令。 Buy，SellShort， BuyToCover，Sell 这四个函数可以针对历史数据在图表上做出讯号标识，同时在实时行情中也可以及时地发出下单委托的动作。因为可以对历史数据的图表上做出讯号的标识记录，交易设置的性能测试也需要由这几个函数参与计算的公式应用方可实现。 Buy —- 对当前合约发出买入开仓的指令，如果图表讯号显示当前持有空仓，则会先平掉空仓，再开多仓； SellShort —- 对当前合约发出卖出开仓的指令，如果图表讯号显示当前持有多仓，则会先平掉多仓，再开空仓； BuyToCover —- 对当前合约发出平空仓的指令，当图表讯号显示有空头持仓时，方可执行此指令； Sell —- 对当前合约发出平多仓的指令，当图表讯号显示有多头持仓时，方可执行此指令； if(condition1 &amp;&amp; marketposition!=1) //条件满足且没有持多仓情况下开多 { buy(); }else if(condition2 &amp;&amp; marketposition!=-1) //条件满足且没有持空仓时开空 { sellshort(); } 调试语句输出If(close&gt;open) Commentary(&quot;收阳=&quot;+Text(close)); // 在收阳的 K 线上显示收盘价]]></content>
      <categories>
        <category>投资</category>
      </categories>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[量化投资基础知识]]></title>
    <url>%2F2021%2F04%2F29%2F%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[夏普比率（SharpeRatio）夏普比率可以理解为风险调整后的收益，衡量的是每承受一单位风险，会产生多少超额报酬。既然重点在回报上，当然是希望同样的风险，能获得更高的回报；或者说当回报相同时，风险更小。 敲黑板：夏普比率越大越好 最大回撤率（DrawDown）在选定周期内任一时点往后推，基金净值在最低点时的收益下跌幅度的最大值。最大回撤代表投资者可能面临的最大亏损。 敲黑板：最大回撤率越小越好 年度收益率（Annual Return）]]></content>
      <categories>
        <category>投资</category>
      </categories>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[考驾照]]></title>
    <url>%2F2021%2F02%2F28%2F%E8%80%83%E9%A9%BE%E7%85%A7%2F</url>
    <content type="text"><![CDATA[系安全带，查看方向盘是否回正，挡位是否在空挡，手刹是否拉起 坡道定点停车和起步一档上坡，要对好边缘线，与右侧边缘线保持30cm。 左后视镜可看到坡起实线踩离合踩刹车。 拉起手刹停留超过3秒钟，放下手刹。 缓抬离合至车身抖动保持不动，松掉刹车，完成起步。 侧方位停车一档行车，车身离边缘线30cm-50cm为宜，当从右侧后视镜看到库位前角时候，踩离合踩刹车停车。 挂倒挡，松刹车慢松离合开始倒车，当库位前角从右侧后视镜消失时，向右打满方向盘，继续倒车。 观察左后视镜，当库位后角出现，回正方向盘继续倒车。 当左侧车门把手越过库位左侧虚线（左后车轮），向左打死方向盘继续倒车。 观察右后视镜，当车身与右侧边线平行踩离合踩刹车，完成入库。 挂一档，打左转向灯，松离合向前行驶，当车头中间将触及行车道左侧线向右回正方向盘继续行驶，完成出库。 直角转弯一档行车，车身离右侧边缘线30cm为宜，进入项目打开左转向灯，当听到直角转弯关闭左转向灯。 观察左侧后视镜，当超过直角边五指向左打死方向盘，继续行驶。 观察右侧后视镜，车身与右侧边缘线平行回正方向盘，继续向前驶出项目。 曲线行驶一档行车进入曲线行驶，当左侧大灯触及右侧曲线边缘线时，向左打一圈方向盘，保持左侧大灯沿着右侧边缘线行驶，直至第一个弯出来。 当左侧大灯触及左侧边缘线回正方向盘。 当右侧大灯触及左侧边缘线，当向右打一圈方向盘，保持右侧大灯贴左侧边缘线行驶，当驶出第二个弯，回正方向盘。 倒车入库。。。]]></content>
      <categories>
        <category>驾照</category>
      </categories>
      <tags>
        <tag>驾照</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本杰明巴顿奇事]]></title>
    <url>%2F2021%2F02%2F12%2F%E6%9C%AC%E6%9D%B0%E6%98%8E%E5%B7%B4%E9%A1%BF%E5%A5%87%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[一直很好奇，街上是什么样子的？ 下个街角又是怎么样？ Always had a healthy curiouly , what was up the street ? Or around the next corner ? 有时候，我感觉一切和前一天大不一样了。 Some days I feel different than the day before. 每个人在某种程度上都对自己有不同的认识，但是我们最后都会去往同一个地方，只是走的路不同罢了。 Everybody feels different about themselves , one way or another . Butwe’re all going the same way . Just taking different roads to get there ,that’s all . You’re on your own road . 有时候我觉得很可笑，那些在我们记忆里占据着很小一部分的人们，却往往给我们留下了最深刻的印象。 It’s funny how sometimes the people we remember the least , make the greatest impression on us . 我们注定要失去我们所爱的人，要不然我们怎么会知道，他们对我们有多么的重要。 We’re meant to lose the people we love . How else would we know how importantthey are to us ? 长大是件很有意思的事，不经意间就发生了，本来是这样一个人，然后突然间变成了另一个人，不再是原来那样子。 Growing up’s a funny thing , Sneak up on you . One person is there , than suddenly somebodyelse has taken here place . She wasn`t all albows and knees anymore. 你可以像疯狗那样对周围的一切愤愤不平，你可以诅咒命运，但是等到最后一刻，你还是得平静的放手而去。 You can be as mad as a mad dog at the way things went . You could swear, curse the fates . But when it comes to the end , you have to let go . 我们的生命因为各种各样的机遇而变得更有意义。 Our lives are defined by opportunities。 有些时候我们就活在即将发生冲撞的轨道上，浑然不知无论它是意外发生还是蓄谋已久，对此我们都无能为力。 Sometimes we’re on a collision course and we just don’t know it . Whether it’s by accident or by design , there’s not a thing we can do about it . 一件事无论太晚或者对于我来说太早， 都不会阻拦你成为你想成为的那个人， 这个过程没有时间的期限，只要你想，随时都可以开始， 要改变或者保留原状都无所谓， 做事本不应该有所束缚， 我们可以办好这件事却也可以把它搞砸， 但我希望最终你能成为你想成为的人。 For what it’s worth, it’s never too late, Or in my case, too early, To be whoever you want to be. There’s no time limit, stop whenever you want. You can change or stay the same. There’s no rules to this thing. We can make the best or the worst of it. I hope you make the best of it. 我希望你有时能驻足于这个令你感到惊叹的世界， 体会你从未有过的感觉； 我希望你能见到其他与你观点不同的人们； 我希望你能有一个值得自豪的人生， 如果你想象的生活不一样； 我希望你能有勇气重新再来。 I hope you see things that startle you. I hope you feel things you never felt before. I hope you meet people with a different point of view. I hope you live a life you’re proud of. If you find that you are not, I hope you have the strength to start all over again.]]></content>
      <categories>
        <category>电影台词</category>
      </categories>
      <tags>
        <tag>电影台词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派初始化设置]]></title>
    <url>%2F2021%2F02%2F10%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E5%88%9D%E5%A7%8B%E5%8C%96%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[先执行sudo apt-get install vim WIFI设置每当重新安装树莓派的系统或者初始化一块全新的树莓派都会遇到这样的问题：连接WIFI。 那当我们没有显示器和键盘的情况下怎末可以将树莓连接到当前的WIFI网络呢？ 方法非常简单，首先在SD卡的根目录下添加一个名为 wpa_supplicant.conf的文件，然后在该文件内添加以下的内容： country=CN ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=&quot;AlexVB&quot; key_mgmt=WPA-PSK psk=&quot;*********&quot; priority=1 } 密码设置树莓派很多功能都需要解锁root权限后才能完成，比如下载安装，更改文件等等。 在命令行界面执行以下命令，再输入两次想设置的root密码，执行后设置成功。 sudo passwd root 输入以下命令开启root权限： sudo passwd --unlock root 更改pi用户密码： sudo passwd pi python设置树莓派的python默认版本是python2.7，如果想用python3.x的话，树莓派有自带的python3.5，此乃大坑，不需要再下载python3，直接把python和自带的python3建立软链接就OK。 which python /usr/bin/python which python3 /usr/bin/python3 注意：不能直接就把python链接到python3上，会报错，先把原来的python2备份一下。ln: 无法创建符号链接’/usr/bin/python’: 文件已存在 sudo mv /usr/bin/python /usr/bin/python2 sudo ln -s /usr/bin/python3 /usr/bin/python 之后在终端输入python就可以看到python的默认版本改为python3.5了。在这里插入图片描述 更改软件源树莓派系统安装后默认使用国外的镜像源来更新软件，由于不可描述原因，国内访问速度非常慢，因此需要换成国内源。 树莓派官方提供了一个更新源列表，在这里我们使用中科大的软件源和系统源。 首先还是要备份原来的源文件： sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak sudo cp /etc/apt/sources.list.d/raspi.list /etc/apt/sources.list.d/raspi.list.bak 修改软件更新源，执行如下命令： sudo vim /etc/apt/sources.list 将原来的内容注释掉，改为中科大的源： deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi 在这里插入图片描述修改系统更新源，执行如下命令： sudo vim /etc/apt/sources.list.d/raspi.list 将原来的内容注释掉，改为中科大的源： deb http://mirrors.ustc.edu.cn/archive.raspberrypi.org/debian/ stretch main ui 在这里插入图片描述之后就可以快速更新和安装了。 sudo apt-get update sudo apt-get upgrade -y 这时候可能会有提示“有些没有软件包是自动安装的但是当前已经不需要了，可以使用如下命令卸载： sudo apt autoremove ！！千万别用！！更改pip源pip更换为国内源，可以大大的提高安装成功率和速度。 不管你用的是pip3还是pip，方法都是一样的，如下： sudo vim /etc/pip.conf 更改文件内容： index-url = https://mirrors.aliyun.com/pypi/simple 输入法 sudo apt-get install scim-pinyin 一、安装 1.安装mongodb sudo apt-get install mongodb 2. 启动服务 /usr/bin/mongod -f /etc/mongodb.conf 3.启动命令行 /usr/bin/mongo 此时可以看到版本是MongoDB shell version: 2.4.14 4.关闭mongo /usr/bin/mongod -shutdown -dbpath=/home/data/mongodb 二、配置 1.配置主机ip： sudo vim /etc/mongodb.conf 加入如下：127.0.0.1，自己的ip 2.配置防火墙，让局域网环境可以访问数据库 2.1 安装：sudo apt-get install ufw 2.2 启用：sudo ufw enable 2.3 禁止外部访问 sudo ufw default deny 2.4 启用局域网mongodb端口访问： sudo ufw allow from 192.168.0.0/8 三、使用 1.pymongo下载 这里注意，最新版本pymongo是不兼容MongoDB2.4的，所以下载的时候记得下载老版本 pip install pymongo==3.1.1 2.使用 使用起来非常方便：连接、关闭、增删改查。具体操作见官网文档： http://api.mongodb.com/python/current/index.html python3.7导入gevent模块报错的解决方案 RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144, got 128 return f(*args, **kwds) pip3 install -U --force-reinstall --no-binary :all: gevent 安装64位 官方64位测试版系统下载地址 https://www.raspberrypi.org/forums/viewtopic.php?f=117&amp;t=275370 换源 sudo vim /etc/apt/sources.list deb https://mirrors.tuna.tsinghua.edu.cn/debian/ buster main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ buster main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ buster-updates main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ buster-updates main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ buster-backports main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ buster-backports main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian-security buster/updates main contrib non-free deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security buster/updates main contrib non-free sudo vim /etc/apt/sources.list.d/raspi.list deb http://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ buster main ui 更新 sudo apt-get update]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[compose]]></title>
    <url>%2F2020%2F12%2F07%2Fcompose%2F</url>
    <content type="text"><![CDATA[如何写docker-compose.yml，Docker compose file 参考文档Compose 简介Compose 文件是一个YAML文件，用于定义services、netword和volumes。 Compose 文件的默认路径为./docker-compose.yaml(后缀为.yml和.yaml都可以)。 Compose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。 Compose 使用的三个步骤： 使用 Dockerfile 定义应用程序的环境。 使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行。 最后，执行 docker-compose up 命令来启动并运行整个应用程序。 安装 Docker Composesudo curl -L https://github.com/docker/compose/releases/download/1.21.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose -v 工程、服务、容器Docker Compose 将所管理的容器分为三层，分别是工程（project）、服务（service）、容器（container） Docker Compose 运行目录下的所有文件（docker-compose.yml）组成一个工程,一个工程包含多个服务，每个服务中定义了容器运行的镜像、参数、依赖，一个服务可包括多个容器实例 Docker Compose默认的模板文件是docker-compose.yml，其中定义的每个服务都必须通过image指令指定镜像或使用Dockerfile的build指令进行自动构建，其它大部分指令跟docker run中选项类似。 如果使用Dockerfile的build指令，则在Dockerfile中设置的选项如CMD、EXPOSE、VOLUME、ENV等将会自动被获取，无需在docker-comopse.yml文件中再次设置。 如果使用image指定为镜像名称或镜像ID时镜像在本地不存在，Compose将会尝试去拉取这个镜像。 典型的docker-comopse.yml文件格式 version: &apos;2&apos; services: web: image: dockercloud/hello-world ports: - 8080 networks: - front-tier - back-tier redis: image: redis links: - web networks: - back-tier lb: image: dockercloud/haproxy ports: - 80:80 links: - web networks: - front-tier - back-tier volumes: - /var/run/docker.sock:/var/run/docker.sock networks: front-tier: driver: bridge back-tier: driver: bridge docker-compose.ymlversion：指定 docker-compose.yml 文件的写法格式services：多个容器集合build：配置构建时，Compose 会利用它自动构建镜像，该值可以是一个路径，也可以是一个对象，用于指定 Dockerfile 参数 Compose文件是一个定义服务services、网络networks和卷volumes的YAML文件，默认路径是./docker-compose.yml，可使用.yml或.yaml作为文件扩展名。 服务services定义包含应用于为该服务启动的每个容器的配置，类似传递命令行参数一样docker container create。同样，网络networks和卷volumes的定义类似于docker network create和docker volume create。正如docker container create在Dockerfile指定选项，如CMD、EXPOSE、VOLUME、ENV，在默认情况下，不需要在docker-compose.yml配置中再次指定。可以使用Bash类${VARIABLE}语法在配置值中使用环境变量。 标准配置文件应该包含version、services、networks三部分，其中最关键的是services和networks两个部分。 imagesimages用来指定服务的镜像名称或镜像ID，如果镜像在本地不存在，compose将会尝试去拉取这个镜像。 services: web: image:redis volumesvolumes指令用于设置数据卷挂载路径，数据卷挂载路径可以是一个目录或一个已经存在的数据卷容器，可以设置宿主机路径HOST:CONTAINER或加上访问模式HOST:CONTAINER:ro。使用ro表示对于容器来说数据卷是只读的，这样可以有效地保护宿主机的文件系统。 volumes: # 指定一个容器内的路径，Docker会自动创建一个数据卷。 - /var/lib/mysql # 使用绝对路径挂载数据卷 - /opt/data:/var/lib/mysql # 以compose配置文件所在目录为根的相对路径作为数据卷挂载到容器 - ./cache:/tmp/cache # 使用用户的相对路径 - ~/configs:/etc/configs:ro networksnetworks指令用于设置指定网络 services: some-service: networks: - some-network 多服务version: &apos;2&apos; services: service-eureka: image: java volumes: - /Users/objcat/jar/service-eureka.jar:/usr/local/service-eureka.jar ports: - 8081:8081 command: - /bin/sh - -c - | echo 192.168.1.126 servicehost &gt;&gt; /etc/hosts java -jar /usr/local/service-eureka.jar service-a: image: java volumes: - /Users/objcat/jar/service-a.jar:/usr/local/service-a.jar ports: - 8082:8082 command: - /bin/sh - -c - | echo 192.168.1.126 servicehost &gt;&gt; /etc/hosts java -jar /usr/local/service-a.jar service-b: image: java volumes: - /Users/objcat/jar/service-b.jar:/usr/local/service-b.jar ports: - 8083:8083 command: - /bin/sh - -c - | echo 192.168.1.126 servicehost &gt;&gt; /etc/hosts java -jar /usr/local/service-b.jar]]></content>
      <categories>
        <category>compose</category>
      </categories>
      <tags>
        <tag>compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[arctic]]></title>
    <url>%2F2020%2F12%2F07%2Farctic%2F</url>
    <content type="text"><![CDATA[北极介绍北极是位于MongoDB之上的时间序列/数据框架数据库。Arctic支持将许多数据类型序列号以存储在mongo文档模型中。 为什么使用北极 序列化许多数据类型，例如Pandas DataFrames,Numpy数组,Python对象等,SO不必手动处理不同的数据类型。 默认情况下客户端使用LZ4压缩，以节省大量网络/磁盘空间。 允许对队形的不同阶段进行版本控制并快照状态（某种程度上类似于git）,并允许自由的试验,然后紧还原快照。【仅限VersionStore】 是否为您进行分块（将数据拆分为较小的部分） 添加了可以在Mongo的auth上构建的Users and Per User Libraries的概念。 拥有不同类型的商店，每种都有其自身的优势。Versionstore允许您对版本和快照内容进行版本控制，TickStore用于存储和高效检索流数据，ChunkStore允许您对数据块进行分块并有效地检索范围。 限制对Mongo的数据访问，从而防止对未索引/未分片的集合进行临时查询 基本操作Arctic提供了一个包装器,用于处理与Mongo的连接。该Arctic实际连接到北极。 conn = Arctic(&apos;127.0.0.1&apos;) 仅此连接句柄可以执行许多操作。最基本的是list_libraries和initiailize_library。 北极将数据分为不同的库。它们可能是不同的用户，不同的市场，不同的地理区域等。库名称是字符串，完全由用户定义。 conn.list_libraries() [] 在这种情况下，系统上没有库，因此可以对其进行初始化。 &gt;&gt;&gt; conn.initialize_library(&apos;library_name&apos;) &gt;&gt;&gt; conn.list_libraries() [u&apos;library_name&apos;] initialize_library有一个名为arg的可选参数，lib_type默认为VersionStore（稍后会在Arctic存储引擎类型中提供更多信息）。 库初始化后，可以像这样访问它： &gt;&gt;&gt; lib = conn[&apos;library_name&apos;] 使用此库的句柄，我们可以开始从Arctic存储和检索数据。 （注意，大多数存储引擎的支持相同的基本方法（read，write，等），但每个人都有自己一套独特的方法为好） write最基本的形式是北极symbol和数据。的symbol是，用于存储/检索数据的用户定义键。在data多数情况下是一个熊猫数据帧，虽然有些存储引擎支持其他类型的（所有支持dataframes，和一些支持类型的字典，并与pickle对象）。 read如您所料，会symbol读取数据。不同的存储引擎具有不同的参数，这些参数使您可以对数据进行子集设置（稍后会详细介绍）。 &gt;&gt;&gt; data = pd.DataFrame(.....) &gt;&gt;&gt; lib.write(&apos;symbolname&apos;, data) &gt;&gt;&gt; df = lib.read(&apos;symbolname&apos;) &gt;&gt;&gt; df data date 2016-01-01 1 2016-01-02 2 2016-01-03 3 其他基本方法： library.list_symbols() 符合您的期望-列出给定库中的所有符号 [‘US_EQUITIES’, ‘EUR_EQUITIES’, …] arctic.get_quota(library_name)， arctic.set_quota(library_name, quota_in_bytes) 北极内部设置库的配额，这样它们就不会占用太多空间。您可以使用这两种方法检查和设置配额。请注意，这些操作针对 Arctic对象，而非库。 北极存储引擎Arctic的设计具有很高的可扩展性，目前支持多种不同的用例。要了解Arctic的功能，必须了解其使用的存储模型。北极目前支持三个存储引擎 TickStore 版本库 块存储 每一个都有各种功能，旨在支持特定和通用的用例。 快速开始安装北极pip install git+https://github.com/manahl/arctic.git 运行一个mongomongod --dbpath &lt;path/to/db_directory&gt; 使用VersionStorefrom arctic import Arctic import quandl # Connect to Local MONGODB store = Arctic(&apos;localhost&apos;) # Create the library - defaults to VersionStore store.initialize_library(&apos;NASDAQ&apos;) # Access the library library = store[&apos;NASDAQ&apos;] # Load some data - maybe from Quandl aapl = quandl.get(&quot;WIKI/AAPL&quot;, authtoken=&quot;your token here&quot;) # Store the data in the library library.write(&apos;AAPL&apos;, aapl, metadata={&apos;source&apos;: &apos;Quandl&apos;}) # Reading the data item = library.read(&apos;AAPL&apos;) aapl = item.data metadata = item.metadata 存储引擎版本库VersionStore在MongoDB中序列化并存储Pandas对象，numpy数组以及其他python类型。修改versioneda时，对象是并且创建了新版本symbol。 使用VersionStore读写数据from arctic import Arctic a = Arctic(‘localhost’) a.initialize_library(&apos;vstore&apos;) lib = a[‘vstore’] 此时，您有一个空的VersionStore库。您不需要指定存储类型，因为VersionStore是Arctic中的默认库类型。您可以通过几种方式向其中写入数据。最基本的就是使用该write方法。写采用以下参数： symbol, data, metadata=None, prune_previous_version=True, **kwargs symbol是用于在北极存储/检索数据的名称。data是要存储在MongoDB中的数据。metadata是可选的用户定义元数据。它一定是一个dict。prune_previous_versions将删除/删除数据的先前版本（前提是快照中未包含它们）。kwargs被传递给各个写处理程序。有针对不同数据类型的写处理程序。 write用于写入和替换数据。如果test使用一个数据集写入符号，然后使用另一个数据集再次写入符号，则原始数据将被替换为新版本的数据。 &gt;&gt;&gt; from pandas import DataFrame, MultiIndex &gt;&gt;&gt; from datetime import datetime as dt &gt;&gt;&gt; df = DataFrame(data={&apos;data&apos;: [1, 2, 3]}, index=MultiIndex.from_tuples([(dt(2016, 1, 1), 1), (dt(2016, 1, 2), 1), (dt(2016, 1, 3), 1)], names=[&apos;date&apos;, &apos;id&apos;])) &gt;&gt;&gt; lib.write(&apos;test&apos;, df) VersionedItem(symbol=test,library=arctic.vstore,data=&lt;class &apos;NoneType&apos;&gt;,version=1,metadata=None,host=127.0.0.1) &gt;&gt;&gt; lib.read(&apos;test&apos;).data data date id 2016-01-01 1 1 2016-01-02 1 2 2016-01-03 1 3 &gt;&gt;&gt; df = DataFrame(data={&apos;data&apos;: [100, 200, 300]}, index=MultiIndex.from_tuples([(dt(2016, 1, 1), 1), (dt(2016, 1, 2), 1), (dt(2016, 1, 3), 1)], names=[&apos;date&apos;, &apos;id&apos;])) &gt;&gt;&gt; lib.write(&apos;test&apos;, df) VersionedItem(symbol=test,library=arctic.vstore,data=&lt;class &apos;NoneType&apos;&gt;,version=2,metadata=None,host=127.0.0.1) &gt;&gt;&gt; lib.read(&apos;test&apos;).data data date id 2016-01-01 1 100 2016-01-02 1 200 2016-01-03 1 300 write返回一个VersionedItem对象。VersionedItem包含以下成员： 符号图书馆版本-写入数据的版本号元数据-元数据（如果存在），或无数据（对于写入，它为None；对于读取，它包含从数据库读取的数据）。主办您还应注意，VersionStorewrite有效覆盖了已写入的数据。在上面的示例中，第二个示例用新数据帧write替换了符号test中的数据。原始数据（版本1）仍然可用，但必须以其版本号引用才能检索它。该read方法采用以下参数： symbol, as_of=None, date_range=None, from_version=None, allow_secondary=None, **kwargs as_of允许您检索特定时间点的数据。您可以通过多种方式定义该时间点。 快照的名称（字符串） 版本号（整数） 日期时间（datetime.datetime） date_range使您可以通过Arctic DateRange对象对数据进行子集化。DateRanges允许您指定日期范围（“ 2016-01-01”，“ 2016-09-30”），开始日期和结束日期以及开放式范围（无，“ 2016-09-30”）。范围可以在任一端打开。allow_secondary使您可以覆盖默认行为，以允许或禁止从mongo集群的辅助成员读取。 &gt;&gt;&gt; lib.read(&apos;test&apos;).data data date id 2016-01-01 1 100 2016-01-02 1 200 2016-01-03 1 300 &gt;&gt;&gt; lib.read(&apos;test&apos;, as_of=1).data date id data 2016-01-01 1 1 2016-01-02 1 2 2016-01-03 1 3 &gt;&gt;&gt; from arctic.date import DateRange &gt;&gt;&gt; lib.read(&apos;test&apos;, date_range=DateRange(&apos;2016-01-01&apos;, &apos;2016-01-01&apos;)).data date id data 2016-01-01 1 100 DateRange仅适用于pandas DataFrame，并且该数据帧必须具有datetime索引。 写入数据的另一种方法是使用append方法。append接受以下参数： symbol, data, metadata=None, prune_previous_version=True, upsert=True, **kwargs upsert是唯一的新论据。upsert表示如果该符号不存在，它将创建它。如果upsert出现False错误，因为将没有现有数据要追加。 &gt;&gt;&gt; lib.append(&apos;new&apos;, df, upsert=False) ~/arctic/arctic/store/version_store.py in append(self, symbol, data, metadata, prune_previous_version, upsert, **kwargs) 505 return self.write(symbol=symbol, data=data, prune_previous_version=prune_previous_version, metadata=metadata) 506 --&gt; 507 assert previous_version is not None 508 dirty_append = False 509 AssertionError: lib.append(‘new’, df, upsert=True)VersionedItem(symbol=new,library=arctic.vstore,data=,version=1,metadata=None,host=127.0.0.1) 实用方法 删除 has_symbol list_versions read_metadata write_metadata restore_version delete符合您的预期-从库中删除符号。它只有一个参数symbol。has_symbol并且list_symbols将返回有关库符号的当前状态信息。他们的签名是： list_symbols(self, all_symbols=False, snapshot=None, regex=None, **kwargs) def has_symbol(self, symbol, as_of=None) for list_symbols，all_symbols如果设置为，则将true从所有快照返回所有符号，即使该符号已在当前版本中删除（但已保存在快照中）也是如此。snapshot允许您在指定的下方列出符号snapshot。regex允许您提供正则表达式以进一步限制查询返回的符号列表。北极使用MongoDB的$regex功能。Mongo支持PERL语法正则表达式；更多信息在这里 has_symbol返回True或False基于符号是否存在。您可以version通过将此检查限制为特定检查as_of。 &gt;&gt;&gt; lib.delete(&apos;new&apos;) &gt;&gt;&gt; lib.list_symbols() [&apos;test&apos;] &gt;&gt;&gt; lib.has_symbol(&apos;new&apos;) False &gt;&gt;&gt; lib.write(&apos;test2&apos;, df) &gt;&gt;&gt; lib.list_symbols(regex=&quot;.*2&quot;) [&apos;test2&apos;] read_metadata并write_metadata允许您直接为给定符号读取/设置用户定义的元数据。 &gt;&gt;&gt; lib.read_metadata(&apos;test2&apos;) VersionedItem(symbol=test2,library=arctic.vstore,data=&lt;class &apos;NoneType&apos;&gt;,version=1,metadata=None,host=127.0.0.1) &gt;&gt;&gt; lib.read_metadata(&apos;test2&apos;).metadata &gt;&gt;&gt; lib.write_metadata(&apos;test2&apos;, {&apos;meta&apos;: &apos;data&apos;}) VersionedItem(symbol=test2,library=arctic.vstore,data=&lt;class &apos;NoneType&apos;&gt;,version=2,metadata={&apos;meta&apos;: &apos;data&apos;},host=127.0.0.1) &gt;&gt;&gt; lib.read_metadata(&apos;test2&apos;).metadata {&apos;meta&apos;: &apos;data&apos;} restore_version使您可以将最新版本设置为旧版本。您可以list_versions用来查看有关版本当前状态的信息。 &gt;&gt;&gt; lib.list_versions(&apos;test&apos;) [{&apos;symbol&apos;: &apos;test&apos;, &apos;version&apos;: 3, &apos;deleted&apos;: False, &apos;date&apos;: datetime.datetime(2018, 8, 16, 17, 59, 47, tzinfo=tzfile(&apos;/usr/share/zoneinfo/America/New_York&apos;)), &apos;snapshots&apos;: []}, {&apos;symbol&apos;: &apos;test&apos;, &apos;version&apos;: 2, &apos;deleted&apos;: False, &apos;date&apos;: datetime.datetime(2018, 8, 15, 18, 0, 33, tzinfo=tzfile(&apos;/usr/share/zoneinfo/America/New_York&apos;)), &apos;snapshots&apos;: []}] &gt;&gt;&gt; lib.restore_version(&apos;test&apos;, 2) VersionedItem(symbol=test,library=arctic.vstore,data=&lt;class &apos;NoneType&apos;&gt;,version=4,metadata=None,host=127.0.0.1) &gt;&gt;&gt; lib.read(&apos;test&apos;).data data date id 2016-01-01 1 100 2016-01-02 1 200 2016-01-03 1 300 &gt;&gt;&gt; lib.list_versions(&apos;test&apos;) [{&apos;symbol&apos;: &apos;test&apos;, &apos;version&apos;: 4, &apos;deleted&apos;: False, &apos;date&apos;: datetime.datetime(2018, 8, 16, 18, 54, 10, tzinfo=tzfile(&apos;/usr/share/zoneinfo/America/New_York&apos;)), &apos;snapshots&apos;: []}, {&apos;symbol&apos;: &apos;test&apos;, &apos;version&apos;: 3, &apos;deleted&apos;: False, &apos;date&apos;: datetime.datetime(2018, 8, 16, 17, 59, 47, tzinfo=tzfile(&apos;/usr/share/zoneinfo/America/New_York&apos;)), &apos;snapshots&apos;: []}, {&apos;symbol&apos;: &apos;test&apos;, &apos;version&apos;: 2, &apos;deleted&apos;: False, &apos;date&apos;: datetime.datetime(2018, 8, 15, 18, 0, 33, tzinfo=tzfile(&apos;/usr/share/zoneinfo/America/New_York&apos;)), &apos;snapshots&apos;: []}] 使用restore_version不会删除最新版本，它只是使用用户提供的版本引用的数据创建了一个新版本。 快照VersionStore允许您创建数据快照并为其分配名称。快照中仍包含属于快照的一部分的数据。快照方法是： 快照 delete_snapshot list_snapshots snapshot允许您创建快照。它的签名很简单 snap_name, metadata=None, skip_symbols=None, versions=None snap_name是要创建的快照的名称。metadata允许您向快照提供用户定义的元数据。skip_symbols允许您从快照中排除符号。versions允许您指定要包含在快照中的特定版本。 delete_snapshot和list_snapshot功能分别类似于delete和list_versions。list_snapshots返回snapshot映射到metadata快照的名称的字典。 &gt;&gt;&gt; lib.list_symbols() [&apos;test&apos;, &apos;test2&apos;] &gt;&gt;&gt; lib.snapshot(&apos;backup&apos;) &gt;&gt;&gt; lib.list_snapshots() {&apos;backup&apos;: None} &gt;&gt;&gt; lib.list_symbols(snapshot=&apos;backup&apos;) [&apos;test&apos;, &apos;test2&apos;] &gt;&gt;&gt; lib.delete(&apos;test&apos;) &gt;&gt;&gt; lib.delete(&apos;test2&apos;) &gt;&gt;&gt; lib.list_symbols() [] &gt;&gt;&gt; lib.list_symbols(snapshot=&apos;backup&apos;) [&apos;test&apos;, &apos;test2&apos;] &gt;&gt;&gt; lib.read(&apos;test&apos;) ~/arctic/arctic/store/version_store.py in _read_metadata(self, symbol, as_of, read_preference) 455 metadata = _version.get(&apos;metadata&apos;, None) 456 if metadata is not None and metadata.get(&apos;deleted&apos;, False) is True: --&gt; 457 raise NoDataFoundException(&quot;No data found for %s in library %s&quot; % (symbol, self._arctic_lib.get_name())) 458 459 return _version NoDataFoundException: No data found for test in library arctic.vstore &gt;&gt;&gt; lib.read(&apos;test&apos;, as_of=&apos;backup&apos;) VersionedItem(symbol=test,library=arctic.vstore,data=&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;,version=4,metadata=None,host=127.0.0.1) &gt;&gt;&gt; lib.read(&apos;test&apos;, as_of=&apos;backup&apos;).data data date id 2016-01-01 1 100 2016-01-02 1 200 2016-01-03 1 300 API参考附加 append(symbol, item) Appends data from item to symbol&apos;s data in the database. Is not idempotent Parameters ---------- symbol: str the symbol for the given item in the DB item: DataFrame or Series the data to append 用法示例： &gt;&gt;&gt; df = DataFrame(data={&apos;data&apos;: [100, 200, 300]}, index=MultiIndex.from_tuples([(dt(2016, 1, 1), 1), (dt(2016, 1, 2), 1), (dt(2016, 1, 3), 1)], names=[&apos;date&apos;, &apos;id&apos;])) &gt;&gt;&gt; lib.write(&apos;test&apos;, df, chunk_size=&apos;M&apos;) &gt;&gt;&gt; lib.read(&apos;test&apos;) data date id 2016-01-01 1 100 2016-01-02 1 200 2016-01-03 1 300 &gt;&gt;&gt; lib.append(&apos;test&apos;, df) &gt;&gt;&gt; lib.append(&apos;test&apos;, df) &gt;&gt;&gt; lib.read(&apos;test&apos;) data date id 2016-01-01 1 100 1 100 1 100 2016-01-02 1 200 1 200 1 200 2016-01-03 1 300 1 300 1 300 删除 delete(symbol, chunk_range=None) Delete all chunks for a symbol, or optionally, chunks within a range Parameters ---------- symbol : str symbol name for the item chunk_range: range object a date range to delete 用法示例： &gt;&gt;&gt; lib.read(&apos;test&apos;) data date id 2016-01-01 1 100 2016-01-03 1 300 &gt;&gt;&gt; lib.delete(&apos;test&apos;, chunk_range=pd.date_range(&apos;2016-01-01&apos;, &apos;2016-01-01&apos;)) &gt;&gt;&gt; lib.read(&apos;test&apos;) data date id 2016-01-03 1 300 get_chunk_ranges get_chunk_ranges(symbol, chunk_range=None, reverse=False) Returns a generator of (Start, End) tuples for each chunk in the symbol Parameters ---------- symbol: str the symbol for the given item in the DB chunk_range: None, or a range object allows you to subset the chunks by range reverse: boolean Returns ------- generator 用法示例： &gt;&gt;&gt; list(lib.get_chunk_ranges(&apos;new_name&apos;)) [(&apos;2016-01-01&apos;, &apos;2016-01-01&apos;), (&apos;2016-01-03&apos;, &apos;2016-01-03&apos;)] 获取信息 get_info(symbol) Returns information about the symbol, in a dictionary Parameters ---------- symbol: str the symbol for the given item in the DB Returns ------- dictionary 迭代器 iterator(symbol, chunk_range=None): Returns a generator that accesses each chunk in ascending order Parameters ---------- symbol: str the symbol for the given item in the DB chunk_range: None, or a range object allows you to subset the chunks by range Returns ------- generator list_symbols list_symbols() Returns all symbols in the library Returns ------- list of str 读 read(symbol, chunk_range=None, filter_data=True, **kwargs) Reads data for a given symbol from the database. Parameters ---------- symbol: str the symbol to retrieve chunk_range: object corresponding range object for the specified chunker (for DateChunker it is a DateRange object or a DatetimeIndex, as returned by pandas.date_range filter_data: boolean perform chunk level filtering on the data (see filter in _chunker) only applicable when chunk_range is specified kwargs: ? values passed to the serializer. Varies by serializer Returns ------- DataFrame or Series 用法示例： &gt;&gt;&gt; dr = pd.date_range(start=&apos;2010-01-01&apos;, periods=1000, freq=&apos;D&apos;) &gt;&gt;&gt; df = DataFrame(data={&apos;data&apos;: np.random.randint(0, 100, size=1000), &apos;date&apos;: dr }) &gt;&gt;&gt; lib.write(&apos;symbol_name&apos;, df, chunk_size=&apos;M&apos;) &gt;&gt;&gt; lib.read(&apos;symbol_name&apos;, chunk_range=pd.date_range(&apos;2012-09-01&apos;, &apos;2016-01-01&apos;)) data date 0 61 2012-09-01 1 69 2012-09-02 2 96 2012-09-03 3 23 2012-09-04 4 66 2012-09-05 5 54 2012-09-06 6 21 2012-09-07 7 92 2012-09-08 8 95 2012-09-09 9 24 2012-09-10 10 87 2012-09-11 11 33 2012-09-12 12 59 2012-09-13 13 54 2012-09-14 14 48 2012-09-15 15 67 2012-09-16 16 73 2012-09-17 17 72 2012-09-18 18 6 2012-09-19 19 24 2012-09-20 20 8 2012-09-21 21 50 2012-09-22 22 40 2012-09-23 23 45 2012-09-24 24 8 2012-09-25 25 73 2012-09-26 改名 rename(from_symbol, to_symbol) Rename a symbol Parameters ---------- from_symbol: str the existing symbol that will be renamed to_symbol: str the new symbol name reverse_iterator reverse_iterator(symbol, chunk_range=None): Returns a generator that accesses each chunk in descending order Parameters ---------- symbol: str the symbol for the given item in the DB chunk_range: None, or a range object allows you to subset the chunks by range Returns ------- generator 更新 update(symbol, item, chunk_range=None, upsert=False, **kwargs) Overwrites data in DB with data in item for the given symbol. Is idempotent Parameters ---------- symbol: str the symbol for the given item in the DB item: DataFrame or Series the data to update chunk_range: None, or a range object If a range is specified, it will clear/delete the data within the range and overwrite it with the data in item. This allows the user to update with data that might only be a subset of the original data. upsert: bool if True, will write the data even if the symbol does not exist. kwargs: optional keyword args passed to write during an upsert. Includes: chunk_size chunker 写 write(symbol, item, chunker=DateChunker(), **kwargs) Writes data from item to symbol in the database Parameters ---------- symbol: str the symbol that will be used to reference the written data item: Dataframe or Series the data to write the database chunker: Object of type Chunker A chunker that chunks the data in item kwargs: optional keyword args that are passed to the chunker. Includes: chunk_size: used by chunker to break data into discrete chunks. see specific chunkers for more information about this param. 用法示例： &gt;&gt;&gt; dr = pd.date_range(start=&apos;2010-01-01&apos;, periods=1000, freq=&apos;D&apos;) &gt;&gt;&gt; df = DataFrame(data={&apos;data&apos;: np.random.randint(0, 100, size=1000), &apos;date&apos;: dr }) &gt;&gt;&gt; lib.write(&apos;symbol_name&apos;, df, chunk_size=&apos;M&apos;)]]></content>
      <categories>
        <category>arctic</category>
      </categories>
      <tags>
        <tag>arctic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[量化投资]]></title>
    <url>%2F2020%2F11%2F30%2F%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84%2F</url>
    <content type="text"><![CDATA[关于量化投资的学习和思考 数学角度看量化在量化投资单纯从数学角度看，一个交易系统/交易模型仅仅是一个从行情序列到资金曲线的映射。 f(ts,para) = E 其中f是一个交易系统，ts是某一个投资标的（股票、期货、期权、外汇等）的行情时间序列para是交易系统的参数组，E是资金曲线。 任何一个模型都会有参数，有参数就会碰到参数寻优，过拟合等问题。 参数优化是基于历史数据进行的策略优化。而历史并不能完全重演。历史中表现优秀的参数，在交易中未必会表现很好。]]></content>
      <categories>
        <category>投资</category>
      </categories>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRINCIPLES]]></title>
    <url>%2F2020%2F10%2F29%2FPRINCIPLES%2F</url>
    <content type="text"><![CDATA[介绍《原则》全书一共分为三个部分 ： 第一部分是个人经历的介绍；第二部分是生活原则的描述；第三部分是工作原则的描述。 生活原则拥抱现实；应对现实 不要混淆愿望与现实 不要担心自己形象；而关心能不能实现你的目标 要重视后续的结果以及后续的后续 不要让痛苦妨碍自己的进步 不要把不好归咎与任何人；从自己身上找原因 做一个超级现实的人在乎梦想；但是扎根于现实 梦想 + 现实 + 决心 = 成功的生活 理解现实；是任何良好结果的根本依据因为当真相与愿望不符的时候；大部分人抗拒真相。 这种认知偏差；有很多心理学现象与之相关；如房间里的大象等。在《需求》 和 《思考快与慢中》都有过相关的论述。说到底；就是要扎根现实。理解现实，然后，改变自己；改变现实 头脑需要极度开放头脑极度开放；极度透明有利于快速学习与改进这几乎是整个生活原则中最重要的一条原则了你头脑越是开放，越是不会自欺欺人。其他人给你的反馈就会更加诚实（“你如果是喜欢听与自己意见一样的话；就会得到的反馈就越是虚假”） 如果这些人是可信的人 ；你将会收获很多。 不要担心其他人的看法；那会成为你的障碍你必须以你认为最好的独特的方式行事；这样做一定会收到反馈；我们必须以开放的头脑思考这些反馈。 尽管你的极度透明会让你感觉到不舒服；但是这样对你是最好的。 极度求真，极度透明会带来更加有意义的工作和更好的人际关系 观察自然；学习现实规律两种视角： 自上而下 找事务背后规律： 理解市场；供求关系；宏观大势 自下而上去验证具体情况 ： 验证具体情况是否相符合 不要固守你认为事物 “应该” 是什么样子保持客观： 不要让偏见使我们无法认识客观情况。 当看到一个认为自然是错误的想法的时候，先假设自己是错的，然后想办法弄明白为什么自然如此是合理的。一个例子： 大草原上鬣狗吃了小幼马，我们会心生同情。但是事实上这是自然法则，自然会走向整体最优，而不是个体最优化。 人也是一样。人们会把对自己或者自己相关的人不利的事情叫做坏事，而忽略更大的好。 群体中也有这种倾向，如宗教歧视。 只根据事务对个人的影响就判断绝对的好与坏是不合理的。（这点很难做到） 一个东西要好，必须符合现实规律，并促进整体的进化。这会带来更大的好。 进化是宇宙中最强大的力量 这里推荐一本书：《基因之河》； 关于进化：还有一本书，《自私的基因》， 也非常开脑洞。 进化是生命最大的成就和回报个人激励机制必须符合群体目标。比如自然给了性行为个体巨大的快乐激励，来达到群体的不断进化迭代。 现实为了趋向整体最大化，而不是个体。为了整体做贡献，你就有可能收到回报。自然选择让更好产品得以保留，结果是整体的最优化。 通过快速试错以适应现实是无价的。实验和适应能带来更快的进步。 要意识到你即是一切，又什么都不是，并决定你想成为什么样子。“个人即是一切，又什么都不是，这是一个巨大的悖论。” 你的未来取决于你的视角。你的未来取决于你的如何看待事物，关心什么事物。必须决定你多大程度将别人的利益放在你的自己利益之上。拥抱现实，从自然的角度俯视自身很美妙。 理解自然提供的现实教训收益递减规律。 任何东西在从太少变太多的过程中，边际收益都会递减。 没有痛苦就没有收获。 “人需要困难，这对健康来说是必须的。” 痛苦 + 反思 = 进步如果以正确的态度面对痛苦，感到痛苦就是你的幸运！！即使反思痛苦。 最好的就是在痛苦的当时就进行反思。 考虑后续与再后续的结果直接结果很可能是诱惑或者痛苦。 如果因为直接结果的痛苦而不去做，就很难获得大的成功。 如果因为直接结果的诱惑而去做了，就会遭遇更大的失败。 接受结果：内控点在生活中不论遇到什么情况，如果你能够负起责任，进行良好的决策，而不是抱怨你无法控制的事情，你将更加容易知找到幸福。 不要为喜不喜欢自己的处境担忧，你必须根据自己的愿望找到实现愿望的途径。然后，鼓起勇气坚持下去。 后面的五步流程会给到你一定帮助。 从更高的层次腐蚀机器想象自己是一个大机器里的一个小机器，拥有改变自己而变得更好的能力。 通过比较你实现的结果和你的目标，你就能确定如何改进你的机器。 却别作为机器设计者的你和作为机器中工作者的你。 最难的事情是在自身所处的环境中，客观的看待我们自己，不高看自己，不承担不应该承担的任务。 擅于请教领域达人，因为你很难客观看待自己，所以你需要依赖他人的意见以及证据。 如果你头脑开放，足够有决心，你几乎可以实现任何愿望。 五部流程，实现你的人生愿望 明确目标 找到阻碍目标的问题，并且不容忍问题 准确诊断问题，找到问题根源 规划可以解决问题的方案 做一切必要的事儿来践行这些方案，实现成果 这五个步骤形成一个循环。 需要注意的点是： 专注每个点。设定目标的时候就设定目标，不要想实现和出错（延迟批判）。 当你诊断问题的时候，就不要想如何解决问题，混淆这两个问题会导致你无法发现真正的问题。 坚持这些规则。挫折，会让你难受，不完美永远存在。好消息是你可以从错误中吸收学习和成长。 坚持下去你就会有收获。 有明确的目标排列目标优先级。你几乎可以得到你想要的任何东西，但你无法得到所有东西。 分清目标和欲望 目标是你真正需要的东西 欲望是你想要但会阻碍你实现目标的东西（比如偷懒） 不要因为某个目标无法实现就否定它 伟大的期望创造伟大的能力 拥有灵活性和自我归咎，那没什么能阻止你 知道如何应对挫折很重要 逆境中，重要的是守住优势，减少损失 你的任务永远是做出尽可能少的选择 成功不难，关键在于少犯错误 找出问题但是不容忍令人痛苦的问题当做考验你潜在进步的机会 当你遇到一个问题，那就是一个机会。 大多数人不喜欢这么做 不要逃避问题，承认问题是改变的第一步 忍痛前行，痛苦会给你汇报 不要把某个原因当做问题本身 我无法得到很好的睡眠是一个原因，我工作效率低是一个问题，前者可能是后者的原因。 重点解决大问题 找到问题根源弄清楚问题，这需要时间去诊断，一次良好的诊断一般需要15-60分钟。 区分直接与根本原因 规划方案前进之前先回顾 设置方案，写下来所有人都能看到，严格执行 规划先于行动，好规划不一定要很多时间 坚定的从头到尾执行方案保持谦逊与其他人高质量交流 保持谦逊，你可以从别人那里得到你需要的东西 找到你最大的弱点，并处理掉 理解自己的认知，理解他人与你不同提升认知能力，保持头脑开放，从他人那里获得帮助，你可以实现很多事情 做到头脑极度开放这一章几乎是生活原则中最重要的一章。(雷达里奥说；这也是全书最重要的一章)主要重点有两个： 一个是为什么要保持头脑开放；一个是什么情况下你是头脑封闭的。认识到第二点其实很重要，这会帮助你的日常反思。 认识到自己的障碍和不足； 寻求可信度高的人的意见，设身处地思考和理解，对比自己的，最终做出更好的决策。 认识你的两大障碍障碍一：意识障碍；理解你的自我意识障碍 主要是你潜意识里的防备机制； 使你难以接受你的错误和缺点。 我们有一些根植于内心的需求： 被爱 被需要 害怕死亡 害怕失去 害怕自己无意义 不能让“想要自己正确的需求 ” 压倒 “找出真相” 的需求 当有人和你意见不一样；并且要求你解释的时候；你的大脑会把这样的东西当做 攻击；你会变得愤怒。 如果你想要成功；你需要克制这一点。 这样的人你也可以观察一下；身边到处都是。你也可以反思一下；你自己是不是这样的人。（怎么判断自己有这种倾向在后面会说到） 障碍二；思维障碍；理解你的思维盲点障碍 人很难理解自己看不到的东西；《需求》 这本书里也有说到 塞缪尔思反射 。 如果你一心只想告诉对方自己的认为正确的想法；你就是 一个头脑封闭的人 这样的话；当其他人给你展示各种可能性威胁和批评的时候；你可能会看不见。也无法领会。 奉行头脑极度开放；不仅仅是“承认自己可能错了”如果你知道自己有盲点；你就能找到一种解决办法。 头脑开放不仅仅是“承认自己可能错了”，但是依旧坚持自己的观点。这样作用不大。 a. 诚恳的想想自己也许并不知道最好的解决办法是什么。能不能妥善处理“不知道”很重要很多糟糕的决定是因为他们相信自己是对的。而头脑极度开放的人知道。找到问题的答案很重要。 b. 决策有两个步骤： 1 分析所有相关信息；2 决策听听其他人的观点并加以思考，不会削弱你独立思考，自主决策的自由。只会帮你拥有更广的角度 c. 不要担心你的形象，只担心如何实现目标做出优秀决策的人，很少坚信自己已经掌握了最好的答案，承认自己有缺点 和 盲点 ，并试图了解更多，克服缺陷和盲点。 d 不吸收，产出也不大好。 e 从他人的角度，设身处地，才能评估另一种观点的价值高度接受自己错了的可能性，鼓励别人告诉自己错在哪里 f 记住：你是在寻找最好的答案，不是自己能得出最好的答案知道自己不知道，无比重要。 自问一下，我是不是只是从自己的角度看问题 g 搞清楚你是在争论还是在试图理解一个问题，根据可信度，想想哪种更加合理 可信度 有两个特征： 反复的在相关领域成功找到答案（至少三次，拥有硬履历） 再被责问的情况下能对自己的观点做出很好的解释 领会并感激：深思熟虑的意见分歧沟通方式要让对方觉得，你是试图在理解 你需要提问，而不是做出陈述，心平气和的进行讨论，并鼓励对方也这么做。 （某些时候）人们在产生分歧时变得愤怒是毫无意义的 当讨论陷入僵局，最没效果的就是，你试图在脑子中将所有的事情都弄得清楚。 和可信的，愿意表达分歧的人一起审视你的观点既单独询问专家，也鼓励专家在我面前展现意见分歧。 为最坏的做准备。使其看起来不那么糟糕。 （重要）识别你头脑封闭的迹象a 封闭的人：不喜欢看到自己的观点被挑战 不开放：会因为无法说服他人而沮丧，而不是好奇对方为何看法不同。 开放： 更想了解为什么会有分歧，明白自己可能是错的 b 封闭的人：喜欢做陈述而不是提问 开放的人，可信度很高的人，经常会提出很多问题。并真诚的相信自己可能是错的 c 封闭的人: 更关心自己是否被理解，而不是理解他人 封闭的人： 通常担心自己没有被理解 开放的人：觉得有必要从他人的视角看问题。 d 封闭的人：“我可能错了。。。但这是我的观点” 这是一个敷衍的回答，人们借此来固守自己的观点 最好提出一个问题，而不是做出一个断言 e 封闭的人 ： 封闭的人，阻挠别人的发言 开放的人更喜欢倾听发言，鼓励表达 f 封闭的人 : 很难同时拥有两种想法 同时持有两种想法，并且能保持独立思考。 g 封闭的人 ： 缺乏谦逊 开放的人： 时刻担忧自己可能是错误的。 如何做到头脑开放？ 利用自己的痛苦进行高质量的思考 一旦觉得愤怒，冷静下来，以深思熟虑的方式看待眼前问题 一定要客观，愿意倾听 重视证据 冥想 理解人与人大大不相同要理解：左脑思考偏逻辑，右脑思考偏情感。 要理解：最长发生的斗争就是意识与潜意识，情绪和思考的斗争。 如果你意识不到你的潜意识的存在，你的行为就会像西奥迪尼在《影响力》中做的那个比喻一样：是一个带着按钮的录音机；一按就播放。 比如：听到别人反对时候的被侵犯感。 要知道：我们可以改变，通过习惯。 如何做出正确的决策好决策最大的敌人是坏情绪 如果你被情绪绑架，你将不可能作出好的决策。作出决策时候必须用逻辑，理性，事实。 正如荣格所说：“如果你不知道潜意识的存在，否则潜意识就会主导你的人生，而你称之为命运” 先了解，后决策 是什么 ：先了解决策的基础知识，既包括“是什么”，也包括宏观的因果关系 “习惯性的问自己，我在了解相关情况吗？我已经掌握了决策的所有知识了么？” 为了了解： 要知道应该问什么人 不要高估自己的可信度 不要不区分别人的可信度（在相同领域有过3次以上成功经验的硬简历） 区分事实和观点，不要听到什么信什么，别人说的和做的很可能不一样。 80/20原则：你从20%的信息获得80%的价值，明白关键性的20%是什么 不要完美主义，完美主义的边际效用是 递减的 怎么做 ：权衡结果，考虑结果，后续的结果，后续的后续的结果]]></content>
      <categories>
        <category>财务</category>
      </categories>
      <tags>
        <tag>财务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[backtrader]]></title>
    <url>%2F2020%2F09%2F05%2Fbacktrader%2F</url>
    <content type="text"><![CDATA[Backtrader引言前基于Python的量化回测框架有很多，开源框架有zipline、vnpy、pyalgotrader和backtrader等，而量化平台有Quantopian（国外）、聚宽、万矿、优矿、米筐、掘金等，这些量化框架或平台各有优劣。就个人而言，比较偏好用backtrader，因为它功能十分完善，有完整的使用文档，安装相对简单（直接pip安装即可）。优点是运行速度快，支持pandas的矢量运算；支持参数自动寻优运算，内置了talib股票分析技术指标库；支持多品种、多策略、多周期的回测和交易；支持pyflio、empyrica分析模块库、alphalens多因子分析模块库等；扩展灵活，可以集成TensorFlow、PyTorch和Keras等机器学习、神经网络分析模块。而不足之处在于，backtrader学习起来相对复杂，编程过程中使用了大量的元编程（类class），如果Python编程基础不扎实（尤其是类的操作），学起来会感到吃力。本文作为backtrader的入门系列之一，对其运行框架进行简要介绍，并以实际案例展示量化回测的过程。 Backtrader介绍如果将backtrader包分解为核心组件，主要包括以下组成部分： 数据加载（Data Feed）：将交易策略的数据加载到回测框架中。 交易策略（Strategy）：该模块是编程过程中最复杂的部分，需要设计交易决策，得出买入/卖出信号。 回测框架设置（ Cerebro）：需要设置（i）初始资金（ii）佣金（iii）数据馈送（iv）交易策略交易头寸大小。 运行回测：运行Cerebro回测并打印出所有已执行的交易。 评估性能（Analyzers）:以图形和风险收益等指标对交易策略的回测结果进行评价。 “Lines”是backtrader回测的数据，由一系列的点组成，通常包括以下类别的数据：Open（开盘价）, High（最高价）, Low（最低价）, Close（收盘价）, Volume（成交量）, OpenInterest（无的话设置为0）。Data Feeds（数据加载）、Indicators（技术指标）和Strategies（策略）都会生成 Lines。价格数据中的所有”Open” (开盘价)按时间组成一条 Line。所以，一组含有以上6个类别的价格数据，共有6条 Lines。如果算上“DateTime”（时间，可以看作是一组数据的主键），一共有7条 Lines。当访问一条 Line 的数据时，会默认指向下标为 0 的数据。最后一个数据通过下标 -1 来访问，在-1之后是索引0，用于访问当前时刻。因此，在回测过程中，无需知道已经处理了多少条/分钟/天/月，”0”一直指向当前值，下标 -1 来访问最后一个值。 Backtrader环境搭建 安装python环境 (anaconda) pip install backtrader[plotting] 新建jupyterProject文件夹，在其路径栏输入 jupyter lab,按enter键,等待启用jupyter 回测应用实例量化回测说白了是使用历史数据去验证交易策略的性能，因此回测的第一步是搭建交易策略，这也是backtrader要设置的最重要和复杂的部分，策略设定好后，其余部分的代码编写是手到擒来。 构建策略（Strategy）交易策略类代码包含重要的参数和用于执行策略的功能，要定义的参数或函数名如下： （1）params-全局参数，可选：更改交易策略中变量/参数的值，可用于参数调优。 （2）log：日志，可选：记录策略的执行日志，可以打印出该函数提供的日期时间和txt变量。 （3） init：用于初始化交易策略的类实例的代码。 （4）notify_order，可选：跟踪交易指令（order）的状态。order具有提交，接受，买入/卖出执行和价格，已取消/拒绝等状态。 （5）notify_trade，可选：跟踪交易的状态，任何已平仓的交易都将报告毛利和净利润。 （6）next，必选：制定交易策略的函数，策略模块最核心的部分。 下面以一个简单的单均线策略为例，展示backtrader的使用过程，即当收盘价上涨突破20日均线买入（做多），当收盘价下跌跌穿20日均线卖出（做空）。为简单起见，不报告交易回测的日志，因此log、notify_order和notify_trade函数省略不写。 class my_strategy1(bt.Strategy): #全局设定交易策略的参数 params=( (&apos;maperiod&apos;,20), ) def __init__(self): #指定价格序列 self.dataclose=self.datas[0].close # 初始化交易指令、买卖价格和手续费 self.order = None self.buyprice = None self.buycomm = None #添加移动均线指标，内置了talib模块 self.sma = bt.indicators.SimpleMovingAverage( self.datas[0], period=self.params.maperiod) def next(self): if self.order: # 检查是否有指令等待执行, return # 检查是否持仓 if not self.position: # 没有持仓 #执行买入条件判断：收盘价格上涨突破20日均线 if self.dataclose[0] &gt; self.sma[0]: #执行买入 self.order = self.buy(size=500) else: #执行卖出条件判断：收盘价格跌破20日均线 if self.dataclose[0] &lt; self.sma[0]: #执行卖出 self.order = self.sell(size=500) 数据加载（Data Feeds）策略设计好后，第二步是数据加载，backtrader提供了很多数据接口，包括quandl（美股）、yahoo、pandas格式数据等，我们主要分析A股数据。 #先引入后面可能用到的包（package） import pandas as pd from datetime import datetime import backtrader as bt import matplotlib.pyplot as plt %matplotlib inline #正常显示画图时出现的中文和负号 from pylab import mpl mpl.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;]使用tushare获取浦发银行（代码：600000）数据。 #使用tushare旧版接口获取数据 import tushare as ts def get_data(code,start=&apos;2010-01-01&apos;,end=&apos;2020-03-31&apos;): df=ts.get_k_data(code,autype=&apos;qfq&apos;,start=start,end=end) df.index=pd.to_datetime(df.date) df[&apos;openinterest&apos;]=0 df=df[[&apos;open&apos;,&apos;high&apos;,&apos;low&apos;,&apos;close&apos;,&apos;volume&apos;,&apos;openinterest&apos;]] return df dataframe=get_data(&apos;600000&apos;) #回测期间 start=datetime(2010, 3, 31) end=datetime(2020, 3, 31) # 加载数据 data = bt.feeds.PandasData(dataname=dataframe,fromdate=start,todate=end) 回测设置（Cerebro）回测设置主要包括几项：回测系统初始化，数据加载到回测系统，添加交易策略， broker设置（如交易资金和交易佣金），头寸规模设置作为策略一部分的交易规模等，最后显示执行交易策略时积累的总资金和净收益。 初始化cerebro回测系统设置cerebro = bt.Cerebro() #将数据传入回测系统cerebro.adddata(data) 将交易策略加载到回测系统中cerebro.addstrategy(my_strategy1) 设置初始资本为10,000startcash = 10000cerebro.broker.setcash(startcash) 设置交易手续费为 0.2%cerebro.broker.setcommission(commission=0.002) 执行回测输出回测结果。 print(f&apos;净收益: {round(pnl,2)}&apos;) d1=start.strftime(&apos;%Y%m%d&apos;) d2=end.strftime(&apos;%Y%m%d&apos;) print(f&apos;初始资金: {startcash}\n回测期间：{d1}:{d2}&apos;) #运行回测系统 cerebro.run() #获取回测结束后的总资金 portvalue = cerebro.broker.getvalue() pnl = portvalue - startcash #打印结果 print(f&apos;总资金: {round(portvalue,2)}&apos;)结果如下： 初始资金: 10000 回测期间：20100331:20200331 总资金: 12065.36 净收益: 2065.36 可视化对上述结果进行可视化，使用内置的matplotlib画图。至此，简单的单均线回测就完成了。下面图形展示了浦发银行在回测期间的价格走势、买卖点和交易总资金的变化等。 # 画图 cerebro.plot() 回测实例from __future__ import (absolute_import, division, print_function, unicode_literals) import datetime # For datetime objects import os.path # To manage paths import sys # To find out the script name (in argv[0]) # Import the backtrader platform import backtrader as bt # Create a Stratey class TestStrategy(bt.Strategy): params = ( (&apos;exitbars&apos;, 5), (&apos;maperiod&apos;, 24), (&apos;printlog&apos;, False), ) def log(self, txt, dt=None, doprint=False): &apos;&apos;&apos; Logging function for this strategy&apos;&apos;&apos; if self.params.printlog or doprint: dt = dt or self.datas[0].datetime.date(0) print(&apos;[%s] %s&apos; % (dt.isoformat(), txt)) def __init__(self): # Keep a reference to the &quot;close&quot; line in the data[0] dataseries self.dataclose = self.datas[0].close # To keep track of pending orders and buy price/commission self.order = None self.buyprice = None self.buycomm = None # Add a MovingAverageSimple indicator self.sma = bt.indicators.SimpleMovingAverage( self.datas[0], period=self.params.maperiod) # Indicators for the plotting show bt.indicators.ExponentialMovingAverage(self.datas[0], period=25) bt.indicators.WeightedMovingAverage( self.datas[0], period=25).subplot = True bt.indicators.StochasticSlow(self.datas[0]) bt.indicators.MACDHisto(self.datas[0]) rsi = bt.indicators.RSI(self.datas[0]) bt.indicators.SmoothedMovingAverage(rsi, period=10) bt.indicators.ATR(self.datas[0]).plot = False def notify_order(self, order): if order.status in [order.Submitted, order.Accepted]: # Buy/Sell order submitted/accepted to/by broker - Nothing to do return # Check if an order has been completed # Attention: broker could reject order if not enough cash if order.status in [order.Completed]: if order.isbuy(): self.log( &apos;BUY EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f&apos; % (order.executed.price, order.executed.value, order.executed.comm)) self.buyprice = order.executed.price self.buycomm = order.executed.comm else: # Sell self.log(&apos;SELL EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f&apos; % (order.executed.price, order.executed.value, order.executed.comm)) self.bar_executed = len(self) elif order.status in [order.Canceled, order.Margin, order.Rejected]: self.log(&apos;Order Canceled/Margin/Rejected&apos;) self.order = None def notify_trade(self, trade): if not trade.isclosed: return self.log(&apos;OPERATION PROFIT： GROSS %.2f, NET %.2f&apos; % (trade.pnl, trade.pnlcomm)) def next(self): # Simply log the closing price of the series from the reference self.log(&apos;Close: %.2f&apos; % self.dataclose[0]) # Check if an order is pending ... if yes, we cannot send a 2nd one if self.order: return # Check if we are in the market if not self.position: # Not yet ... we MIGHT BUY if ... if self.dataclose[0] &gt; self.sma[0]: # current close less than previous close if self.dataclose[-1] &lt; self.dataclose[-2]: # previous close less than the previous close # BUY, BUY, BUY!!! (with default parameters) self.log(&apos;BUY CREATE: %.2f&apos; % self.dataclose[0]) # Keep track of the created order to avoid a 2nd order self.order = self.buy() else: # Already in the market ... we might sell if self.dataclose[0] &lt; self.sma[0]: # SELL, SELL, SELL!!! (with all possible default parameters) self.log(&apos;SELL CREATE: %.2f&apos; % self.dataclose[0]) # Keep track of the created order to avoid a 2nd order self.order = self.sell() def stop(self): self.log(&apos;(MA Period %2d) Ending Value %.2f&apos; % (self.params.maperiod, self.broker.getvalue()), doprint=True) if __name__ == &apos;__main__&apos;: cerebro = bt.Cerebro() # Add a strategy cerebro.addstrategy(TestStrategy) # cerebro.optstrategy(TestStrategy, maperiod=range(10, 31)) # 初始化数据的路径 modpath = os.path.dirname(os.path.abspath(sys.argv[0])) datapath = os.path.join(modpath, &apos;.\\/datas\\/orcl-1995-2014.txt&apos;) # Create a Data Feed,reverse 代表是否反转数据 data = bt.feeds.YahooFinanceCSVData( dataname=datapath, # Do not pass values before this date fromdate=datetime.datetime(2000, 1, 1), # Do not pass values after this date todate=datetime.datetime(2000, 12, 31), reverse=False) # Add the Data Feed to Cerebro cerebro.adddata(data) # 改变账户初始金额 cerebro.broker.set_cash(100000.0) # Set the commission - 0.1% ... divide by 100 to remove the % 交易佣金设置 cerebro.broker.setcommission(commission=0.001) # 设置每笔交易交易的股票数量 cerebro.addsizer(bt.sizers.FixedSize, stake=10) print(&apos;Starting Portfolio Value: %.2f&apos; % cerebro.broker.getvalue()) cerebro.run() print(&apos;Final Portfolio Value: %.2f&apos; % cerebro.broker.getvalue()) # 画图 cerebro.plot() 先来回顾一下交易策略模块（Strategy）的构成。交易策略类代码包含参数或函数名如下： （1）params-全局参数，可选：更改交易策略中变量/参数的值，可用于参数调优。 （2）log：日志，可选：记录策略的执行日志，可以打印出该函数提供的日期时间和txt变量。 （3） init：用于初始化交易策略的类实例的代码。 （4）notify_order，可选：跟踪交易指令（order）的状态。order具有提交，接受，买入/卖出执行和价格，已取消/拒绝等状态。 （5）notify_trade，可选：跟踪交易的状态，任何已平仓的交易都将报告毛利和净利润。 （6）next，必选：制定交易策略的函数，策略模块最核心的部分。 下面仍然以简单均线策略为例，重点介绍参数寻优和交易日志报告。 实现代码如下： #先引入后面可能用到的包（package） import pandas as pd import numpy as np import tushare as ts import matplotlib.pyplot as plt %matplotlib inline #正常显示画图时出现的中文和负号 from pylab import mpl mpl.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;] mpl.rcParams[&apos;axes.unicode_minus&apos;]=False params是全局参数，maperiod是MA均值的长度，默认15天，printlog为打印交易日志，默认不输出结果，策略模块的核心在next（）函数。 from datetime import datetime import backtrader as bt class MyStrategy(bt.Strategy): params=((&apos;maperiod&apos;,15), (&apos;printlog&apos;,False),) def __init__(self): #指定价格序列 self.dataclose=self.datas[0].close # 初始化交易指令、买卖价格和手续费 self.order = None self.buyprice = None self.buycomm = None #添加移动均线指标 self.sma = bt.indicators.SimpleMovingAverage( self.datas[0], period=self.params.maperiod) #策略核心，根据条件执行买卖交易指令（必选） def next(self): # 记录收盘价 #self.log(f&apos;收盘价, {dataclose[0]}&apos;) if self.order: # 检查是否有指令等待执行, return # 检查是否持仓 if not self.position: # 没有持仓 #执行买入条件判断：收盘价格上涨突破15日均线 if self.dataclose[0] &gt; self.sma[0]: self.log(&apos;BUY CREATE, %.2f&apos; % self.dataclose[0]) #执行买入 self.order = self.buy() else: #执行卖出条件判断：收盘价格跌破15日均线 if self.dataclose[0] &lt; self.sma[0]: self.log(&apos;SELL CREATE, %.2f&apos; % self.dataclose[0]) #执行卖出 self.order = self.sell() #交易记录日志（可省略，默认不输出结果） def log(self, txt, dt=None,doprint=False): if self.params.printlog or doprint: dt = dt or self.datas[0].datetime.date(0) print(f&apos;{dt.isoformat()},{txt}&apos;) #记录交易执行情况（可省略，默认不输出结果） def notify_order(self, order): # 如果order为submitted/accepted,返回空 if order.status in [order.Submitted, order.Accepted]: return # 如果order为buy/sell executed,报告价格结果 if order.status in [order.Completed]: if order.isbuy(): self.log(f&apos;买入:\n价格:{order.executed.price},\ 成本:{order.executed.value},\ 手续费:{order.executed.comm}&apos;) self.buyprice = order.executed.price self.buycomm = order.executed.comm else: self.log(f&apos;卖出:\n价格：{order.executed.price},\ 成本: {order.executed.value},\ 手续费{order.executed.comm}&apos;) self.bar_executed = len(self) # 如果指令取消/交易失败, 报告结果 elif order.status in [order.Canceled, order.Margin, order.Rejected]: self.log(&apos;交易失败&apos;) self.order = None #记录交易收益情况（可省略，默认不输出结果） def notify_trade(self,trade): if not trade.isclosed: return self.log(f&apos;策略收益：\n毛收益 {trade.pnl:.2f}, 净收益 {trade.pnlcomm:.2f}&apos;) #回测结束后输出结果（可省略，默认输出结果） def stop(self): self.log(&apos;(MA均线： %2d日) 期末总资金 %.2f&apos; % (self.params.maperiod, self.broker.getvalue()), doprint=True) 下面定义一个主函数，用于对某股票指数（个股）在指定期间进行回测，使用tushare的旧接口获取数据，包含开盘价、最高价、最低价、收盘价和成交量。这里主要以3到30日均线为例进行参数寻优，考察以多少日均线与价格的交叉作为买卖信号能获得最大的收益。 def main(code,start,end=&apos;&apos;,startcash=10000,qts=500,com=0.001): #创建主控制器 cerebro = bt.Cerebro() #导入策略参数寻优 cerebro.optstrategy(MyStrategy,maperiod=range(3, 31)) #获取数据 df=ts.get_k_data(code,autype=&apos;qfq&apos;,start=start,end=end) df.index=pd.to_datetime(df.date) df=df[[&apos;open&apos;,&apos;high&apos;,&apos;low&apos;,&apos;close&apos;,&apos;volume&apos;]] #将数据加载至回测系统 data = bt.feeds.PandasData(dataname=df) cerebro.adddata(data) #broker设置资金、手续费 cerebro.broker.setcash(startcash) cerebro.broker.setcommission(commission=com) #设置买入设置，策略，数量 cerebro.addsizer(bt.sizers.FixedSize, stake=qts) print(&apos;期初总资金: %.2f&apos; % cerebro.broker.getvalue()) cerebro.run(maxcpus=1) print(&apos;期末总资金: %.2f&apos; % cerebro.broker.getvalue()) 再定义一个画图函数，对相应股票（指数）在某期间的价格走势和累计收益进行可视化。 def plot_stock(code,title,start,end): dd=ts.get_k_data(code,autype=&apos;qfq&apos;,start=start,end=end) dd.index=pd.to_datetime(dd.date) dd.close.plot(figsize=(14,6),color=&apos;r&apos;) plt.title(title+&apos;价格走势\n&apos;+start+&apos;:&apos;+end,size=15) plt.annotate(f&apos;期间累计涨幅:{(dd.close[-1]/dd.close[0]-1)*100:.2f}%&apos;, xy=(dd.index[-150],dd.close.mean()), xytext=(dd.index[-500],dd.close.min()), bbox = dict(boxstyle = &apos;round,pad=0.5&apos;, fc = &apos;yellow&apos;, alpha = 0.5), arrowprops=dict(facecolor=&apos;green&apos;, shrink=0.05),fontsize=12) plt.show() 以上证综指为例，回测期间为2010-01-01至2020-03-30，期间累计收益率为-15.31%，惨不忍睹。 面分别对3-30日均线进行回测，这里假设指数可以交易，初始资金为100万元，每次交易100股，注意如果指数收盘价乘以100超过可用资金，会出现交易失败的情况，换句话说在整个交易过程中，是交易固定数量的标的，因此仓位的大小跟股价有直接关系。 main(&apos;sh&apos;,&apos;2010-01-01&apos;,&apos;&apos;,1000000,100) plot_stock(&apos;sh&apos;,&apos;上证综指&apos;,&apos;2010-01-01&apos;,&apos;2020-03-30&apos;) Analyzers模块Analyzers模块涵盖了评价一个量化策略的完整指标，如常见的夏普比率、年化收益率、最大回撤、Calmar比率等等。Analyzers模块原生代码能获取的评价指标如下图所示，其中TradeAnalyzer和PeriodStats又包含了不少指标。由于采用元编程，Analyzers的扩展性较强，可以根据需要添加自己的分析指标，如获取回测期间每一时刻对应的总资金。 策略模块编写（1）params-全局参数，可选：更改交易策略中变量/参数的值，可用于参数调优。 （2）log：日志，可选：记录策略的执行日志，可以打印出该函数提供的日期时间和txt变量。 （3） init：用于初始化交易策略的类实例的代码。 （4）notify_order，可选：跟踪交易指令（order）的状态。order具有提交，接受，买入/卖出执行和价格，已取消/拒绝等状态。 （5）notify_trade，可选：跟踪交易的状态，任何已平仓的交易都将报告毛利和净利润。 （6）next，必选：制定交易策略的函数，策略模块最核心的部分。 （7）其他，包括start()、nextsstart()、stop()、prenext()、notify_fund()、notify_store()和notify_cashvalue。 面以技术分析指标RSI（不了解的请自行百度）的择时策略为例，当RSI&lt;30时买入，RSI&gt;70时卖出。为了简便起见，策略模块中只包含最核心的交易信号。 import pandas as pd import backtrader as bt from datetime import datetime class MyStrategy(bt.Strategy): params=((&apos;short&apos;,30), (&apos;long&apos;,70),) def __init__(self): self.rsi = bt.indicators.RSI_SMA( self.data.close, period=21) def next(self): if not self.position: if self.rsi &lt; self.params.short: self.buy() else: if self.rsi &gt; self.params.long: self.sell() 回测设置回测系统设置与之前一样，主要是数据加载、交易本金、手续费、交易数量的设置，此处以tushare的旧接口获取股票002537的交易数据进行量化回测。 from __future__ import (absolute_import, division, print_function, unicode_literals) import tushare as ts #以股票002537为例 df=ts.get_k_data(&apos;002537&apos;,start=&apos;2010-01-01&apos;) df.index=pd.to_datetime(df.date) #df[&apos;openinterest&apos;] = 0 df=df[[&apos;open&apos;,&apos;high&apos;,&apos;low&apos;,&apos;close&apos;,&apos;volume&apos;]] data = bt.feeds.PandasData(dataname=df, fromdate=datetime(2013, 1, 1), todate=datetime(2020, 4, 17) ) # 初始化cerebro回测系统设置 cerebro = bt.Cerebro() # 加载数据 cerebro.adddata(data) # 将交易策略加载到回测系统中 cerebro.addstrategy(MyStrategy) # 设置初始资本为100,000 cerebro.broker.setcash(100000.0) #每次固定交易数量 cerebro.addsizer(bt.sizers.FixedSize, stake=1000) #手续费 cerebro.broker.setcommission(commission=0.001) 运行回测这里重点是Analyzers模块的调用与结果输出，调用模块是cerebro.addanalyzer()，再从模块中获取分析指标，如夏普比率是bt.analyzers.SharpeRatio，然后是给该指标重命名方便之后调用，即 _name=’SharpeRatio’。要获取分析指标，需要先执行回测系统，cerebro.run()，并将回测结果赋值给变量results，分析指标存储在results[0]里 (strat变量代替)，通过strat.analyzers.SharpeRatio.get_analysis()即可获取相应数据，其他指标操作方法类似。 print(&apos;初始资金: %.2f&apos; % cerebro.broker.getvalue()) cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name = &apos;SharpeRatio&apos;) cerebro.addanalyzer(bt.analyzers.DrawDown, _name=&apos;DW&apos;) results = cerebro.run() strat = results[0] print(&apos;最终资金: %.2f&apos; % cerebro.broker.getvalue()) print(&apos;夏普比率:&apos;, strat.analyzers.SharpeRatio.get_analysis()) print(&apos;回撤指标:&apos;, strat.analyzers.DW.get_analysis()) 输出结果： 初始资金: 100000.00 最终资金: 110215.33 夏普比率: OrderedDict([(&apos;sharperatio&apos;, 0.094)]) 回撤指标: AutoOrderedDict([(&apos;len&apos;, 280), (&apos;drawdown&apos;, 1.01), (&apos;moneydown&apos;, 1126.60), (&apos;max&apos;, AutoOrderedDict([(&apos;len&apos;, 280), (&apos;drawdown&apos;, 3.61), (&apos;moneydown&apos;, 4016.60)]))]) 回测结果可视化下面输出回测图表，一张大图上包含了三张图： （1）资金变动图：可以看到在实施交易策略的数据期内，资金的盈利/损失。 （2）交易收益/亏损。蓝色（红色）点表示获利（亏损）交易以及获利（亏损）多少。 （3）价格图表。绿色和红色箭头分别表示交易策略的进入点和退出点。黑线是交易标的随时间变化的价格， 条形图表示每个条形图期间资金的交易量。 Analyzers模块指标可视化其他init 任何类在生成的时候都是先调用这一初始化构造函数。也就是说，在实例生成的时候，这个函数将被调用。 Birth: start start方法在cerebro告诉strategy，是时候开始行动了，也就是说，通知策略激活的时候被调用。 Childhood: prenext 有些技术指标，比如我们提到的MA，存在一个窗口，也就是说，需要n天的数据才能产生指标，那么在没有产生之前呢？这个prenext方法就会被自动调用。 Adulthood: next这个方法是最核心的，就是每次移动到下一的时间点，策略将会调用这个方法，所以，策略的核心往往都是写在这个方法里的。 Death: stop 策略的生命周期结束，cerebro把这一策略退出。 策略当中的回调函数Strategy 类就像真实世界的交易员一样，当交易执行的时候，他会得到一些消息，譬如order是否执行，一笔trader赚了多少钱，等等。这些消息都将在Strategy类中通过回调函数被得以知晓。这些回调函数如下： notify_order(order)：下的单子，order的任何状态变化都将引起这一方法的调用 notify_trade(trade)：任何一笔交易头寸的改变都将调用这一方法 notify_cashvalue(cash, value)：任何现金和资产组合的变化都将调用这一方法notify_store(msg, *args, **kwargs)：可以结合cerebro类进行自定义方法的调用 那么问题接踵而至，这里我们只关注前2种方法中监测对象的可变化方式。 trade指的是一笔头寸，trade是open的状态指当前时刻，这一标的的头寸从0变到某一非零值。trade是closed则刚好相反。 trade大概有如下常用属性 ref: 唯一id size (int): trade的当前头寸 price (float): trade资产的当前价格 value (float): trade的当前价值 commission (float): trade的累计手续费 pnl (float): trade的当前pnl pnlcomm (float): trade的当前pnl减去手续费 isclosed (bool): 当前时刻trade头寸是否归零 isopen (bool): 新的交易更新了trade justopened (bool): 新开头寸 dtopen (float): trade open的datetime dtclose (float): trade close的datetime Orders order是strategy发出的指令，让cerebro去执行。 strategy自身有buy, sell and close方法来生成order，cancel方法来取消一笔order。下单的方式有很多，后续会介绍，这里主要讲回调函数中，咱们可以获得哪些信息。 order.status可以返回order的当前状态 order.isbuy可以获得这笔order是否是buy order.executed.price order.executed.value order.executed.comm 分别可以获得执行order的价格，总价，和手续费 class TestStrategy(bt.Strategy): params = ( (&apos;maperiod&apos;, 15), ) def log(self, txt, dt=None): &apos;&apos;&apos; Logging function fot this strategy&apos;&apos;&apos; dt = dt or self.datas[0].datetime.date(0) print(&apos;%s, %s&apos; % (dt.isoformat(), txt)) def __init__(self): # Keep a reference to the &quot;close&quot; line in the data[0] dataseries self.dataclose = self.datas[0].close # To keep track of pending orders and buy price/commission self.order = None self.buyprice = None self.buycomm = None # Add a MovingAverageSimple indicator self.sma = bt.indicators.SimpleMovingAverage( self.datas[0], period=self.params.maperiod) def start(self): print(&quot;the world call me!&quot;) def prenext(self): print(&quot;not mature&quot;) def notify_order(self, order): if order.status in [order.Submitted, order.Accepted]: # Buy/Sell order submitted/accepted to/by broker - Nothing to do return # Check if an order has been completed # Attention: broker could reject order if not enougth cash if order.status in [order.Completed]: if order.isbuy(): self.log( &apos;BUY EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f&apos; % (order.executed.price, order.executed.value, order.executed.comm)) self.buyprice = order.executed.price self.buycomm = order.executed.comm else: # Sell self.log(&apos;SELL EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f&apos; % (order.executed.price, order.executed.value, order.executed.comm)) self.bar_executed = len(self) elif order.status in [order.Canceled, order.Margin, order.Rejected]: self.log(&apos;Order Canceled/Margin/Rejected&apos;) self.order = None 可以看到打印出来的结果中，有start和prenext，最后当然也有death Backtrader的indicatordef __init__(self): # Keep a reference to the &quot;close&quot; line in the data[0] dataseries self.dataclose = self.datas[0].close # To keep track of pending orders and buy price/commission self.order = None self.buyprice = None self.buycomm = None # Add a MovingAverageSimple indicator self.sma = bt.indicators.SimpleMovingAverage( self.datas[0], period=self.params.maperiod) 这里的最后，我们使用了一个backtrader内置的indicator，后续我们将尝试自己编写一个indicator。 数据的获取datafeed，也就是cerebro的本源，数据 dataframe = pd.read_csv(&apos;dfqc.csv&apos;, index_col=0, parse_dates=True) dataframe[&apos;openinterest&apos;] = 0 data = bt.feeds.PandasData(dataname=dataframe, fromdate = datetime.datetime(2015, 1, 1), todate = datetime.datetime(2016, 12, 31) ) # Add the Data Feed to Cerebro cerebro.adddata(data) 2014-03-13 00:00:00.005,1.425,1.434,1.449,1.418,457767208.0 2014-03-14 00:00:00.005,1.429,1.422,1.436,1.416,196209439.0 2014-03-17 00:00:00.005,1.433,1.434,1.437,1.422,250946201.0 2014-03-18 00:00:00.005,1.434,1.425,1.437,1.424,245516577.0 2014-03-19 00:00:00.005,1.423,1.419,1.423,1.406,331866195.0 2014-03-20 00:00:00.005,1.412,1.408,1.434,1.407,379443759.0 2014-03-21 00:00:00.005,1.406,1.463,1.468,1.403,825467935.0 dataframe = pd.read_csv(&apos;dfqc.csv&apos;, index_col=0, parse_dates=True) 把csv读入pandas的参数，index_col=0表示第一列时间数据是作为pandas 的index的，parse_dates=Ture是自动把数据中的符合日期的格式变成datetime类型。为什么要这样呢？其实读入后的pandas长怎么样都是由backtrader规定的 pandas的要求的结构，我们就知道，不仅仅有self.datas[0].close,还会有self.datas[0].open。也确实如此。只是我们通常拿close作为一个价格基准 self.datas[0].close 返回的是一个lines。lines是backtrader一个很重要的概念，可以理解为时间序列流，这类数据，后面可以跟index，也就是说，可以有 self.datas[0].close[0] self.datas[0].close[-1] 这里的index是有意义的，0代表当前时刻，-1代表前一时刻，1代表后一时刻，以此类推 所以在next中使用self.dataclose[0],self.dataclose[-1] 安装TA-lib下载TA_Lib-0.4.19-cp37-cp37m-win_amd64 pip install TA_Lib-0.4.19-cp37-cp37m-win_amd64.whl]]></content>
      <categories>
        <category>投资</category>
      </categories>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[qa函数]]></title>
    <url>%2F2020%2F08%2F29%2Fqa%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[数据下单QA_Account()QA_Account() 是quantaxis的核心类, 其作用是一个可以使用规则兼容各种市场的账户类1.3.0以后, QA_Account需要由组合来进行创建(推荐) 调用方式 import QUANTAXIS as QA user = QA.QA_User(username =&apos;quantaxis&apos;, password = &apos;quantaxis&apos;) portfolio=user.new_portfolio(&apos;x1&apos;) account = *portfolio.new_account*(account_cookie=&apos;test&apos;) QA_AccountPRO? Init signature: QA_AccountPRO( user_cookie:str, portfolio_cookie:str, account_cookie=None, strategy_name=None, market_type=&apos;stock_cn&apos;, frequence=&apos;day&apos;, broker=&apos;backtest&apos;, init_hold={}, init_cash=1000000, commission_coeff=0.00025, tax_coeff=0.001, margin_level={}, allow_t0=False, allow_sellopen=False, allow_margin=False, running_environment=&apos;backtest&apos;, auto_reload=False, generated=&apos;direct&apos;, start=None, end=None, ) Docstring: JOB是worker 需要接受QA_EVENT 需要完善RUN方法 👻QA_Broker 继承这个类 👻QA_Account 继承这个类 👻QA_OrderHandler 继承这个类 这些类都要实现run方法，在其它线程🌀中允许自己的业务代码 File: /usr/local/lib/python3.6/site-packages/QUANTAXIS/QAARP/QAAccountPro.py Type: type Subclasses: accpro = *portfolio.new_accountpro*(&apos;pro2&apos;,market_type=QA.MARKET_TYPE.STOCK_CN) accpro.positions {} *accpro.receive_deal*? Signature: accpro.receive_deal( code, trade_id:str, order_id:str, realorder_id:str, trade_price, trade_amount, trade_towards, trade_time, message=None, ) Docstring: &lt;no docstring&gt; File: /usr/local/lib/python3.6/site-packages/QUANTAXIS/QAARP/QAAccountPro.py Type: method `accpro.receive_deal`(&apos;000001&apos;,&apos;001&apos;,&apos;001&apos;,&apos;0001&apos;,trade_price=12,trade_amount=10000,trade_towards=QA.ORDER_DIRECTION.BUY,trade_time=&apos;2020-08-18&apos;) *accpro.positions* {&apos;000001&apos;: &lt; QAPOSITION 000001 amount 10000/0 &gt;} *accpro.history_table* *accpro.send_order?* Signature: accpro.send_order( code=None, amount=None, time=None, towards=None, price=None, money=None, order_model=&apos;LIMIT&apos;, amount_model=&apos;by_amount&apos;, order_id=None, position_id=None, *args, **kwargs, ) Docstring: &lt;no docstring&gt; File: /usr/local/lib/python3.6/site-packages/QUANTAXIS/QAARP/QAAccountPro.py Type: method 基于期货市场的账户初始化 future_account = portfolio.new_account(account_cookie =&apos;future&apos;,allow_t0=True,allow_margin=True,allow_sellopen=True, running_environment=QA.MARKET_TYPE.FUTURE_CN) account的其他属性可以.出来，可以自己试着看 *accpro.cash* [1000000, 879970.0, 779945.0] *accpro.cash_available* 779945.0 *accpro.get_history* *accpro.get_position*(&apos;000001&apos;) pos2 = accpro.get_position(&apos;000002&apos;) *pos2.message* {&apos;code&apos;: &apos;000002&apos;, &apos;instrument_id&apos;: &apos;000002&apos;, &apos;user_id&apos;: &apos;pro2&apos;, &apos;portfolio_cookie&apos;: &apos;x1&apos;, &apos;username&apos;: &apos;quantaxis&apos;, &apos;position_id&apos;: &apos;3e2cd89e-7143-4e0e-b344-40571e84eda5&apos;, &apos;account_cookie&apos;: &apos;pro2&apos;, &apos;frozen&apos;: {}, &apos;name&apos;: None, &apos;spms_id&apos;: None, &apos;oms_id&apos;: None, &apos;market_type&apos;: &apos;stock_cn&apos;, &apos;exchange_id&apos;: None, &apos;moneypreset&apos;: 100000, &apos;moneypresetLeft&apos;: 0.0, &apos;lastupdatetime&apos;: &apos;&apos;, &apos;volume_long_today&apos;: 5000, &apos;volume_long_his&apos;: 0, &apos;volume_long&apos;: 5000, &apos;volume_short_today&apos;: 0, &apos;volume_short_his&apos;: 0, &apos;volume_short&apos;: 0, &apos;volume_long_frozen_today&apos;: 0, &apos;volume_long_frozen_his&apos;: 0, &apos;volume_long_frozen&apos;: 0, &apos;volume_short_frozen_today&apos;: 0, &apos;volume_short_frozen_his&apos;: 0, &apos;volume_short_frozen&apos;: 0, &apos;margin_long&apos;: 100000.0, &apos;margin_short&apos;: 0, &apos;margin&apos;: 100000.0, &apos;position_price_long&apos;: 20.0, &apos;position_cost_long&apos;: 100000.0, &apos;position_price_short&apos;: 0, &apos;position_cost_short&apos;: 0.0, &apos;open_price_long&apos;: 20.0, &apos;open_cost_long&apos;: 100000.0, &apos;open_price_short&apos;: 0, &apos;open_cost_short&apos;: 0.0, &apos;trades&apos;: [], &apos;orders&apos;: {}, &apos;last_price&apos;: 20, &apos;float_profit_long&apos;: 0.0, &apos;float_profit_short&apos;: 0.0, &apos;float_profit&apos;: 0.0, &apos;position_profit_long&apos;: 0.0, &apos;position_profit_short&apos;: 0.0, &apos;position_profit&apos;: 0.0} *pos2.volume_long* 5000 *pos2.volume_long_today* 5000 *pos2.volume_long_his* 0 *pos2.last_price* 20 *pos2.on_price_change*(21) if pos2.float_profit_long &gt; 2000: print(&apos;sell&apos;) sell QIFIAccountfrom QIFIAccount import QIFI_Account aifiacc = QIFI_Account(username=&apos;x2&apos;,password=&apos;x2&apos;,) aifiacc.initial() Create new Account aifiacc? Type: QIFI_Account String form: &lt;QIFIAccount.QAQIFIAccount.QIFI_Account object at 0x7f6e3b2dfc18&gt; File: /usr/local/lib/python3.6/site-packages/QIFIAccount/QAQIFIAccount.py Docstring: &lt;no docstring&gt; Init docstring: Initial QIFI Account是一个基于 DIFF/ QIFI/ QAAccount后的一个实盘适用的Account基类 1. 兼容多持仓组合 2. 动态计算权益 使用 model = SIM/ REAL来切换 sr = aifiacc.send_order(&apos;003&apos;,10,12,ORDER_DIRECTION.BUY) aifiacc.send_order? Signature: aifiacc.send_order( code:str, amount:float, price:float, towards:int, order_id:str=&apos;&apos;, ) Docstring: &lt;no docstring&gt; File: /usr/local/lib/python3.6/site-packages/QIFIAccount/QAQIFIAccount.py Type: method sr = aifiacc.send_order(&apos;004&apos;,10.0,12.0,QA.ORDER_DIRECTION.BUY) {&apos;volume_long&apos;: 0, &apos;volume_short&apos;: 0, &apos;volume_long_frozen&apos;: 0, &apos;volume_short_frozen&apos;: 0} {&apos;volume_long&apos;: 0, &apos;volume_short&apos;: 0} order check success 下单成功 b3f155c7-ab2b-4ac1-94b7-08b6af152738 td = aifiacc.make_deal(sr) 全部成交 b3f155c7-ab2b-4ac1-94b7-08b6af152738 update trade]]></content>
      <categories>
        <category>投资</category>
      </categories>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QUANTAXIS]]></title>
    <url>%2F2020%2F08%2F20%2FQUANTAXIS%2F</url>
    <content type="text"><![CDATA[环境准备安装cmder 官网下载地址 http://cmder.net/ 下载好解压包可直接使用 环境变量配置 在系统属性里面配置环境变量，将Cmder.exe所在文件路径添加至path里 win+R,输入cmder,确定，即可运行cmder 配置右键快捷启动 // 设置任意地方鼠标右键启动Cmder Cmder.exe /REGISTER ALL 快捷键 Tab 自动路径补全 Ctrl+T 建立新页签 Ctrl+W 关闭页签 Ctrl+Tab 切换页签 Alt+F4 关闭所有页签 Alt+Shift+1 开启cmd.exe Alt+Shift+2 开启powershell.exe Alt+Shift+3 开启powershell.exe (系统管理员权限) Ctrl+1 快速切换到第1个页签 Ctrl+n 快速切换到第n个页签( n值无上限) Alt + enter 切换到全屏状态 Ctr+r 历史命令搜索 Tab 自动路径补全 Ctrl+T 建立新页签 Ctrl+W 关闭页签 Ctrl+Tab 切换页签 Alt+F4 关闭所有页签 Alt+Shift+1 开启cmd.exe Alt+Shift+2 开启powershell.exe Alt+Shift+3 开启powershell.exe (系统管理员权限) Ctrl+1 快速切换到第1个页签 Ctrl+n 快速切换到第n个页签( n值无上限) Alt + enter 切换到全屏状态 Ctr+r 历史命令搜索 Win+Alt+P 开启工具选项视窗 中文乱码问题 将下面的4行命令添加到cmder/config/aliases文件末尾。 l=ls --show-control-chars la=ls -aF --show-control-chars ll=ls -alF --show-control-chars ls=ls --show-control-chars -F 安装docker桌面版下载https://www.docker.com/ 下载Docker Desktop 安装可能遇到的问题Installation failed:one pre-requisite is not fullfilled 提示我们系统版本低，解决办法，伪装成专业版系统。用管理员权限开启运行[cmd]命令开启命令行，输入如下指令 REG ADD &quot;HKEY_LOCAL_MACHINE\software\Microsoft\Windows NT\CurrentVersion&quot; /v EditionId /T REG_EXPAND_SZ /d Professional /F 再次安装docker可以成功 下载QUANTAXIS的docker-compose.yaml文件`https://github.com/QUANTAXIS/QUANTAXIS` 如果你是股票方向的 ==&gt; 选择 qa-service 下的docker-compose.yaml 如果你是期货方向的 ==&gt; 选择 qa-service-future 下的docker-compose.yaml 你可以理解 docker的构成类似搭积木的模式, 你需要这个功能的积木, 就选择他放在你的docker-compose.yaml里面 期货方向的yaml 比股票多一个 QACTPBEE的docker-container [这是用于分发期货的tick行情所需的 股票则无需此积木] 可通过git拉取全部代码到本地，从本地拷贝出需要的dockerfile文件 docker部署quantaxis 选取一个空间较大的盘，最好不放c盘，新建quantaxis文件夹，将docker-compose.yaml拷贝到quantaxis文件夹 cd到quantaxis文件夹，运行如下命令 docker volume create --name=qamg docker volume create --name=qacode docker-compose up -d 意思在后台启动这个docker环境，如果需要控制台打印输出，则把-d去掉 在无报错的情况下，打开浏览器输入localhost:81即可看到 在上方随意点击栏目，都可进入登陆界面，默认密码是quantaxis docker做了什么 帮你直接开启你需要的服务 27017 mongodb 8888 jupyter （密码）quantaxis 8010 quantaxis_webserver 81 quantaxis_community 社区版界面 61208 系统监控 15672 qa-eventmq （密码）admin admin 日志查看 docker logs cron容器名 其他命令 docker ps docker stats docker-compose top （必须到dockerfile文件夹目录下） docker-compose ps （必须到dockerfile文件夹目录下） docker stop $(docker ps -a -q)停止容器 docker rm $(docker ps -a -q)删除容器 docker-compose pull 更新（必须到dockerfile文件夹目录下） docker-compose up -d 重启服务（必须到dockerfile文件夹目录下） docker-compose stop 停止服务（必须到dockerfile文件夹目录下） docker run --rm -v qamg:/data/db \ -v $(pwd):/backup alpine \tar zcvf /backup/dbbackup.tar /data/db 备份数据库到当前目录下 docker run --rm -v qamg:/data/db \ -v $(pwd):/backup alpine \ sh -c &quot;cd /data/db \ &amp;&amp; rm -rf diagnostic.data \ &amp;&amp; rm -rf journal \ &amp;&amp; rm -rf configdb \ &amp;&amp; cd / \ &amp;&amp; tar xvf /backup/dbbackup.tar&quot; 还原当前目录下的dbbackup.tar到mongod数据库 当更新了dockerfile后重启服务，之前保存的数据不会被清除 保存数据 先进入到jupyter的登陆页登陆，找到terminal 在点开的terminal界面中，输入quantaxis 回车, 进入quantaxis cli的命令行界面 在命令行界面 输入 save 按回车, 你可以看到许多命令行选项 参考 save all (股票/指数 的日线数据 | 权息数据 | 板块数据) save x (股票/指数的 日线/分钟线数据 | 权息数据| 板块数据) save future_min_all (期货的全部合约的分钟线数据) save future_min (q期货主连的分钟线数据) save future_day_all (期货全部合约的日线数据) save future_day (期货主连的日线数据) save index_day (指数数据 此处也要存, 因为在做回测的时候, 需要沪深300作为标的对照物) 存完数据后可以打开notebook,做一个回测 import QUANTAXIS as QA import numpy as np import pandas as pd import datetime st1=datetime.datetime.now() # define the MACD strategy def MACD_JCSC(dataframe, SHORT=12, LONG=26, M=9): &quot;&quot;&quot; 1.DIF向上突破DEA，买入信号参考。 2.DIF向下跌破DEA，卖出信号参考。 &quot;&quot;&quot; CLOSE = dataframe.close DIFF = QA.EMA(CLOSE, SHORT) - QA.EMA(CLOSE, LONG) DEA = QA.EMA(DIFF, M) MACD = 2*(DIFF-DEA) CROSS_JC = QA.CROSS(DIFF, DEA) CROSS_SC = QA.CROSS(DEA, DIFF) ZERO = 0 return pd.DataFrame({&apos;DIFF&apos;: DIFF, &apos;DEA&apos;: DEA, &apos;MACD&apos;: MACD, &apos;CROSS_JC&apos;: CROSS_JC, &apos;CROSS_SC&apos;: CROSS_SC, &apos;ZERO&apos;: ZERO}) # create account user = QA.QA_User(username=&apos;quantaxis&apos;, password=&apos;quantaxis&apos;) portfolio = user.new_portfolio(&apos;qatestportfolio&apos;) Account = portfolio.new_account(account_cookie=&apos;macd_stock&apos;, init_cash=1000000) Broker = QA.QA_BacktestBroker() QA.QA_SU_save_strategy(&apos;MACD_JCSC&apos;,&apos;Indicator&apos;,Account.account_cookie) # get data from mongodb QA.QA_SU_save_strategy(&apos;MACD_JCSC&apos;, &apos;Indicator&apos;, Account.account_cookie, if_save=True) data = QA.QA_fetch_stock_day_adv( [&apos;000001&apos;, &apos;000002&apos;, &apos;000004&apos;, &apos;600000&apos;], &apos;2017-09-01&apos;, &apos;2018-05-20&apos;) data = data.to_qfq() # add indicator ind = data.add_func(MACD_JCSC) # ind.xs(&apos;000001&apos;,level=1)[&apos;2018-01&apos;].plot() data_forbacktest=data.select_time(&apos;2018-01-01&apos;,&apos;2018-05-01&apos;) for items in data_forbacktest.panel_gen: for item in items.security_gen: ################### daily_ind=ind.loc[item.index] if daily_ind.CROSS_JC.iloc[0]&gt;0: order=Account.send_order( code=item.code[0], time=item.date[0], amount=1000, towards=QA.ORDER_DIRECTION.BUY, price=0, order_model=QA.ORDER_MODEL.CLOSE, amount_model=QA.AMOUNT_MODEL.BY_AMOUNT ) #print(item.to_json()[0]) Broker.receive_order(QA.QA_Event(order=order,market_data=item)) trade_mes=Broker.query_orders(Account.account_cookie,&apos;filled&apos;) res=trade_mes.loc[order.account_cookie,order.realorder_id] order.trade(res.trade_id,res.trade_price,res.trade_amount,res.trade_time) elif daily_ind.CROSS_SC.iloc[0]&gt;0: #print(item.code) if Account.sell_available.get(item.code[0], 0)&gt;0: order=Account.send_order( code=item.code[0], time=item.date[0], amount=Account.sell_available.get(item.code[0], 0), towards=QA.ORDER_DIRECTION.SELL, price=0, order_model=QA.ORDER_MODEL.MARKET, amount_model=QA.AMOUNT_MODEL.BY_AMOUNT ) #print Broker.receive_order(QA.QA_Event(order=order,market_data=item)) trade_mes=Broker.query_orders(Account.account_cookie,&apos;filled&apos;) res=trade_mes.loc[order.account_cookie,order.realorder_id] order.trade(res.trade_id,res.trade_price,res.trade_amount,res.trade_time) Account.settle() print(&apos;TIME -- {}&apos;.format(datetime.datetime.now()-st1)) print(Account.history) print(Account.history_table) print(Account.daily_hold) # create Risk analysis Risk = QA.QA_Risk(Account) Account.save() Risk.save() 推荐的做法 使用最新的QAStrategy来做回测/模拟 首先 打开terminal (上面有讲), 输入 pip install qastrategy 然后 新建一个notebook, 输入 from QAStrategy import QAStrategyCTABase import QUANTAXIS as QA import pprint class CCI(QAStrategyCTABase): def on_bar(self, bar): res = self.cci() print(res.iloc[-1]) if res.CCI[-1] &lt; -100: print(&apos;LONG&apos;) if self.positions.volume_long == 0: self.send_order(&apos;BUY&apos;, &apos;OPEN&apos;, price=bar[&apos;close&apos;], volume=1) if self.positions.volume_short &gt; 0: self.send_order(&apos;SELL&apos;, &apos;CLOSE&apos;, price=bar[&apos;close&apos;], volume=1) elif res.CCI[-1] &gt; 100: print(&apos;SHORT&apos;) if self.positions.volume_short == 0: self.send_order(&apos;SELL&apos;, &apos;OPEN&apos;, price=bar[&apos;close&apos;], volume=1) if self.positions.volume_long &gt; 0: self.send_order(&apos;BUY&apos;, &apos;CLOSE&apos;, price=bar[&apos;close&apos;], volume=1) def cci(self,): return QA.QA_indicator_CCI(self.market_data, 61) def risk_check(self): pass # pprint.pprint(self.qifiacc.message) 然后 你可以自由指定回测/模拟 首先实例化这个类 strategy =CCI(code=&apos;RB2005&apos;, frequence=&apos;1min&apos;,strategy_id=&apos;a3916de0-bd28-4b9c-bea1-94d91f1744ac&apos;, start=‘2020-01-01‘, end=‘2020-02-07’) 如果你需要测试这个策略 strategy.debug() 如果你需要做回测 strategy.run_backtest() 如果你需要让他直接挂模拟 在挂模拟的时候, 你需要注意一些东西 挂模拟的标的需要和真实标的一致挂模拟的时候, 你的行情必须是有推送的, 并且申请了你所需要的的分钟线级别的数据 (如何申请行情数据? 你可以看这里 )https://github.com/yutiansut/QUANTAXIS_RealtimeCollector # 期货订阅请求 curl -X POST &quot;http://127.0.0.1:8011?action=new_handler&amp;market_type=future_cn&amp;code=au1911&quot; 12345678910# 股票订阅请求curl -X POST "http://127.0.0.1:8011?action=new_handler&amp;market_type=stock_cn&amp;code=000001"# 二次采样请求curl -X POST "http://127.0.0.1:8011?action=new_resampler&amp;market_type=future_cn&amp;code=au1911&amp;frequence=2min"对于小白可能难以理解curl是个啥, 此处给出一个简单易懂的代码import requestsrequests.post("http://127.0.0.1:8011?action=new_handler&amp;market_type=future_cn&amp;code=&#123;&#125;".format("rb2001")以此类推其他的请求都可以这么做 像 螺纹2001 合约, 你需要改成 rb2001 注意此处是小写 strategy =CCI(code=&apos;rb2001&apos;, frequence=&apos;1min&apos;,strategy_id=&apos;a3916de0-bd28-4b9c-bea1-94d91f1744ac&apos;) strategy.debug_sim() 做完了这些操作以后, 你可以点击 回测 你就可以看到类似这样的结果 更多参考 http://www.yutiansut.com:3000/topic/5dc5da7dc466af76e9e3bc5d 如何修改期货的实盘行情地址 http://www.yutiansut.com:3000/topic/5dfade9efe01257b44740e70 实时如何申请行情 http://www.yutiansut.com:3000/topic/5dd1be9b0c8e672840f3fea7 如何接入你的实盘期货账户 http://www.yutiansut.com:3000/topic/5dc865e8c466af76e9e3bdd1 如何实现模拟盘/实盘的跟单 http://www.yutiansut.com:3000/topic/5ddb5ba8fe01257b4474080a docker 如何访问外部数据库 https://github.com/QUANTAXIS/QUANTAXIS/issues/1346 http://www.yutiansut.com:3000/topic/5e4531c96d3b182e88b4ebb4 docker小白用户的推荐 http://www.yutiansut.com:3000/topic/5e4cb13f6d3b182e88b4ef64 整个环境一览 配置的一些备忘 安装vscode可以调试docker 修改定时保存数据的时间使用vscode进入qa-cron的容器中，进入目录 /etc/cron.d 修改daily_update里的时间即可，如果只需要保存期货数据，要改掉脚本里的 update_future.py 部署 crontab daily_update crontab -l 检查 /etc/init.d/cron start /etc/init.d/cron status 解决在执行时提示 cron: can’t lock /var/run/crond.pid, otherpid may be 2699: Resource temporarily unavailable 解决方案： rm -rf /var/run/crond.pid /etc/init.d/cron reload sudo /usr/sbin/service cron restart 如果定时不启用，可以在本地save保存数据，save不会覆盖之前已经下载好的数据 在界面安装包!ls !pip -v !pip install !pip install qastrategy -U 在terminal界面也可以安装 from QUANTAXIS.QAARP.QAAccountPro import QA_AccountPRO QA_AccountPRO? docker-compose常用命令docker-compose up -d nginx 构建建启动nignx容器 docker-compose exec nginx bash 登录到nginx容器中 docker-compose down 删除所有nginx容器,镜像 docker-compose ps 显示所有容器 docker-compose restart nginx 重新启动nginx容器 docker-compose run --no-deps --rm php-fpm php -v 在php-fpm中不启动关联容器，并容器执行php -v 执行完成后删除容器 docker-compose build nginx 构建镜像 。 docker-compose build --no-cache nginx 不带缓存的构建。 docker-compose logs nginx 查看nginx的日志 docker-compose logs -f nginx 查看nginx的实时日志 docker-compose config -q 验证（docker-compose.yml）文件配置，当配置正确时，不输出任何内容，当文件配置错误，输出错误信息。 docker-compose events --json nginx 以json的形式输出nginx的docker日志 docker-compose pause nginx 暂停nignx容器 docker-compose unpause nginx 恢复ningx容器 docker-compose rm nginx 删除容器（删除前必须关闭容器） docker-compose stop nginx 停止nignx容器 docker-compose start nginx 启动nignx容器]]></content>
      <categories>
        <category>投资</category>
      </categories>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git使用详解]]></title>
    <url>%2F2020%2F06%2F17%2FGIT%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Git的安装和使用下载安装Git[https://git-scm.com/download/win](https://git-scm.com/download/win &quot;下载Git&quot;) 下载完成后双击安装检验是否安装完成鼠标右击如果看到有两个git单词则安装成功 Git基本工作流程Git的工作区域 向仓库中添加文件流程 Git初始化及仓库创建和操作Git安装之后需要进行一些基本信息设置 设置用户名：git config – global user.name ‘你再github上注册的用户名’; 设置用户邮箱：git config – global user.email ‘注册时候的邮箱’; 注意：该配置会在github主页上显示谁提交了该文件 配置ok之后，我们用如下命令来看看是否配置成功 git config –list 注意：git config –global 参数，有了这个参数表示你这台机器上所有的git仓库都会使用这个配置，当然你也可以对某个仓库指定不同的用户名和邮箱 初始化一个新的git仓库 创建文件夹 方法一：可以鼠标右击-》点击新建文件夹test1 方法二：使用git新建：$ mkdir test1 在文件内初始化git（创建git仓库） 方法一：直接输入 $ cd test1 方法一：点击test1文件下进去之后-》鼠标右击选择Git Bash Here-&gt;输入$ git int 向仓库中添加文件 方法一：用打开编辑器新建index.html文件 方法二：使用git命令。$ touch ‘文件名’，然后把文件通过$ git add ‘文件名’添加到暂存区，最后提交操作 修改仓库文件 方法一：用编辑器打开index.html进行修改 方法二：使用git命令。$ vi ‘文件名’，然后在中间写内容，最后提交操作 删除仓库文件 方法一：在编辑器中直接把要删除的文件删除掉 方法二：使用git删除：$ git rm ‘文件名’，然后提交操作 Git管理远程仓库 Git克隆操作目的：将远程仓库（github上对应的项目）复制到本地 代码：git clone 仓库地址仓库地址由来如下： 克隆项目 将本地仓库同步到git远程仓库中：git push 注意 解决：这是通过Git GUI进行提交时发生的错误，由 .git 文件夹中的文件被设为“只读”所致，将 .git 文件夹下的所有文件、文件夹及其子文件的只读属性去掉即可。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列5个应用场景]]></title>
    <url>%2F2020%2F06%2F08%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%975%E4%B8%AA%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削峰等问题，实现高性能、高可用、可伸缩和最终一致性架构，使用较多的消息队列有ActiveMQ、RabbitMQ、ZeroMQ、Kafka、MetaMQ、RocketMQ。 消息队列应用场景异步处理场景说明：用户注册后，需要发送注册邮件和注册短信，传统的做法有两种，串行的方式和并行的方式。 串行方式：将注册信息写入数据库成功后，发送注册邮件，在发送注册短信，以上三个任务全部完成后，返回给用户。 并行的方式：将注册信息写入数据库成功后，发送注册邮件的同时，发送注册短信，以上三个任务完成后，返回给客户端。与串行的差别是，并行的方式可以提高处理的时间。 假设三个业务节点每个使用50毫秒，不考虑网络等其他开销，则串行方式时间是150毫秒，并行的时间是100毫秒。 因为CPU在单位时间内处理的请求数是一定的，假设CPU1秒内的吞吐量是100次。则串行方式1秒内CPU可处理的请求量是7次（1000/150）。并行方式处理的请求是10（1000/100）次。 如以上案例描述，传统的方式系统的性能（并发量，吞吐量，响应时间）会有瓶颈，如何解决呢？ 引入消息队列，将不是必须的业务逻辑，异步处理，改造后如下 按照以上约定，用户的响应时间想当与是注册信息写入数据库的时间，也就是50毫秒。注册邮件，发送短信写入消息队列后，直接返回，因此写入消息队列的速度很快，基本可以忽略，因此用户的响应时间可能就是50毫秒。因此架构改变后，系统的吞吐量提高到每秒20QPS。比串行提高了3倍，比并行提高了2倍。 应用解耦场景说明：用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口。如下图： 缺点：假如库存系统无法访问，则订单减库存将失败，从而导致订单失败，订单系统与库存系统存在耦合 解决：引入应用消息队列 订单系统：用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功。 库存系统：订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作。 假如：在下单时库存系统不能正常使用。也不影响正常下单，因为下单后，订单系统写入消息队列就不再关心其他的后续操作了。实现订单系统与库存系统的应用解耦。 流量削峰流量削峰也是消息队列中常用的场景，一般在秒杀或团抢活动中使用广泛。 应用场景：秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端假如消息队列。 可以控制活动的人数，可以缓解短时间内高流量压垮应用。 用户的请求，服务器接收到后先写入消息队列。假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误的页面。 秒杀业务根据消息队列中的请求信息，再做后续处理。 日志处理日志处理是指将消息ui列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题。架构简化如下： 日志采集客户端，负责日志数据采集，定时写入Kafka队列；Kafak消息队列，负责日志数据的接收，存储和转发；日志处理应用：订阅并消费kafaka队列中的日志数据。 以下是新浪kafka日志处理应用案例： Kafka:接收用户日志的消息队列； Logstach:做日志解析，统一成JSON传输给Elasticsearch; Elasticsearch:实时日志分析服务的核心技术，一个schemaless,实时的数据存储服务，通过index组织数据，兼具强大的搜索和统计功能。 Kibaba:基于Elasticsearch的数据可视化组件，超强的数据可视化能力 消息通讯消息通讯是指，消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯，比如实现点对点消息队列，或者聊天室等。 客户端A，客户端B，客户端N订阅同一主题，进行消息发布和接收。实现类似聊天室效果。 实战用redis实现简单的发布/订阅场景：两台tomcat做集群，配置文件不同时生效的问题 &lt;bean id=&quot;messageContainer&quot; class=&quot;org.springframework.data.redis.listener.RedisMessageListenerContainer&quot; destroy-method=&quot;destroy&quot;&gt; &lt;property name=&quot;connectionFactory&quot; ref=&quot;jedisConnectionFactory&quot; /&gt; &lt;property name=&quot;messageListeners&quot;&gt; &lt;map&gt; &lt;entry key-ref=&quot;subService&quot;&gt; &lt;ref bean=&quot;channelTopic&quot; /&gt; &lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;channelTopic&quot; class=&quot;org.springframework.data.redis.listener.ChannelTopic&quot;&gt; &lt;constructor-arg value=&quot;server:topic&quot; /&gt; &lt;/bean&gt; @Service public class PubServiceImpl implements PubService { private static final Logger logger = Logger.getLogger(PubService.class); @Autowired private RedisTemplate&lt;String, String&gt; redisTemplate; private String channelTopic = &quot;server:topic&quot;; public void publish(String str) { //String str = JsonUtils.toJson(message); logger.error(&quot;Publish message:&quot; + str + &quot;, topic:&quot; + channelTopic); redisTemplate.convertAndSend(channelTopic, str); } public void publish(Map&lt;String, Object&gt; message) { String str = JsonUtils.toJson(message); logger.error(&quot;Publish message:&quot; + str + &quot;, topic:&quot; + channelTopic); redisTemplate.convertAndSend(channelTopic, str); } } @Service(&quot;subService&quot;) public class SubServiceImpl implements MessageListener { private static final Logger logger = Logger.getLogger(SubServiceImpl.class); @Autowired private ChannelTopic channelTopic; @Override public void onMessage(Message message, byte[] pattern) { String str = message.toString(); logger.error(&quot;Subscribe message:&quot; + str + &quot;, topic:&quot; + channelTopic.getTopic()); try { String type = m.getType(); if (&quot;config&quot;.equals(type)) { System.out.println(&quot;接收到消息，doSomething========&quot;); }else if(&quot;me&quot;.equals(type)){ System.out.println(&quot;接收到消息，doSomething========&quot;); }else{ System.out.println(&quot;接收到消息，doSomething========&quot;); } } catch (Exception e) { e.printStackTrace(); } } }]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA的map]]></title>
    <url>%2F2020%2F06%2F08%2FJAVA%E7%9A%84map%2F</url>
    <content type="text"><![CDATA[Map的用法类型介绍 HashMap 最常用的Map,它根据键的HashCode值存储数据，根据键可以直接获取它的值，具有很快的访问速度。HashMap最多只允许一条记录的键为null（多条会覆盖）；允许多条记录的值为null。非同步的。 TreeMap 能够把它保存的记录根据键（key）排序，默认是按升序排序，也可指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。TreeMap不允许key的值为null。非同步的。 Hashtable 与HashMap类似，不同的是：key和value的值均不允许为null;它支持线程同步，即任一时刻只有一个线程能写HashTable,因此也导致了HashTable在写入时会比较慢。 LinkedHashMap 保存了记录的插入顺序，在使用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，在遍历的时候会比HashMap慢。key和value均允许为空。非同步的。 Map用法用法 Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;(); 插入元素 map.put(&quot;key1&quot;,&quot;value1&quot;); 获取元素 map.get(&quot;key1&quot;); 移除元素 map.remove(&quot;key1&quot;) 四种常用Map插入与读取性能比较 测试代码 package net.xsoftlab.baike; import java.util.HashMap; import java.util.Hashtable; import java.util.LinkedHashMap; import java.util.Map; import java.util.Random; import java.util.TreeMap; import java.util.UUID; public class Test { static int hashMapW = 0; static int hashMapR = 0; static int linkMapW = 0; static int linkMapR = 0; static int treeMapW = 0; static int treeMapR = 0; static int hashTableW = 0; static int hashTableR = 0; public static void main(String[] args) { for (int i = 0; i &lt; 10; i++) { Test test = new Test(); test.test(100 * 10000); System.out.println(); } System.out.println(&quot;hashMapW = &quot; + hashMapW / 10); System.out.println(&quot;hashMapR = &quot; + hashMapR / 10); System.out.println(&quot;linkMapW = &quot; + linkMapW / 10); System.out.println(&quot;linkMapR = &quot; + linkMapR / 10); System.out.println(&quot;treeMapW = &quot; + treeMapW / 10); System.out.println(&quot;treeMapR = &quot; + treeMapR / 10); System.out.println(&quot;hashTableW = &quot; + hashTableW / 10); System.out.println(&quot;hashTableR = &quot; + hashTableR / 10); } public void test(int size) { int index; Random random = new Random(); String[] key = new String[size]; // HashMap 插入 Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); long start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) { key[i] = UUID.randomUUID().toString(); map.put(key[i], UUID.randomUUID().toString()); } long end = System.currentTimeMillis(); hashMapW += (end - start); System.out.println(&quot;HashMap插入耗时 = &quot; + (end - start) + &quot; ms&quot;); // HashMap 读取 start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) { index = random.nextInt(size); map.get(key[index]); } end = System.currentTimeMillis(); hashMapR += (end - start); System.out.println(&quot;HashMap读取耗时 = &quot; + (end - start) + &quot; ms&quot;); // LinkedHashMap 插入 map = new LinkedHashMap&lt;String, String&gt;(); start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) { key[i] = UUID.randomUUID().toString(); map.put(key[i], UUID.randomUUID().toString()); } end = System.currentTimeMillis(); linkMapW += (end - start); System.out.println(&quot;LinkedHashMap插入耗时 = &quot; + (end - start) + &quot; ms&quot;); // LinkedHashMap 读取 start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) { index = random.nextInt(size); map.get(key[index]); } end = System.currentTimeMillis(); linkMapR += (end - start); System.out.println(&quot;LinkedHashMap读取耗时 = &quot; + (end - start) + &quot; ms&quot;); // TreeMap 插入 key = new String[size]; map = new TreeMap&lt;String, String&gt;(); start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) { key[i] = UUID.randomUUID().toString(); map.put(key[i], UUID.randomUUID().toString()); } end = System.currentTimeMillis(); treeMapW += (end - start); System.out.println(&quot;TreeMap插入耗时 = &quot; + (end - start) + &quot; ms&quot;); // TreeMap 读取 start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) { index = random.nextInt(size); map.get(key[index]); } end = System.currentTimeMillis(); treeMapR += (end - start); System.out.println(&quot;TreeMap读取耗时 = &quot; + (end - start) + &quot; ms&quot;); // Hashtable 插入 key = new String[size]; map = new Hashtable&lt;String, String&gt;(); start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) { key[i] = UUID.randomUUID().toString(); map.put(key[i], UUID.randomUUID().toString()); } end = System.currentTimeMillis(); hashTableW += (end - start); System.out.println(&quot;Hashtable插入耗时 = &quot; + (end - start) + &quot; ms&quot;); // Hashtable 读取 start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) { index = random.nextInt(size); map.get(key[index]); } end = System.currentTimeMillis(); hashTableR += (end - start); System.out.println(&quot;Hashtable读取耗时 = &quot; + (end - start) + &quot; ms&quot;); } } Map遍历初始化数据Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;(); map.put(&quot;key1&quot;,&quot;value1&quot;); map.put(&quot;key2&quot;,&quot;value2&quot;); 增强for循环遍历使用keySet()遍历 for(String key:map.keySet()){ System.out.println(key+&quot;:&quot;+map.get(key)) } 使用entrySet()遍历 for(Map.Entry&lt;String,String&gt; entry:map.entrySet()){ System.out.println(entry.getKey()+&quot;:&quot;+entry.getValue()); } 迭代器遍历使用keySet()遍历 Iterator&lt;String&gt; iterator = map.keySet().iterator(); while(iterator.hasNext()){ String key = iterator.next(); System.out.println(key+&quot;:&quot;+map.get(key)); } 使用entrySet()遍历 Iterator&lt;Map.Entry&lt;String,String&gt;&gt; iterator = map.entrySet().iterator(); while(iterator.hasNext()){ Map.Entry&lt;String,String&gt; entry = iterator.next(); System.out.println(entry.getKey()+&quot;:&quot;+entry.getValue()); } HashMap四种遍历方式性能比较package net.xsoftlab.baike; import java.util.HashMap; import java.util.Iterator; import java.util.Map; import java.util.Map.Entry; public class TestMap { public static void main(String[] args) { // 初始化，10W次赋值 Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); for (int i = 0; i &lt; 100000; i++) map.put(i, i); /** 增强for循环，keySet迭代 **/ long start = System.currentTimeMillis(); for (Integer key : map.keySet()) { map.get(key); } long end = System.currentTimeMillis(); System.out.println(&quot;增强for循环，keySet迭代 -&gt; &quot; + (end - start) + &quot; ms&quot;); /** 增强for循环，entrySet迭代 */ start = System.currentTimeMillis(); for (Entry&lt;Integer, Integer&gt; entry : map.entrySet()) { entry.getKey(); entry.getValue(); } end = System.currentTimeMillis(); System.out.println(&quot;增强for循环，entrySet迭代 -&gt; &quot; + (end - start) + &quot; ms&quot;); /** 迭代器，keySet迭代 **/ start = System.currentTimeMillis(); Iterator&lt;Integer&gt; iterator = map.keySet().iterator(); Integer key; while (iterator.hasNext()) { key = iterator.next(); map.get(key); } end = System.currentTimeMillis(); System.out.println(&quot;迭代器，keySet迭代 -&gt; &quot; + (end - start) + &quot; ms&quot;); /** 迭代器，entrySet迭代 **/ start = System.currentTimeMillis(); Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; iterator1 = map.entrySet().iterator(); Map.Entry&lt;Integer, Integer&gt; entry; while (iterator1.hasNext()) { entry = iterator1.next(); entry.getKey(); entry.getValue(); } end = System.currentTimeMillis(); System.out.println(&quot;迭代器，entrySet迭代 -&gt; &quot; + (end - start) + &quot; ms&quot;); } } 运行三次，比较结果 第一次 增强for循环，keySet迭代 -&gt; 37 ms 增强for循环，entrySet迭代 -&gt; 19 ms 迭代器，keySet迭代 -&gt; 14 ms 迭代器，entrySet迭代 -&gt; 9 ms 增强for循环，keySet迭代 -&gt; 29 ms 增强for循环，entrySet迭代 -&gt; 22 ms 迭代器，keySet迭代 -&gt; 19 ms 迭代器，entrySet迭代 -&gt; 12 ms 增强for循环，keySet迭代 -&gt; 27 ms 增强for循环，entrySet迭代 -&gt; 19 ms 迭代器，keySet迭代 -&gt; 18 ms 迭代器，entrySet迭代 -&gt; 10 ms 总结： 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。 使用entrySet遍历的速度比keySet快很多，是keySet的1.5倍左右。 Map排序HashMap、HashTable、LinkedHashMap排序 HashMapMap&lt;String,String&gt; map = new HashMap&lt;String,String&gt;(); map.put(&quot;b&quot;,&quot;b&quot;); map.put(&quot;a&quot;,&quot;c&quot;); map.put(&quot;c&quot;,&quot;a&quot;); //排序 List&lt;Map.Entry&lt;String,String&gt;&gt; list = new ArrayList&lt;Map.Entry&lt;String, String&gt;&gt;(map.entrySet().size()); Collections.sort(list, new Comparator&lt;Map.Entry&lt;String, String&gt;&gt;() { @Override public int compare(Map.Entry&lt;String, String&gt; o1, Map.Entry&lt;String, String&gt; o2) { return o1.getKey().compareTo(o2.getKey()); } }); for (Map.Entry&lt;String,String&gt; mapping: list) { System.out.println(mapping.getKey()+&quot;:&quot;+mapping.getValue()); } TreeMapMap&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;(new Comparator&lt;String&gt;() { @Override public int compare(String o1, String o2) { // 降序排序 return o1.compareTo(o2); } }); map.put(&quot;b&quot;, &quot;b&quot;); map.put(&quot;a&quot;, &quot;c&quot;); map.put(&quot;c&quot;, &quot;a&quot;); for (String key : map.keySet()) { System.out.println(key + &quot; ：&quot; + map.get(key)); } 按value排序(通用)Map&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;(); map.put(&quot;b&quot;, &quot;b&quot;); map.put(&quot;a&quot;, &quot;c&quot;); map.put(&quot;c&quot;, &quot;a&quot;); // 通过ArrayList构造函数把map.entrySet()转换成list List&lt;Map.Entry&lt;String, String&gt;&gt; list = new ArrayList&lt;Map.Entry&lt;String, String&gt;&gt;(map.entrySet()); // 通过比较器实现比较排序 Collections.sort(list, new Comparator&lt;Map.Entry&lt;String, String&gt;&gt;() { @Override public int compare(Map.Entry&lt;String, String&gt; mapping1, Map.Entry&lt;String, String&gt; mapping2) { return mapping1.getValue().compareTo(mapping2.getValue()); } }); for (String key : map.keySet()) { System.out.println(key + &quot; ：&quot; + map.get(key)); } 常用API 扩展List如何一边遍历一边删除public static void main(String[] args) { List&lt;String&gt; platformList = new ArrayList&lt;&gt;(); platformList.add(&quot;博客园&quot;); platformList.add(&quot;CSDN&quot;); platformList.add(&quot;掘金&quot;); for (String platform : platformList) { if (platform.equals(&quot;博客园&quot;)) { platformList.remove(platform); } } System.out.println(platformList); } java.util.ConcurrentModificationException异常了，翻译成中文就是：并发修改异常 使用Iterator的remove()方法public static void main(String[] args) { List&lt;String&gt; platformList = new ArrayList&lt;&gt;(); platformList.add(&quot;博客园&quot;); platformList.add(&quot;CSDN&quot;); platformList.add(&quot;掘金&quot;); Iterator&lt;String&gt; iterator = platformList.iterator(); while (iterator.hasNext()) { String platform = iterator.next(); if (platform.equals(&quot;博客园&quot;)) { iterator.remove(); } } System.out.println(platformList); } 使用for循环正序遍历public static void main(String[] args) { List&lt;String&gt; platformList = new ArrayList&lt;&gt;(); platformList.add(&quot;博客园&quot;); platformList.add(&quot;CSDN&quot;); platformList.add(&quot;掘金&quot;); for (int i = 0; i &lt; platformList.size(); i++) { String item = platformList.get(i); if (item.equals(&quot;博客园&quot;)) { platformList.remove(i); i = i - 1; } } System.out.println(platformList); } 使用for循环倒序遍历public static void main(String[] args) { List&lt;String&gt; platformList = new ArrayList&lt;&gt;(); platformList.add(&quot;博客园&quot;); platformList.add(&quot;CSDN&quot;); platformList.add(&quot;掘金&quot;); for (int i = platformList.size() - 1; i &gt;= 0; i--) { String item = platformList.get(i); if (item.equals(&quot;掘金&quot;)) { platformList.remove(i); } } System.out.println(platformList); }]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSON对象和JSON字符串]]></title>
    <url>%2F2020%2F06%2F05%2FJSON%E5%AF%B9%E8%B1%A1%E5%92%8CJSON%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[JSON对象对象的概念对象的属性是可以用： 对象.属性进行调用的。 var person={&quot;name&quot;:&quot;tom&quot;,&quot;sex&quot;:&quot;男&quot;}//json对象 console.log(person.name);//在控制台输出tom alert(typeof(person));//object 以上就是json对象。是一个可以用person.name这种方式进行属性的调用。第三行代码就是看person的类型为object类型 JSON字符串字符串，JS中的字符串是单引号或者双引号引起来的。那么json字符串就是如下var b = &apos;{&quot;name&quot;:&quot;tom&quot;,&quot;sex&quot;:&quot;男&quot;}&apos;;//json字符串 console.log(b);//{&quot;name&quot;:&quot;tom&quot;,&quot;sex&quot;:&quot;男&quot;}; alert(typeof(b));//string 以上b就是一个字符串，是一个string类型 JSON字符串和JSON对象的转换JSON字符串转json对象，调用parse方法；var b = &apos;{&quot;name&quot;:&quot;tom&quot;,&quot;sex&quot;:&quot;男&quot;}&apos;;//json字符串 console.log(b);//{&quot;name&quot;:&quot;tom&quot;,&quot;sex&quot;:&quot;男&quot;}; alert(typeof(b));//string var objb = JSON.parse(b); console.log(objb.name); JSON对象转JSON字符串var person={&quot;name&quot;:&quot;tom&quot;,&quot;sex&quot;:&quot;男&quot;}//json对象 console.log(person.name);//在控制台输出tom alert(typeof(person));//object var personStr = JSON.stringify(person); alert(typeof(personStr));//string console.log(personStr);//{&quot;name&quot;:&quot;tom&quot;,&quot;sex&quot;:&quot;男&quot;} 字符串转json对象场景： 后台返回的person对象上有herf属性，是一个字符串 href = {&apos;href&apos;:&apos;../juvenile/summerCamp/summerCamp.html&apos;,&apos;isLogined&apos;:&apos;1&apos;,&apos;param&apos;:{&apos;activityId&apos;:&apos;2141108450038362510&apos;}} 要解析到herf var configData = eval(&apos;(&apos; + href + &apos;)&apos;);//由字符串转换为JSON对象 console.log(configData.href)]]></content>
      <categories>
        <category>JS</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringSecurity验证流程解析]]></title>
    <url>%2F2020%2F05%2F28%2FSpringSecurity%E9%AA%8C%E8%AF%81%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[SpringSecurity TokenAuthenticationFilter if (httpRequest.getServletPath().equals(loginLink)) { //如果是登录或注销的话，设置不沿着过滤器向下 doNotContinueWithRequestProcessing(httpRequest); checkLoginAnDoSomething(httpRequest, httpResponse, token); } checkLoginAnDoSomething(httpRequest, httpResponse, token); example: Authorization=Basic YWRtaW46MTIzNDU2 (admin:123456) private boolean checkLoginAnDoSomething(HttpServletRequest httpRequest, HttpServletResponse httpResponse, String token) throws IOException { String authorization = httpRequest.getHeader(CacheConstant.AUTHORIZATION); ReturnStatus result = null; if (authorization != null) { result = checkBasicAuthorization(authorization, httpRequest, httpResponse, token); } if (null == result) { result = new ReturnStatus(false); result.getErrors().add(new MError(MErrorCode.e9000)); result.setMessage(MErrorCode.e9000.desc()); } String body; try { body = JsonUtils.toJson(result);// JsonParserFactory.getParser().toJson(result).toString(); httpResponse.setContentType(&quot;text/html;charset=UTF-8&quot;); httpResponse.getWriter().write(body); } catch (Exception e) { throw new IOException(e); } return result.isSuccess(); } checkBasicAuthorization(authorization, httpRequest, httpResponse, token); private ReturnStatus checkBasicAuthorization(String authorization, HttpServletRequest httpRequest, HttpServletResponse httpResponse, String token) throws IOException { StringTokenizer tokenizer = new StringTokenizer(authorization); if (tokenizer.countTokens() &lt; 2) { return null; } if (!tokenizer.nextToken().equalsIgnoreCase(CacheConstant.BASIC_AUTH_PREFIX)) { return null; } String base64 = tokenizer.nextToken(); String loginPassword = new String(Base64.decode(base64.getBytes(StandardCharsets.UTF_8))); tokenizer = new StringTokenizer(loginPassword, &quot;:&quot;); String username = tokenizer.nextToken(); String pwd = tokenizer.nextToken(); String password = passwordEncoder.encodePassword(pwd, null); ReturnStatus status = checkUsernameAndPassword(username, password, httpRequest, httpResponse, token); // 登录成功后返回登录状态 return status; } private ReturnStatus checkUsernameAndPassword(String username, String password, HttpServletRequest httpRequest,HttpServletResponse httpResponse, String oldtoken) private ReturnStatus checkUsernameAndPassword(String username, String password, HttpServletRequest httpRequest, HttpServletResponse httpResponse, String oldtoken) throws IOException { ReturnStatus returnResult = new ReturnStatus(false); TokenInfo tokenInfo = authenticationService.authenticate(username, password, httpRequest); if (tokenInfo != null &amp;&amp; tokenInfo.getUserDetails() != null) { VerifyContext verifyContext = (VerifyContext) tokenInfo.getUserDetails(); Account account = (Account) verifyContext.getUser().getEntity(); if (account.isLogin()) { if (null != oldtoken &amp;&amp; !&quot;&quot;.equals(oldtoken) &amp;&amp; !&quot;null&quot;.equals(oldtoken) &amp;&amp; !&quot;undefined&quot;.equals(oldtoken)) { logger.info(&quot;tokenInfo.setToken use the oldtoken Token :{}&quot;, tokenInfo.getToken()); tokenInfo.setToken(oldtoken); } logger.info(&quot;the new Token :{},entity:{}&quot;, tokenInfo.getToken(), tokenInfo); this.cacheManager.saveObject(CacheEnum.TOKEN, tokenInfo.getToken(), tokenInfo.getUserDetails(), CacheConstant.USER_SESSION_TIME); logger.info(&quot;the token:{}, save object:{}&quot;, tokenInfo.getToken(), this.cacheManager.getObject(CacheEnum.TOKEN, tokenInfo.getToken())); returnResult.setEntity(account); returnResult.setSuccess(true); } else { returnResult.getErrors().add(new MError(MErrorCode.e9001)); returnResult.setMessage(MErrorCode.e9001.desc()); } httpResponse.setHeader(CacheConstant.HEADER_TOKEN, tokenInfo.getToken()); } else { logger.error(&quot;User {} ,Password {} Unauthorized!&quot;, username, password); returnResult.getErrors().add(new MError(MErrorCode.e9000)); returnResult.setMessage(MErrorCode.e9000.desc()); } return returnResult; } 基于用户-角色-权限设计Account accountName//账号名 accountPwd//密码 status//状态 accountType//账号类型 entityID//实体id lastLoginTime//最后一次登录时间 loginTimes//登录次数 roleIds//角色列表 //一个账号可以关联角色，角色呈树状结构，可多选 Function name//功能点名称 parentId//父级 memo//描述 action//资源url order//排序 icon//图标 permissionCode//唯一权限标识 permissionName//权限名称 Role name//角色名称 parentId//父级 memo//描述 functionIds//功能点 order//排序 //角色也是呈树状菜单的，角色里面有权限配置，所有的功能点以树状结构展示，通过角色去关联功能点 递归菜单和角色// 获取标准JSON数据 public static List&lt;Map&lt;String, Object&gt;&gt; getStandardJSON() { // 根据不同框架获取对应的List数据 List&lt;Map&lt;String, Object&gt;&gt; queryList = query.find(); List&lt;Map&lt;String, Object&gt;&gt; parentList = Lists.newArrayList(); for (Map&lt;String, Object&gt; map : queryList) { if (map.get(&quot;pId&quot;).equals(&quot;0&quot;)) { parentList.add(map); } } recursionChildren(parentList, queryList); return parentList; } // 递归获取子节点数据 public static void recursionChildren (List&lt;Map&lt;String, Object&gt;&gt; parentList, List&lt;Map&lt;String, Object&gt;&gt; allList) { for (Map&lt;String, Object&gt; parentMap : parentList) { List&lt;Map&lt;String, Object&gt;&gt; childrenList = Lists.newArrayList(); for (Map&lt;String, Object&gt; allMap : allList) { if (allMap.get(&quot;pId&quot;).equals(parentMap.get(&quot;id&quot;))) { childrenList.add(allMap); } } if (!ParamValidUtils.isEmpty(childrenList)) { parentMap.put(&quot;children&quot;, childrenList); recursionChildren(childrenList, allList); } } }]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot系列]]></title>
    <url>%2F2020%2F05%2F26%2FSpringBoot%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[Spring Boot简介 Spring boot是Spring家族中的一个全新的框架，它用来简化Spring应用程序的创建和开发过程，也可以说Spring boot能简化我们之前采用SpringMVC+Spring+Mybatis框架进行开发的过程。 在以往我们采用SpringMVC+Spring+Mybatis框架进行开发的时候，搭建和整合三大框架，我们需要做很好工作，比如配置web.xml，配置Spring，配置Mybatis,并将它们整合在一起等，而Spring boot框架对此开发过程进行了革命性的颠覆，抛弃了繁琐的xml配置过程，采用大量的默认配置简化我们的开发过程。 所以采用Spring boot可以非常容易和快速的创建基于Spring框架的应用程序，它让编码变简单了，配置变简单了，部署变简单了，监控也变简单了。 正因为Spring boot它化繁为简，让开发变得极其简单和快捷，所以在业界备受关注。Spring boot在国内的关注趋势也日渐超过Spring。 能够快速创建基于Spring的应用程序。（简化配置） 能够直接使用java的main方法启动内嵌的Tomcat，Jetty服务器运行Spring boot程序，不需要部署war包文件。 提供约定的starter POM来简化来简化Maven配置，让Maven配置变得简单。 根据项目的maven依赖配置，Spring boot自动配置Spring,SpringMVC等其它开源框架。.提供程序的健康检查等功能。（检查内部的运行状态等）基本可以完全不使用xml配置文件，采用注解配置。（或者默认约定的配置，代码中已经实现） 微服务微服务：架构风格 一个应用应该是一组小型服务；可以通过HTTP的方式进行互通； 每一个功能元素最终都是一个可独立替换和独立升级的软件单元； 环境准备环境约束-jdk1.8 -maven 3.x -springboot1.5.9RELEASE MAVEN设置123456789101112&lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt;&lt;/profile&gt; Spring Boot HelloWorld浏览器发送hello请求，服务器接受请求并处理，响应HelloWorld字符串； 创建一个maven工程（jar）导入springBoot 依赖&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写主程序，启动Spring Boot应用/** * @Date2020/5/23 12:53 * SpringBootApplication来标注一个主程序 **/ @SpringBootApplication public class HelloWorldMainApplication { public static void main(String[] args) { //spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); } } 编写相关的Controller、Service运行主程序测试简化部署&lt;!--可以将应用打包成一个可执行的jar宝--&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 将这个应用打成jar包，直接用java -jar的命令进行执行；终止运行netstat -aon|findstr &quot;8080&quot; taskkill /f /pid 8976 终止jar命令运行的程序 Hello World探究pom文件父文件&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; Spring Boot的版本仲裁中心 以后我们导入依赖默认是不需要写版本号的（没有在dependencies里面管理的依赖自然需要声明版本号） 导入的依赖&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; spring-boot-starter-web： spring-boot-starter：spring-boot场景启动器；帮我们导入了web模块正常运行所依赖的组件Spring Boot将所有功能场景都抽取出来，做成一个个starters(启动器)，只需要在项目里引入这些starter相关场景的所有依赖都会导入进来。要用什么功能就导入什么场景的启动器 主程序类/** * @Date2020/5/23 12:53 * SpringBootApplication来标注一个主程序,说明这是一个Spring Boot应用 **/ @SpringBootApplication public class HelloWorldMainApplication { private static Logger log= LoggerFactory.getLogger(HelloWorldMainApplication.class); public static void main(String[] args) { log.info(&quot;HelloWorldMainApplication is success!&quot;); //spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); } } @SpringBootApplication Spring Boot应用标注在某个类上说明这个类是SpringBoot的主配置类，SpringBoot就应该运行这个类的main方法来启动SpringBoot应用。 @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan( excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class} ), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class} )} ) @SpringBootConfiguration：SpringBoot的配置类； ​ 标注在某个类上，标识这是一个SpringBoot的配置类； ​ @Configuration:配置类上来标注这个注解： ​ 配置类—配置文件；配置类也是容器中的一个组件；@Componet @EnableAutoConfiguration：开启自动配置功能 ​ 以前需要配置的东西，SpringBoot帮我们自动配置；@EnableAutoConfiguration告诉SpringBoot开启自动配置功能；这样自动配置才能生效。 @AutoConfigurationPackage @Import({EnableAutoConfigurationImportSelector.class}) @AutoConfigurationPackage：自动配置包 @Import({Registrar.class}) ​Spring的底层注解@Import，给容器中导入一个组件；导入的组件由Registrar.class 将主配置类（@SpringBootApplication标注的类）的所在包及下面所有子包里面的所有组件扫描到 Spring容器； ​@Import({EnableAutoConfigurationImportSelector.class}) ​给容器中导入组件 ​EnableAutoConfigurationImportSelector：导入那些组件的选择器； ​将所有需要导入的组件以全类名的方式返回； ​会给容器中导入非常多的自动配置类（xxxAutoConfiguration）;就是给容器中导入这个场景需要的所 有组件，并配置好这些组件；免去了我们手动编写配置注入功能组件的工作。 SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Spring Boot在启动的时候从类路径下的”META-INF/spring.factories”中获取EnableAutoConfiguration指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作。 J2EE的整体整合解决方案和自动配置都在m2\repository\org\springframework\boot\spring-boot-autoconfigure\1.5.9.RELEASE\spring-boot-autoconfigure-1.5.9.RELEASE.jar 使用Spring Initializer快速创建Spring Boot项目IDE都支持使用Spring的项目创建向导快速创建一个Spring Boot项目； 选择我们需要的模块，向导会联网进行项目的创建； 默认生成的Spring Boot项目 主程序已经生成好了，只需要实现我们自己的逻辑 resources文件夹的目录结构 static:保存所有的js css images templates:保存所有的页面模板；（Spring Boot默认jar包使用嵌入式的Tomcat,默认不支持JSP页面）；可以使用模板引擎（freemarker、thymeleaf）; application.properties:Spring Boot 的配置文件，可以修改一些默认设置； 配置文件SpringBoot使用一个全局的配置文件，配置文件名是固定的；application.properties application.yml 配置文件的作用：修改SpringBoot自动配置的默认值 SpringBoot在底层都给我们自动配置好； YAML是一个标记语言：以前的配置大都使用xxx.xml文件，而yaml以数据为中心，比json,xml更适合作配置文件 YAML语法k: v :标识一对键值对（空格必须有）以空格的缩进来控制层级关系，只要左对齐的一列数据，都是一个层级的，属性和值也是大小写敏感； 值的写法字面量：普通的值（数字、字符串、布尔） ​ 字面量直接来写，字符串默认不用加上单引号或者双引号 ​ “”:双引号，不会转义字符串里面的特殊字符，特殊字符会作为本身想表示的意思 ​ name: “zhangsan \n lisi” 输出zhangsan 换行 lisi ​ ‘’:单引号，会转义特殊字符，输出zhangsan \n lisi 对象、map（属性和值）（键值对） ​k:v :对象还是k:v的形式 friends: lastName: zhangsan age: 18 行内写法 friends{lastName:zhangsan,age:18} 数组（list、set） pets: - cat - dog - pig 行内写法 pets:{cat,dog,pig} 配置文件的注入和校验properities配置文件在idea中默认utf-8可能会乱码person: name: zhangsan age: 18 boss: false birth: 2020/12/12 map: {k1: v1,k2: 12} objectList: - lisi - wangwu dog: name: mumu age: 2 javaBean /** * @ClassNamePerson * @Description 将配置文件中的每一个属性的值映射到这个组件中 * @Author * @Date2020/5/23 16:08 * @ConfigurationProperties告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定 * prefix = &quot;person&quot;:配置文件中哪个下面的所有属性进行一一映射 * 只有这个组件是容器中的组件，才能用容器提供的@ConfigurationProperties功能,需要加上@Component * @ConfigurationProperties(prefix = &quot;person&quot;)默认从全局配置文件中获取值 **/ @Component @ConfigurationProperties(prefix = &quot;person&quot;) public class Person { private String name; private Integer age; private boolean boss; private Date birth; private Map&lt;String,Object&gt; map; private List&lt;Object&gt; objectList; private Dog dog; 我们可以导入配置文件处理器，以后编写配置就有提示了 &lt;!--导入配置文件处理器，配置文件进行绑定就会有提示--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; @ConfigurationProperties @Value 功能 批量注入文件的属性 一个个指定 松散绑定 支持（lastName,last-name） 不支持 SpEL 不支持 支持 JSR303数据校验 支持 不支持 复杂类型封装 支持 不支持 配置文件yml还是properties都可获取到值 如果只需要获取简单属性值可用@Value @PropertySource&amp;ImportResource@PropertySource:加载指定的配置文件，需要指定配置文件的路径 /** * @Description 将配置文件中的每一个属性的值映射到这个组件中 * @ConfigurationProperties告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定 * prefix = &quot;person&quot;:配置文件中哪个下面的所有属性进行一一映射 * 只有这个组件是容器中的组件，才能用容器提供的@ConfigurationProperties功能,需要加上@Component **/ @PropertySource(value = {&quot;classpath:person.properties&quot;}) @Component @ConfigurationProperties(prefix = &quot;person&quot;) public class Person { private String name; private Integer age; private boolean boss; private Date birth; private Map&lt;String,Object&gt; map; private List&lt;Object&gt; objectList; private Dog dog; @ImportResource:导入Spring的配置文件，让配置文件里面的内容生效； Spring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别，想让Spring的配置文件生效，加载进来；@ImportResource需要标注在一个配置类上 @ImportResource(locations = {&quot;classpath:beans.xml&quot;}) 导入Spring的配置文件，让其生效 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;helloService&quot; class=&quot;com.think.hello.service.HelloService&quot;&gt;&lt;/bean&gt; &lt;/beans&gt; SpringBoot推荐给容器中添加组件的方式，推荐使用全注解的方式； 1、配置类===Spring配置文件 2、@Bean给容器中添加组件 @Configuration public class myAppConfig { //将方法的返回值添加到容器，容器中这个组件默认的id就是方法名 @Bean public HelloService helloService(){ return new HelloService(); } } 配置文件占位符占位符后期之前配置的值，如果没有可用：指定默认值 person: name: zhangsan${random.uuid} boss: false age: ${random.int} birth: 2020/12/12 map: {k1: v1,k2: 12} objectList: - lisi - wangwu - zhangsan dog: name: ${person.hello:hello}mumu age: 2 Profile多profile文件我们在主配置文件编写的时候，文件名可用applicaton-{profile}.properties/yml 默认使用application.properties的配置 yml支持多文档块的方式server: port: 8080 spring: profiles: active: prod --- server: port: 8081 spring: profiles: dev --- server: port: 8082 spring: profiles: prod 激活指定profile在配置文件中指定spring.profiles.active=dev命令行的方式​ 在启动配置里 –spring.profiles.active=dev或java -jar xxx.jar –spring.profiles.active=dev 虚拟机参数​ -Dspring.profiles.active=dev 配置文件的加载默认的优先级由高到低高优先级的配置会覆盖低优先级的配置生效； SpringBoot会从这四个位置全部加载主配置文件；互补配置； -file:./conifg/ -file:./ -classpath:/config/ -classpath:/ 我们还可用通过spring.config.location来改变默认的配置文件位置 项目打包后可用命令行参数的形式，启动项目的时候来指定配置文件的新位置，指定配置文件会和默认加载的这些配置文件共同起作用形成配置 -jar xxx.jar –server.port=8080 自动配置原理配置文件能配置的属性参照 https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html#common-application-properties @SpringBootApplication @SpringBootApplication是一个复合注解或派生注解，在@SpringBootApplication中有一个注解@EnableAutoConfiguration，该注解是开启自动配置 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { @EnableAutoConfiguration也是一个派生注解，其中的关键功能是由@Import提供，其导入的AutoConfigurationImportSelector的selectImports()方法通过SpringFactoriesLoader.loadFactoryNames()扫描所有具有META-INF/spring.factories的jar包。 spring-boot-autoconfigure-x.x.x.x.jar里就有一个spring.factories文件。spring.factories文件由一组一组的key=value的形式，其中一个key是EnableAutoConfiguration类的全类名，而它的value是一个xxxxAutoConfiguration的类名的列表， 这些类名以逗号分隔。spring-boot-autoconfigure-x.x.x.x.jar -&gt; META-INF/spring.factories -&gt; org.springframework.boot.autoconfigure.xxx.xxxAutoConfiguration 类列表将会被实例化到Spring容器。SpringBoot项目启动时，@SpringBootApplication用在启动类SpringApplication.run()的内部就会置顶selectImports()方法，找到所有JavaConfig自动配置类的全限定名对应的class,然后将所有自动配置类加载到Spring容器中。 以redis自动配置，解析Spring自动配置原理将redis starter依赖加入&lt;!--redis jar--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration会被实例化到容器。该类为什么会被实例化？ 因为它在META-INF/spring.factories的Auto Configure列表。org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration被实例化，然而redis也会被实例化即创建RedisTemplate在Spring容器。 然而实例化redis对象是有条件的即@ConditionalOnClass({RedisOperations.class})，意思：当给定的类名在类路径上存在，则实例化当前Bean。 也就是想Spring创建redis实例对象，必须需要将redis starter包：spring-boot-starter-data-redis依赖引入。有了redis starter依赖springboot自动配置就会检测到classpath路径下有相关的类，然后就可以实例化对应的类了，这就是自动配置的原理。 知识点：类上有该注解@Configuration，类被实例化 时@bean会自动执行，生成对应的bean实例，放入Spring容器。 org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration会导入JedisConnectionConfiguration.class @Import({LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class}) JedisConnectionConfiguration.class有注解：@ConditionalOnClass({GenericObjectPool.class, JedisConnection.class, Jedis.class}) spring是怎样读取redis配置参数？关键是org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration的注解：@EnableConfigurationProperties({RedisProperties.class}) 知识点：@EnableConfigurationProperties会将配置文件的key-value映射成Java对象。redis配置类，如果没redis配置，使用本地的redis这需要本地安装redis服务，如果有redis配置就设置redis host、port等属性 appliccation.yml redis配置。必须以spring.redis开头 spring-boot-starterSpringBoot 可以省略众多的繁琐配置，它的众多starter可以说功不可没。 例如集成redis,只需要pom.xml中引入spring-boot-starter-data-redis,配置文件application.yml中加入spring.redis.database等几个关键配置项即可，常用的starter还有spring-boot-starter-web、spring-boot-starter-test等，相比传统的xml配置大大减少了集成的工作量。 原理利用starter实现自动化配置只需要两个条件–maven依赖、配置文件。引入maven实质就是导入jar包，spring-boot启动的时候会找到starter jar包中的resources/META-INF/spring.factories文件，根据spring.factories文件中的配置，找到需要自动配置的类。 注解 说明 @Configuration 表明是一个配置文件，被注解的类将成为一个bean配置类 @ConditionalOnClass 当classpath下发现该类的情况下进行自动配置 @ConditionalOnBean 当classpath下发现该类的情况下进行自动配置 @EnableConfigurationProperties 使@ConfigurationProperties注解生效 @AutoConfigureAfter 完成自动配置后实例化这个bean 实现pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.air&lt;/groupId&gt; &lt;artifactId&gt;starter-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;starter-demo&lt;/name&gt; &lt;description&gt;spring-boot-starter demo&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- Source --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; spring-boot-configuration-processor 的作用是编译时生成 spring-configuration-metadata.json ，在IDE中编辑配置文件时，会出现提示。 打包选择jar-no-fork，因为这里不需要main函数。 EnableDemoConfiguration@Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited public @interface EnableDemoConfiguration { } DemoProperties@Data @ConfigurationProperties(prefix = &quot;demo&quot;) public class DemoProperties { private String name; private Integer age; } name和age对应application.properties里面的demo.name和demo.age DemoAutoConfiguration@Configuration @ConditionalOnBean(annotation = EnableDemoConfiguration.class) @EnableConfigurationProperties(DemoProperties.class) public class DemoAutoConfiguration { @Bean @ConditionalOnMissingBean DemoService demoService (){ return new DemoService(); } } 这里设置自动配置的相关条件，和相关操作，由于这里只想写一个最简单的demo，所以这里只需要简单注入一个bean，没有复杂逻辑，实际开发中，这个类是最关键的。 DemoServicepublic class DemoService { @Autowired private DemoProperties demoProperties; public void print() { System.out.println(demoProperties.getName()); System.out.println(demoProperties.getAge()); } } 这里不需要@Service，因为已经通过DemoAutoConfiguration注入spring容器了。 spring.factories在resources/META-INF/下创建spring.factories文件:org.springframework.boot.autoconfigure.EnableAutoConfiguration=\com.air.starterdemo.config.DemoAutoConfiguration告诉spring-boot，启动时需要扫描的类。 测试pom.xml 本地mvn install之后，在新的spring-boot项目里面引入 com.air starter-demo 0.0.1-SNAPSHOT 配置文件demo.name = ooo demo.age = 11 如果使用的是IDEA，在编辑时会出现提示。 测试@SpringBootApplication @EnableDemoConfiguration public class Demo1Application { @Autowired private DemoService demoService; public static void main(String[] args) { SpringApplication.run(Demo1Application.class, args); } @PostConstruct public void test() { demoService.print(); } } 启动main函数，控制台会打印出配置文件中的name和age，一个简单的spring-boot-starter就写好了 spring.factories# Initializers org.springframework.context.ApplicationContextInitializer=\ org.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\ org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener # Application Listeners org.springframework.context.ApplicationListener=\ org.springframework.boot.autoconfigure.BackgroundPreinitializer # Auto Configuration Import Listeners org.springframework.boot.autoconfigure.AutoConfigurationImportListener=\ org.springframework.boot.autoconfigure.condition.ConditionEvaluationReportAutoConfigurationImportListener # Auto Configuration Import Filters org.springframework.boot.autoconfigure.AutoConfigurationImportFilter=\ org.springframework.boot.autoconfigure.condition.OnBeanCondition,\ org.springframework.boot.autoconfigure.condition.OnClassCondition,\ org.springframework.boot.autoconfigure.condition.OnWebApplicationCondition # Auto Configure org.springframework.boot.autoconfigure.EnableAutoConfiguration=\ org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\ org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\ org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\ org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\ org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\ org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\ org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\ org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration,\ org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\ org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\ org.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\ org.springframework.boot.autoconfigure.dao.PersistenceExceptionTranslationAutoConfiguration,\ org.springframework.boot.autoconfigure.data.cassandra.CassandraDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.cassandra.CassandraRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.couchbase.CouchbaseRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.elasticsearch.ReactiveElasticsearchRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.elasticsearch.ReactiveElasticsearchRestClientAutoConfiguration,\ org.springframework.boot.autoconfigure.data.jdbc.JdbcRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.jpa.JpaRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.ldap.LdapRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.mongo.MongoDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.mongo.MongoReactiveDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.mongo.MongoReactiveRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.mongo.MongoRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.neo4j.Neo4jDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.neo4j.Neo4jRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.solr.SolrRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.r2dbc.R2dbcDataAutoConfiguration,\ org.springframework.boot.autoconfigure.data.r2dbc.R2dbcRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.r2dbc.R2dbcTransactionManagerAutoConfiguration,\ org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration,\ org.springframework.boot.autoconfigure.data.redis.RedisReactiveAutoConfiguration,\ org.springframework.boot.autoconfigure.data.redis.RedisRepositoriesAutoConfiguration,\ org.springframework.boot.autoconfigure.data.rest.RepositoryRestMvcAutoConfiguration,\ org.springframework.boot.autoconfigure.data.web.SpringDataWebAutoConfiguration,\ org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration,\ org.springframework.boot.autoconfigure.flyway.FlywayAutoConfiguration,\ org.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration,\ org.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAutoConfiguration,\ org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration,\ org.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration,\ org.springframework.boot.autoconfigure.hateoas.HypermediaAutoConfiguration,\ org.springframework.boot.autoconfigure.hazelcast.HazelcastAutoConfiguration,\ org.springframework.boot.autoconfigure.hazelcast.HazelcastJpaDependencyAutoConfiguration,\ org.springframework.boot.autoconfigure.http.HttpMessageConvertersAutoConfiguration,\ org.springframework.boot.autoconfigure.http.codec.CodecsAutoConfiguration,\ org.springframework.boot.autoconfigure.influx.InfluxDbAutoConfiguration,\ org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration,\ org.springframework.boot.autoconfigure.integration.IntegrationAutoConfiguration,\ org.springframework.boot.autoconfigure.jackson.JacksonAutoConfiguration,\ org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\ org.springframework.boot.autoconfigure.jdbc.JdbcTemplateAutoConfiguration,\ org.springframework.boot.autoconfigure.jdbc.JndiDataSourceAutoConfiguration,\ org.springframework.boot.autoconfigure.jdbc.XADataSourceAutoConfiguration,\ org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,\ org.springframework.boot.autoconfigure.jms.JmsAutoConfiguration,\ org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration,\ org.springframework.boot.autoconfigure.jms.JndiConnectionFactoryAutoConfiguration,\ org.springframework.boot.autoconfigure.jms.activemq.ActiveMQAutoConfiguration,\ org.springframework.boot.autoconfigure.jms.artemis.ArtemisAutoConfiguration,\ org.springframework.boot.autoconfigure.jersey.JerseyAutoConfiguration,\ org.springframework.boot.autoconfigure.jooq.JooqAutoConfiguration,\ org.springframework.boot.autoconfigure.jsonb.JsonbAutoConfiguration,\ org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration,\ org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration,\ org.springframework.boot.autoconfigure.ldap.embedded.EmbeddedLdapAutoConfiguration,\ org.springframework.boot.autoconfigure.ldap.LdapAutoConfiguration,\ org.springframework.boot.autoconfigure.liquibase.LiquibaseAutoConfiguration,\ org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration,\ org.springframework.boot.autoconfigure.mail.MailSenderValidatorAutoConfiguration,\ org.springframework.boot.autoconfigure.mongo.embedded.EmbeddedMongoAutoConfiguration,\ org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\ org.springframework.boot.autoconfigure.mongo.MongoReactiveAutoConfiguration,\ org.springframework.boot.autoconfigure.mustache.MustacheAutoConfiguration,\ org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration,\ org.springframework.boot.autoconfigure.quartz.QuartzAutoConfiguration,\ org.springframework.boot.autoconfigure.r2dbc.R2dbcAutoConfiguration,\ org.springframework.boot.autoconfigure.rsocket.RSocketMessagingAutoConfiguration,\ org.springframework.boot.autoconfigure.rsocket.RSocketRequesterAutoConfiguration,\ org.springframework.boot.autoconfigure.rsocket.RSocketServerAutoConfiguration,\ org.springframework.boot.autoconfigure.rsocket.RSocketStrategiesAutoConfiguration,\ org.springframework.boot.autoconfigure.security.servlet.SecurityAutoConfiguration,\ org.springframework.boot.autoconfigure.security.servlet.UserDetailsServiceAutoConfiguration,\ org.springframework.boot.autoconfigure.security.servlet.SecurityFilterAutoConfiguration,\ org.springframework.boot.autoconfigure.security.reactive.ReactiveSecurityAutoConfiguration,\ org.springframework.boot.autoconfigure.security.reactive.ReactiveUserDetailsServiceAutoConfiguration,\ org.springframework.boot.autoconfigure.security.rsocket.RSocketSecurityAutoConfiguration,\ org.springframework.boot.autoconfigure.security.saml2.Saml2RelyingPartyAutoConfiguration,\ org.springframework.boot.autoconfigure.sendgrid.SendGridAutoConfiguration,\ org.springframework.boot.autoconfigure.session.SessionAutoConfiguration,\ org.springframework.boot.autoconfigure.security.oauth2.client.servlet.OAuth2ClientAutoConfiguration,\ org.springframework.boot.autoconfigure.security.oauth2.client.reactive.ReactiveOAuth2ClientAutoConfiguration,\ org.springframework.boot.autoconfigure.security.oauth2.resource.servlet.OAuth2ResourceServerAutoConfiguration,\ org.springframework.boot.autoconfigure.security.oauth2.resource.reactive.ReactiveOAuth2ResourceServerAutoConfiguration,\ org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration,\ org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration,\ org.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration,\ org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration,\ org.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration,\ org.springframework.boot.autoconfigure.transaction.jta.JtaAutoConfiguration,\ org.springframework.boot.autoconfigure.validation.ValidationAutoConfiguration,\ org.springframework.boot.autoconfigure.web.client.RestTemplateAutoConfiguration,\ org.springframework.boot.autoconfigure.web.embedded.EmbeddedWebServerFactoryCustomizerAutoConfiguration,\ org.springframework.boot.autoconfigure.web.reactive.HttpHandlerAutoConfiguration,\ org.springframework.boot.autoconfigure.web.reactive.ReactiveWebServerFactoryAutoConfiguration,\ org.springframework.boot.autoconfigure.web.reactive.WebFluxAutoConfiguration,\ org.springframework.boot.autoconfigure.web.reactive.error.ErrorWebFluxAutoConfiguration,\ org.springframework.boot.autoconfigure.web.reactive.function.client.ClientHttpConnectorAutoConfiguration,\ org.springframework.boot.autoconfigure.web.reactive.function.client.WebClientAutoConfiguration,\ org.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration,\ org.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration,\ org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration,\ org.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration,\ org.springframework.boot.autoconfigure.web.servlet.MultipartAutoConfiguration,\ org.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration,\ org.springframework.boot.autoconfigure.websocket.reactive.WebSocketReactiveAutoConfiguration,\ org.springframework.boot.autoconfigure.websocket.servlet.WebSocketServletAutoConfiguration,\ org.springframework.boot.autoconfigure.websocket.servlet.WebSocketMessagingAutoConfiguration,\ org.springframework.boot.autoconfigure.webservices.WebServicesAutoConfiguration,\ org.springframework.boot.autoconfigure.webservices.client.WebServiceTemplateAutoConfiguration # Failure analyzers org.springframework.boot.diagnostics.FailureAnalyzer=\ org.springframework.boot.autoconfigure.diagnostics.analyzer.NoSuchBeanDefinitionFailureAnalyzer,\ org.springframework.boot.autoconfigure.flyway.FlywayMigrationScriptMissingFailureAnalyzer,\ org.springframework.boot.autoconfigure.jdbc.DataSourceBeanCreationFailureAnalyzer,\ org.springframework.boot.autoconfigure.jdbc.HikariDriverConfigurationFailureAnalyzer,\ org.springframework.boot.autoconfigure.r2dbc.ConnectionFactoryBeanCreationFailureAnalyzer,\ org.springframework.boot.autoconfigure.session.NonUniqueSessionRepositoryFailureAnalyzer # Template availability providers org.springframework.boot.autoconfigure.template.TemplateAvailabilityProvider=\ org.springframework.boot.autoconfigure.freemarker.FreeMarkerTemplateAvailabilityProvider,\ org.springframework.boot.autoconfigure.mustache.MustacheTemplateAvailabilityProvider,\ org.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAvailabilityProvider,\ org.springframework.boot.autoconfigure.thymeleaf.ThymeleafTemplateAvailabilityProvider,\ org.springframework.boot.autoconfigure.web.servlet.JspTemplateAvailabilityProvider 每一个autoconfigure类都是容器中的一个组件，都加入到容器中，用他们来做自动配置 每一个自动配置类进行自动配置功能; 以HttpEncodingAutoConfiguration为例解释自动配置原理 @Configuration(//表示这是一个自动配置类 proxyBeanMethods = false ) @EnableConfigurationProperties({ServerProperties.class})//启用configurationProperties功能，将配置文件中对应的值和xxxProperties绑定起来 @ConditionalOnWebApplication(//Spring底层@Conditional注解，根据不同条件，如果满足指定条件，整个配置里里面的配置就会生效，判断当前应用是否是web应用，是就生效不是就不生效 type = Type.SERVLET ) @ConditionalOnClass({CharacterEncodingFilter.class})//判断当前项目有没有这个类 CharacterEncodingFilter：SpringMVC中进行乱码解决的过滤器 @ConditionalOnProperty(//判断配置文件中是否存在某个配置server.servlet.encoding.enabled如果不存在判断也是成立的，即使我们的配置文件中不配置server.servlet.encoding.enabled=true,也是默认生效的 prefix = &quot;server.servlet.encoding&quot;,//从配置文件中获取指定的值和bean的属性进行绑定 value = {&quot;enabled&quot;}, matchIfMissing = true ) public class HttpEncodingAutoConfiguration { 根据当前不同的条件判断，决定这个配置类是否生效 如果生效，这个配置类就会给容器中添加各种组件，这些组件的属性是从对应的properties类中获取的，这些类里面的每一个又是和配置文件绑定的 @Bean//给容器中添加一个组件,这个组件的某些值需要从properties中获取 @ConditionalOnMissingBean public CharacterEncodingFilter characterEncodingFilter() { CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(org.springframework.boot.web.servlet.server.Encoding.Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(org.springframework.boot.web.servlet.server.Encoding.Type.RESPONSE)); return filter; } 所有在配置文件中能配置的属性都是xxxProperties类中封装着，配置文件能配置什么就可以参照某个功能对应的这个属性类 总结 1、SpringBoot启动会加载大量的自动配置类； ​ 2、我们看需要的功能有没有SpringBoot默认写好的自动配置类 ​ 3、我们再来看这个自动配置类中配置了哪些组件，只要我们要用的组件有，就不需要再来配置了， ​ 4、给容器中自动配置类添加组件的时候，会从properties类中获取某些属性，我们就可以再配置文件总指定这些属性的值。 xxxAutoConfiguration:自动配置类 给容器中添加组件 xxxProperties：封装配置文件中相关属性 自动配置类哪个生效了 我们可以通过debug=true来让控制台打印自动配置报告，这样就可以很方便知道哪些自动配置生效了 ============================ CONDITIONS EVALUATION REPORT ============================ Positive matches:（自动配置类启用的） ----------------- AopAutoConfiguration matched: - @ConditionalOnProperty (spring.aop.auto=true) matched (OnPropertyCondition) AopAutoConfiguration.ClassProxyingConfiguration matched: - @ConditionalOnMissingClass did not find unwanted class &apos;org.aspectj.weaver.Advice&apos; (OnClassCondition) - @ConditionalOnProperty (spring.aop.proxy-target-class=true) matched (OnPropertyCondition) DispatcherServletAutoConfiguration matched: - @ConditionalOnClass found required class &apos;org.springframework.web.servlet.DispatcherServlet&apos; (OnClassCondition) - found &apos;session&apos; scope (OnWebApplicationCondition) DispatcherServletAutoConfiguration.DispatcherServletConfiguration matched: - @ConditionalOnClass found required class &apos;javax.servlet.ServletRegistration&apos; (OnClassCondition) - Default DispatcherServlet did not find dispatcher servlet beans (DispatcherServletAutoConfiguration.DefaultDispatcherServletCondition) Negative matches:（没有启用的，没匹配成功的） ----------------- ActiveMQAutoConfiguration: Did not match: - @ConditionalOnClass did not find required class &apos;javax.jms.ConnectionFactory&apos; (OnClassCondition) AopAutoConfiguration.AspectJAutoProxyingConfiguration: Did not match: - @ConditionalOnClass did not find required class &apos;org.aspectj.weaver.Advice&apos; (OnClassCondition) ArtemisAutoConfiguration: Did not match: - @ConditionalOnClass did not find required class &apos;javax.jms.ConnectionFactory&apos; (OnClassCondition) SpringBoot与日志日志门面SLF4J日志实现Logback;SpringBoot:底层是Spring框架，Spring框架默认用JCL; SpringBoot选用SLF4J(日志的抽象层)和logback; 以后开发的时候，日志记录方法的调用，不应该来直接调用日志的实现类，而是调用日志抽象曾里面的方法。 应该给系统导入slf4j的jar和logback的实现jar import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class HelloWorld { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(HelloWorld.class); logger.info(&quot;Hello World&quot;); } } 每一个日志的实现框架都有自己的配置文件。使用slf4j后，配置文件还是做成日志实现框架自己本身的配置文件； 遗留问题不同系统有不同的日志框架，需要做到统一日志记录，即使别的框架和我一起使用slf4j进行输出 如何让系统中所有日志都统一到slf4j 将系统中其他日志框架排除，用中间包替换原有的日志框架，再导入slf4d其他的实现 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.3.0.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; SpringBoot 使用它来做日志 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;version&gt;2.3.0.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jul-to-slf4j&lt;/artifactId&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 总结：1、SpringBoot底层也是使用slf4j+logback的方式进行日志记录 ​ 2、SpringBoot也把其他的日志都替换成了slf4j ​ 3、中间替换包 ​ 4、如果要引入其他框架，一定要把这个框架的默认日志移除掉 ​ Spring框架用的commons-logging; &lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-console&lt;/artifactId&gt; &lt;version&gt;${activemq.version}&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; SpringBoot能自动适配所有的日志，而且底层使用slf4j+logback的方式记录日志，引入其他框架的时候，只需要把这个框架依赖的日志框架排除掉。 private static Logger logger = LoggerFactory.getLogger(HelloApplication.class); public static void main(String[] args) { SpringApplication.run(HelloApplication.class, args); //日志级别由低到高 //可以调整输出的日志级别 logger.trace(&quot;trace&quot;); logger.debug(&quot;debug&quot;); logger.info(&quot;HelloApplication is Success&quot;); logger.warn(&quot;warn&quot;); logger.error(&quot;error&quot;); } logging.pattern.console=%d{yyyy-MM-dd} [%thread] %-5level %logger{50} - %msg%n %d表示日期时间 %thread表示线程名 %-5level:级别从左显示5个字符的宽度 %logger{50}表示logger名字最长50个字符，否则按照句点分割 %msg:日志消息 %n换行符 SpringBoot修改默认日志配置 logging.level.com.think=trace #当前项目下生成springboot.log日志，可以指定完整的路径D:/springboot.log #logging.file.name=springboot.log #在当前磁盘的根路径下创建spring文件夹和里面的log文件夹；使用spring.log做为默认文件 logging.file.path=/spring/log #在控制台输出的日志的格式 logging.pattern.console=%d{yyyy-MM-dd} [%thread] %-5level %logger{50} - %msg%n #指定文件中日志的输出格式 logging.pattern.file=%d{yyyy-MM-dd} [%thread] %-5level %logger{50} - %msg%n &lt;!-- 日志记录器，日期滚动记录 --&gt; &lt;appender name=&quot;FILE_ERROR&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;${LOG_PATH}/log_error.log&lt;/file&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!-- 归档的日志文件的路径，%d{yyyy-MM-dd}指定日期格式，%i指定索引 --&gt; &lt;fileNamePattern&gt;${LOG_PATH}/error/log-error-%d{yyyy-MM-dd}.%i.log&lt;/fileNamePattern&gt; &lt;!-- 除按日志记录之外，还配置了日志文件不能超过2M，若超过2M，日志文件会以索引0开始， 命名日志文件，例如log-error-2013-12-21.0.log --&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP&quot;&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;/rollingPolicy&gt; &lt;!-- 追加方式记录日志 --&gt; &lt;append&gt;true&lt;/append&gt; &lt;!-- 日志文件的格式 --&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;${FILE_LOG_PATTERN}&lt;/pattern&gt; &lt;charset&gt;utf-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 此日志文件只记录error级别的 --&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;error&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; 可以按照slfj的日志适配图，进行相关的切换 idea依赖分析-pom.xml-右键-Diagrams-Show Dependencies SpringBoot与Web开发使用SpringBoot创建SpringBoot应用，选中我们需要的模块；SpringBoot已经默认将这些场景配置好了，只需要在配置文件中指定少量配置就可以运行起来；自己编写业务逻辑代码；自动配置原理这个场景SpringBoot帮我们配置了什么，能不能修改，能修改哪些配置，能不能扩展 xxxAutoConfiguration:帮我们给容器中自动配置组件 xxxProperties：配置类来封装配置文件中的内容 SpringBoot对静态资源的映射规则ResourceProperties可以设置静态资源有关的参数，缓存时间等 @ConfigurationProperties(prefix = &quot;spring.resources&quot;, ignoreUnknownFields = false) public class ResourceProperties { private static final String[] CLASSPATH_RESOURCE_LOCATIONS = { &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; }; /** * Locations of static resources. Defaults to classpath:[/META-INF/resources/, * /resources/, /static/, /public/]. @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { if (!this.resourceProperties.isAddMappings()) { logger.debug(&quot;Default resource handling disabled&quot;); return; } Duration cachePeriod = this.resourceProperties.getCache().getPeriod(); CacheControl cacheControl = this.resourceProperties.getCache().getCachecontrol().toHttpCacheControl(); if (!registry.hasMappingForPattern(&quot;/webjars/**&quot;)) { customizeResourceHandlerRegistration(registry.addResourceHandler(&quot;/webjars/**&quot;) .addResourceLocations(&quot;classpath:/META-INF/resources/webjars/&quot;) .setCachePeriod(getSeconds(cachePeriod)).setCacheControl(cacheControl)); } String staticPathPattern = this.mvcProperties.getStaticPathPattern(); if (!registry.hasMappingForPattern(staticPathPattern)) { customizeResourceHandlerRegistration(registry.addResourceHandler(staticPathPattern) .addResourceLocations(getResourceLocations(this.resourceProperties.getStaticLocations())) .setCachePeriod(getSeconds(cachePeriod)).setCacheControl(cacheControl)); } } 所有/webjars/**都去classpath:/META-INF/resources/webjars/找资源 webjars:以jar包的方式引入静态资源； https://www.webjars.org/ http://localhost:8080/webjars/jquery/3.5.1/jquery.js &lt;!--引入jquery-webjar--&gt; &lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;/dependency&gt; /**访问当前项目的任何资源（静态资源文件夹） &quot;/&quot;：当前项目的根路径 &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/ 以什么样的路径访问静态资源spring.mvc.static-path-pattern=/static/**Spring Boot 2.3要在配置文件配置静态资源访问路径 欢迎页，静态资源文件夹下所有的index.html页面；被”/**“映射 所有的**/favicon.ico都是在静态资源文件下找 模板引擎Thymeleaf 语法更简单，功能更强大 引入Thymeleaf&lt;!--模板引擎--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; @ConfigurationProperties(prefix = &quot;spring.thymeleaf&quot;) public class ThymeleafProperties { private static final Charset DEFAULT_ENCODING = StandardCharsets.UTF_8; //只要我们把html页面放在classpath:/templates/，thymeleaf就能自动渲染 public static final String DEFAULT_PREFIX = &quot;classpath:/templates/&quot;; public static final String DEFAULT_SUFFIX = &quot;.html&quot;; /** * Whether to check that the template exists before rendering it. */ private boolean checkTemplate = true; 导入thymeleaf的名称空间 &lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; 语法参考thymeleaf手册 SpringMVC自动配置Spring Boot自动配置好了SpringMVC以下是SpringBoot对SpringMVC的默认 The auto-configuration adds the following features on top of Spring’s defaults: Inclusion of ContentNegotiatingViewResolver and BeanNameViewResolver beans. 自动配置了ViewResolver（视图解析器：根据方法的返回值得到视图对象（View）视图对象决定如何渲染，是转发还是重定向） ContentNegotiatingViewResolver符合所有的视图解析器的 如何定制：我们可以给容器中添加一个视图解析器@Bean；自动将其组合进来 如何验证是否添加进来了-搜索DispatcherServlet-doDispatch-在这打断点，用debug方式运行 随便请求一个页面可以看到DispatcherServlet的viewResolvers里面已经包含了我们自定义的视图解析器 Support for serving static resources, including support for WebJars (covered later in this document)).静态资源文件夹路径，webjars Automatic registration of Converter, GenericConverter, and Formatter beans. Converter:转换器 类型转换使用 Formatter:格式化器日期的转换 Support for HttpMessageConverters (covered later in this document). HttpMessageConverters:SpringMVC用来转换Http请求和响应的；User–Json Automatic registration of MessageCodesResolver (covered later in this document). 定义错误代码生成规则的 Static index.html support.(静态首页访问) Custom Favicon support (covered later in this document).（favicon.ico） Automatic use of a ConfigurableWebBindingInitializer bean (covered later in this document). 我们可以配置一个ConfigurableWebBindingInitializer来替换默认的 初始化WebDataBinder;请求数据===JavaBean If you want to keep those Spring Boot MVC customizations and make more MVC customizations (interceptors, formatters, view controllers, and other features), you can add your own @Configuration class of type WebMvcConfigurer but without @EnableWebMvc. 使用WebMvcConfigurerAdapter扩展SpringMVC的功能编写一个配置类（@Configuration）,是WebMvcConfigurerAdapter类型；不能标注@EnableWebMvc。 既保留了所有的自动配置，也能用我们扩展的配置 //作用：请求me的时候会到success页面 @Configuration public class MyConfig extends WebMvcConfigurerAdapter{ @Override public void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(&quot;/me&quot;).setViewName(&quot;success&quot;); } } 原理： WebMvcAutoConfiguration 是SpringMVC的自动配置类 在做其他自动配置时会导入@Import(EnableWebMvcConfiguration.class) @Configuration(proxyBeanMethods = false) public static class EnableWebMvcConfiguration extends DelegatingWebMvcConfiguration implements ResourceLoaderAware { private final WebMvcConfigurerComposite configurers = new WebMvcConfigurerComposite(); //从容器中获取所有WebMvcConfigurer的配置 @Autowired(required = false) public void setConfigurers(List&lt;WebMvcConfigurer&gt; configurers) { if (!CollectionUtils.isEmpty(configurers)) { this.configurers.addWebMvcConfigurers(configurers); //一个参考实现，将所有的WebMvcConfigurer相关配置都来一起调用 @Override public void addViewControllers(ViewControllerRegistry registry) { for (WebMvcConfigurer delegate : this.delegates) { delegate.addViewControllers(registry); } } } } 容器中所有的WebMvcConfigurer都会一起起作用，包括自己写的配置类 全面接管SpringMVCSpringBoot对SpringMVC的自动配置不需要，所有都是我们自己配，只需要在配置类中添加@EnableWebMvc 原理： @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Documented @Import(DelegatingWebMvcConfiguration.class) public @interface EnableWebMvc {//这个注解上导入了DelegatingWebMvcConfiguration } @Configuration(proxyBeanMethods = false) public class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport {//DelegatingWebMvcConfiguration.class又继承了WebMvcConfigurationSupport @Configuration(proxyBeanMethods = false) @ConditionalOnWebApplication(type = Type.SERVLET) @ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class }) //容器种没有（WebMvcConfigurationSupport）这个组件的时候，这个自动配置类才生效 @ConditionalOnMissingBean(WebMvcConfigurationSupport.class) @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10) @AutoConfigureAfter({ DispatcherServletAutoConfiguration.class, TaskExecutionAutoConfiguration.class, ValidationAutoConfiguration.class }) public class WebMvcAutoConfiguration { @EnableWebMvc将WebMvcConfigurationSupport组件导入进来； 导入的WebMvcConfigurationSupport只是SpringMVC最基本的功能； 如何修改SpringBoot的默认配置 SpringBoot在自动配置很多组件的时候，先看容器中有没有用户自己配置的（@Bean,@Component）如果有就用用户配置的，如果没有才自动配置；如果有些组件可以有多个，他是将用户配置的和自己默认的组合起来 在SpringBoot种会有非常多的xxxConfigurer帮助我们进行扩展配置 登陆页面 //所有的WebMvcConfigurerAdapter组件都会一起起作用 @Bean//将组件注册到容器中 public WebMvcConfigurerAdapter webMvcConfigurerAdapter(){ WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() { @Override public void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(&quot;/&quot;).setViewName(&quot;login&quot;); registry.addViewController(&quot;/login.html&quot;).setViewName(&quot;login&quot;); } }; return adapter; } 国际化 编写国际化文件，抽取页面需要显示的国际化消息 SpringBoot自动配置好了管理国际化资源文件的组件 @Configuration(proxyBeanMethods = false)@ConditionalOnMissingBean(name = AbstractApplicationContext.MESSAGE_SOURCE_BEAN_NAME, search = SearchStrategy.CURRENT)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE)@Conditional(ResourceBundleCondition.class)@EnableConfigurationPropertiespublic class MessageSourceAutoConfiguration { private static final Resource[] NO_RESOURCES = {}; @Bean @ConfigurationProperties(prefix = “spring.messages”) public MessageSourceProperties messageSourceProperties() { return new MessageSourceProperties(); } @Bean public MessageSource messageSource(MessageSourceProperties properties) { ResourceBundleMessageSource messageSource = new ResourceBundleMessageSource(); if (StringUtils.hasText(properties.getBasename())) { //设置国际化资源文件的基础名 messageSource.setBasenames(StringUtils .commaDelimitedListToStringArray(StringUtils.trimAllWhitespace(properties.getBasename()))); } if (properties.getEncoding() != null) { messageSource.setDefaultEncoding(properties.getEncoding().name()); } messageSource.setFallbackToSystemLocale(properties.isFallbackToSystemLocale()); Duration cacheDuration = properties.getCacheDuration(); if (cacheDuration != null) { messageSource.setCacheMillis(cacheDuration.toMillis()); } messageSource.setAlwaysUseMessageFormat(properties.isAlwaysUseMessageFormat()); messageSource.setUseCodeAsDefaultMessage(properties.isUseCodeAsDefaultMessage()); return messageSource; } 去页面获取国际化的值 idea file-setting-FileEncodings-fileEncodeing将properties编码改成UTF-8，让他自动转成ascii码（只对当前项目生效）要改默认的在file-Other Setting-DefaultSetting来修改全局默认设置]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongo]]></title>
    <url>%2F2020%2F05%2F23%2Fmongo%2F</url>
    <content type="text"><![CDATA[什么是MongoDB ?MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。在高负载的情况下，添加更多的节点，可以保证服务器性能。MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。 NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写，是对不同于传统的关系型数据库的数据库管理系统的统称。NoSQL用于超大规模数据的存储。（例如谷歌或Facebook每天为他们的用户收集万亿比特的数据）。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。 主要特点 MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。 你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。 你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。 Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。 MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。 Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。 Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。 Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。 GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。 MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。 MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。 MongoDB安装简单。 安装 下载mongodb-linux-x86_64-3.6.3.tar.gz 解压tar -zxvf mongodb-linux-x86_64-3.0.6.tgz 在与bin同级的目录下建立data文件夹 mongo.config文件内容 12345678port=27017logpath=/airthink/mongodb-linux-x86_64-3.6.3/mongod.logpidfilepath=/airthink/mongodb-linux-x86_64-3.6.3/mongod.pidlogappend=truefork=truemaxConns=3000dbpath=/airthink/mongodb-linux-x86_64-3.6.3/dataauth=true 启动./mongodb-linux-x86_64-3.6.3/bin/mongo 1234567var schema = db.system.version.findOne(&#123;_id:&quot;authSchema&quot;&#125;);//驱动版本5--SCRAM-SHA-1需要修改成：3--Mongodb-CRschema. currentVersion = 3;db.system.version.save(schema);db.createUser(&#123;user:&quot;admin&quot;,pwd:&quot;mypass&quot;,roles:[&quot;readWriteAnyDatabase&quot;, &quot;userAdminAnyDatabase&quot;, &quot;dbAdminAnyDatabase&quot;]&#125;);windows下的启动命令到bin目录下 start mongod MongoDB用户角色配置 基本知识介绍MongoDB基本的角色 数据库用户角色：read、readWrite; 数据库管理角色：dbAdmin、dbOwner、userAdmin； 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager； 备份恢复角色：backup、restore； 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase 超级用户角色：root (这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase,其中MongoDB默认是没有开启用户认证的，也就是说游客也拥有超级管理员的权限。userAdminAnyDatabase：有分配角色和用户的权限，但没有查写的权限) 连接到MongoDB服务器12345678910111213141516171819202122232425262728293031建库操作开始./mongodb-linux-x86_64-3.6.3/bin/mongo 进入mongo shelluse admin(使用)db.createUser(&#123;user:&quot;root&quot;,pwd:&quot;password&quot;,roles:[&quot;root&quot;]&#125;)//创建root用户db.createUser(&#123;user:&quot;admin&quot;,pwd:&quot;admin&quot;,roles:[&#123;role:&quot;userAdminAnyDatabase&quot;,db:&quot;admin&quot;&#125;]&#125;)（创建admin用户）db.auth(&apos;admin&apos;, &apos;admin&apos;);（授权）修改mongod.conf文件security:authorization: enabled//启用授权重启MongoDB服务器service mongod restart创建数据库读写权限用户use admindb.auth(&quot;admin&quot;,&quot;password&quot;);use ballmatchdb.createUser(&#123; user: &quot;baidu&quot;, pwd: &quot;password&quot;, roles: [&#123;role: &quot;readWrite&quot;,db: &quot;baidu&quot;&#125;]&#125;) Java程序连接MongoDB1234567891011MongoCredential credential = MongoCredential.createCredential(&quot;username&quot;, &quot;dbName&quot;, &quot;password&quot;.toCharArray());ServerAddress serverAddress = new ServerAddress(&quot;192.168.10.242&quot;, 27017);MongoClient mongoClient = new MongoClient(serverAddress, Arrays.asList(credential));DB db = mongoClient.getDB(&quot;dbName&quot;);returndb;//方式二String sURI = String.format(&quot;mongodb://%s:%s@%s:%d/%s&quot;, &quot;username&quot;, &quot;password&quot;, &quot;192.168.10.242&quot;, 27017, &quot;dbName&quot;); MongoClientURI uri = new MongoClientURI(sURI); MongoClient mongoClient = new MongoClient(uri); DB db = mongoClient.getDB(&quot;dbName&quot;); 命令介绍12345678910111213141516171819修改用户密码db.updateUser( &quot;admin&quot;,&#123;pwd:&quot;password&quot;&#125;);密码认证db.auth(&quot;admin&quot;,&quot;password&quot;);MongoDB连接信息查询db.serverStatus().connections;关闭MongoDB服务use admin;db.shutdownServer();删除用户删除用户(需要root权限，会将所有数据库中的football用户删除)db.system.users.remove(&#123;user:&quot;baidu&quot;&#125;);删除用户(权限要求没有那么高，只删除本数据中的football用户)db.dropUser(&quot;baidu&quot;); mongoTemplate使用示例对匹配到的数据进行更新(inc)12345678910111213141516Query query = new Query();query.addCriteria(Criteria.where(&quot;accountName&quot;).is(username));if (null != password)&#123; query.addCriteria(Criteria.where(&quot;accountPwd&quot;).is(password));&#125;Account account = this.mongoTemplate.findOne(query, Account.class);if (account != null) &#123; Update update = new Update(); update.inc(&quot;loginTimes&quot;, 1); this.mongoTemplate.updateFirst(query, update, Account.class);//查找并更新&#125;mongoTemplate.updateFirst(Query.query(Criteria.where(&quot;_id&quot;).is(favor.getTypeId())),new Update().inc(&quot;favorTimes&quot;, 1), News.class);mongoTemplate.updateFirst(Query.query(Criteria.where(&quot;_id&quot;).is(favor.getTypeId())),new Update().inc(&quot;favorTimes&quot;, -1), News.class); 查询一条记录和保存（findOne/save）12345Account account = mongoTemplate.findOne(Query.query(Criteria.where(&quot;accountName&quot;).is(accountName)),Account.class);account.setAccountPwd(passwd);mongoTemplate.save(account); 多条件匹配（并的关系）123456789Criteria criteria = new Criteria();criteria.andOperator(Criteria.where(&quot;phone&quot;).is(phone), Criteria.where(&quot;status&quot;).is(0));List&lt;VerifyCode&gt; codes = mongoTemplate.find(Query.query(criteria), VerifyCode.class);Criteria criteria = new Criteria();criteria.andOperator(Criteria.where(&quot;entityID&quot;).is(entityID), Criteria.where(&quot;accountType&quot;).is(accountType), Criteria.where(&quot;weUnionId&quot;).ne(&quot;&quot;).ne(null));Account Account = mongoTemplate.findOne(Query.query(criteria), Account.class);Boolean isFavor = mongoTemplate.exists(Query.query(Criteria.where(&quot;typeId&quot;).is(favor.getTypeId()).and(&quot;type&quot;).is(favor.getType()).and(&quot;userId&quot;).is(favor.getUserId())), Favor.class); 多条件匹配（或的关系）123Criteria criteria = new Criteria();criteria.orOperator(Criteria.where(&quot;status&quot;).is(1), Criteria.where(&quot;status&quot;).is(0));List&lt;VerifyCode&gt; codes = mongoTemplate.find(Query.query(criteria), VerifyCode.class); 根据id查找并删除12Account account = mongoTemplate.findById(id, Account.class); mongoTemplate.remove(account); 计数1long count = mongoTemplate.count(Query.query(Criteria.where(&quot;accountName&quot;).is(accountName)), Account.class); 模糊匹配并排序123Criteria criteria = new Criteria();criteria.andOperator(Criteria.where(&quot;province&quot;).regex(province));//正则mongoTemplate.find(Query.query(criteria).with(new Sort(Direction.ASC, &quot;order&quot;)), Bbatch.class); distinct查询1234 DBObject dbObject = new BasicDBObject(&quot;schoolId&quot;, bschool.get_id());List&lt;String&gt; provinces = mongoTemplate.getCollection(mongoTemplate.getCollectionName(BschoolScoreline.class)) .distinct(&quot;province&quot;, dbObject);bschool.setProvinces(provinces); 查找并所有排序1mongoTemplate.find(new Query().with(new Sort(Direction.ASC, &quot;order&quot;,&quot;createTime&quot;)), Bschool.class); in查询1List&lt;MpropValue&gt; mpropValueList = mongoTemplate.find(Query.query(Criteria.where(&quot;propId&quot;).in(mprop1.get_id())), MpropValue.class); 设置唯一字段12345678//used 是1 将其他所有是1的更新为0 if(address.getUsed()==1)&#123; Query q = new Query(Criteria.where(&quot;userId&quot;).is(user.get_id()));//查询 Update u = new Update().set(&quot;used&quot;,0); BulkOperations ops = mongoTemplate.bulkOps(BulkOperations.BulkMode.UNORDERED, &quot;address&quot;); ops.updateMulti(q,u);//全部更新 ops.execute();&#125; 插入多条数据123456public ReturnStatus upsert(List&lt;BspecialLink&gt; specialLinks) &#123; ReturnStatus status; mongoTemplate.insertAll(specialLinks); status = new ReturnStatus(true); return status;&#125; 获取数据库所有集合名称1Set&lt;String&gt; sets = mongoTemplage.getDb().getCollectionNames(); 查询当天数据private static Date getStartTime() { Calendar todayStart = Calendar.getInstance(); todayStart.set(Calendar.HOUR_OF_DAY, 0); todayStart.set(Calendar.MINUTE, 0); todayStart.set(Calendar.SECOND, 0); todayStart.set(Calendar.MILLISECOND, 0); return todayStart.getTime(); } private static Date getEndTime() { Calendar todayEnd = Calendar.getInstance(); todayEnd.set(Calendar.HOUR_OF_DAY, 23); todayEnd.set(Calendar.MINUTE, 59); todayEnd.set(Calendar.SECOND, 59); todayEnd.set(Calendar.MILLISECOND, 999); return todayEnd.getTime(); } Criteria criteria = new Criteria(); criteria.andOperator( Criteria.where(&quot;type&quot;).is(type), Criteria.where(&quot;typeId&quot;).is(typeId), Criteria.where(&quot;userId&quot;).is(user.get_id()), Criteria.where(&quot;createTime&quot;).gte(getStartTime()) .lte(getEndTime())); return mongoTemplate.exists(Query.query(criteria), Zan.class); 其他 删除字段 1db.yourcollection.update(&#123;&#125;,&#123;$unset:&#123;&quot;需要删除的字段&quot;:&quot;&quot;&#125;&#125;,false,true) 将匹配到的值全部更新 1db.getCollection(&quot;bschool&quot;).update(&#123;&quot;state&quot;:0&#125;,&#123;$set:&#123;&quot;state&quot;:&quot;1&quot;&#125;&#125;,&#123;&apos;multi&apos;:true&#125;) db.getCollection(“pdfChapterItem”).update({}, {$rename : {“remark” : “abbr”}}, false, true)//更新所有字段名 建立索引 1db.bschoolScorelineDetail.ensureIndex(&#123;&quot;schoolId&quot;:1,&quot;type&quot;:1,&quot;batch&quot;:1,&quot;province&quot;:1&#125;,&#123;&quot;name&quot;：&quot;schoolScorelineDetail&quot;&#125;) mongo中建立索引用ensureIndex({“字段1”:1,”字段2”:”-1”},{“name”:”索引名”}) 1和-1代表正序和倒序 mongo数据库备份操作(在mongo的安装目录的bin下执行如下命令) 123456789101112131415不带验证的数据库备份：mongodump -d mrmf -o /tools/test mrmf数据库名 /tools/test要备份到的地方不带验证的数据库恢复mongorestore --db mrmf /tools/test/mrmf mrmf数据库名 /tools/test要恢复的数据带验证的数据库备份：mongodump -d mrmf -u admin -p &quot;ie8*kskIkd123&quot; --authenticationDatabase=admin -o /tools/test 带验证的数据库恢复：mongorestore --db mrmf -u admin -p &quot;ie8*kskIkd123&quot; --authenticationDatabase=admin /tools/test/red 开启慢日志 12345678910111213141516171819202122232425262728293031323334353637在客户端调用db.setProfilingLevel(级别) 命令来实时配置。可以通过db.getProfilingLevel()命令来获取当前的Profile级别。&gt; db.setProfilingLevel(2); &gt; &#123;&quot;was&quot; : 0 , &quot;ok&quot; : 1&#125; &gt; db.getProfilingLevel()上面斜体的级别可以取0，1，2 三个值，他们表示的意义如下：0 – 不开启1 – 记录慢命令 (默认为&gt;100ms)2 – 记录所有命令Profile 记录在级别1时会记录慢命令，那么这个慢的定义是什么?上面我们说到其默认为100ms，当然有默认就有设置，其设置方法和级别一样有两种，一种是通过添加–slowms启动参数配置。第二种是调用db.setProfilingLevel时加上第二个参数：db.setProfilingLevel( level , slowms ) db.setProfilingLevel( 1 , 10 );Mongo Profile 记录是直接存在系统db里的，记录位置 system.profile ，所以，我们只要查询这个Collection的记录就可以获取到我们的 Profile 记录了。Profile 信息内容详解：ts-该命令在何时执行.millis Time-该命令执行耗时，以毫秒记.info-本命令的详细信息.query-表明这是一个query查询操作.ntoreturn-本次查询客户端要求返回的记录数.比如, findOne()命令执行时 ntoreturn 为 1.有limit(n) 条件时ntoreturn为n.query-具体的查询条件(如x&gt;3).nscanned-本次查询扫描的记录数.reslen-返回结果集的大小.nreturned-本次查询实际返回的结果集.update-表明这是一个update更新操作.upsert-表明update的upsert参数为true.此参数的功能是如果update的记录不存在，则用update的条件insert一条记录. moved-表明本次update是否移动了硬盘上的数据，如果新记录比原记录短，通常不会移动当前记录，如果新记录比原记录长，那么可能会移动记录到其它位置，这时候会导致相关索引的更新.磁盘操作更多，加上索引更新，会使得这样的操作比较慢. insert-这是一个insert插入操作. getmore-这是一个getmore 操作，getmore通常发生在结果集比较大的查询时，第一个query返回了部分结果，后续的结果是通过getmore来获取的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#下面是一个超过200ms的查询语句&#123; &quot;op&quot; : &quot;query&quot;, #操作类型，有insert、query、update、remove、getmore、command &quot;ns&quot; : &quot;F10data3.f10_2_8_3_jgcc&quot;, &quot;query&quot; : &#123; #具体的查询语句 包括过滤条件，limit行数 排序字段 filter&quot; : &#123; &quot;jzrq&quot; : &#123; &quot;$gte&quot; : ISODate(&quot;2017-03-31T16:00:00.000+0000&quot;), &quot;$lte&quot; : ISODate(&quot;2017-06-30T15:59:59.000+0000&quot;) &#125;, &quot;jglxfldm&quot; : 10.0 &#125;, &quot;ntoreturn&quot; : 200.0, &quot;sort&quot; : &#123; #如果有排序 则显示排序的字段 这里是 RsId &quot;RsId&quot; : 1.0 &#125;&#125;, &quot;keysExamined&quot; : 0.0, #索引扫描数量 这里是全表扫描，没有用索引 所以是 0&quot;docsExamined&quot; : 69608.0, #浏览的文档数 这里是全表扫描 所以是整个collection中的全部文档数&quot;numYield&quot; : 546.0, #该操作为了使其他操作完成而放弃的次数。通常来说，当他们需要访问 还没有完全读入内存中的数据时，操作将放弃。这使得在MongoDB为了 放弃操作进行数据读取的同时，还有数据在内存中的其他操作可以完成。&quot;locks&quot; : &#123; #锁信息，R：全局读锁；W：全局写锁；r：特定数据库的读锁；w：特定数据库的写锁 &quot;Global&quot; : &#123; &quot;acquireCount&quot; : &#123; &quot;r&quot; : NumberLong(1094) #该操作获取一个全局级锁花费的时间。 &#125; &#125;, &quot;Database&quot; : &#123; &quot;acquireCount&quot; : &#123; &quot;r&quot; : NumberLong(547) &#125; &#125;, &quot;Collection&quot; : &#123; &quot;acquireCount&quot; : &#123; &quot;r&quot; : NumberLong(547) &#125; &#125;&#125;, &quot;nreturned&quot; : 200.0, #返回的文档数量&quot;responseLength&quot; : 57695.0, #返回字节长度，如果这个数字很大，考虑值返回所需字段&quot;millis&quot; : 264.0, #消耗的时间（毫秒）&quot;planSummary&quot; : &quot;COLLSCAN, COLLSCAN&quot;, #执行概览 从这里看来 是全表扫描 &quot;execStats&quot; : &#123; #详细的执行计划 这里先略过 后续可以用 explain来具体分析&#125;, &quot;ts&quot; : ISODate(&quot;2017-08-24T02:32:49.768+0000&quot;), #命令执行的时间&quot;client&quot; : &quot;10.3.131.96&quot;, #访问的ip或者主机&quot;allUsers&quot; : [], &quot;user&quot; : &quot;&quot;&#125; MongoDB 查询优化 如果发现 millis 值比较大，那么就需要作优化。 如果docsExamined数很大，或者接近记录总数（文档数），那么可能没有用到索引查询，而是全表扫描。 如果keysExamined数为0，也可能是没用索引。 结合 planSummary 中的显示，上例中是 “COLLSCAN, COLLSCAN” 确认是全表扫描 如果 keysExamined 值高于 nreturned 的值，说明数据库为了找到目标文档扫描了很多文档。这时可以考虑创建索引来提高效率。 索引的键值选择可以根据 query 中的输出参考，上例中 filter:包含了 jzrq和jglxfldm 并且按照RsId排序，所以 我们的索引索引可以这么建: db.f10_2_8_3_jgcc.ensureindex({jzrq:1,jglxfldm:1,RsId:1}) 聚合操作12345678910111213141516171819示例一Criteria criteria = new Criteria();criteria.andOperator(Criteria.where(&quot;userId&quot;).is(uid), Criteria.where(&quot;type&quot;).is(2));//查询条件Aggregation aggregation = Aggregation.newAggregation(Aggregation.match(criteria),Aggregation.group(&quot;content&quot;).count().as(&quot;count&quot;),Aggregation.sort(Sort.Direction.ASC,&quot;count&quot;));AggregationResults&lt;SearchKeyWords&gt; ar = mongoTemplate.aggregate(aggregation,Searched.class, SearchKeyWords.class);//Searched为要查询的类，SearchKeyWords为查询结果的vo对象List&lt;SearchKeyWords&gt; list = ar.getMappedResults();public class SearchKeyWords &#123; public String _id;//_id: &quot;中华&quot; 重复的字段 public int count;//count: 57 重复次数 public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113示例二//分组group、排序sort统计每个分类下商品个数Criteria criteria = new Criteria();Aggregation aggregation = Aggregation.newAggregation(Aggregation.match(criteria), Aggregation.group(&quot;categoryId&quot;).count().as(&quot;count&quot;),Aggregation.sort(Sort.Direction.ASC,&quot;count&quot;));AggregationResults&lt;BasicDBObject&gt; ar = mongoTemplate.aggregate(aggregation, Goods.class, BasicDBObject.class);for (BasicDBObject basicDBObject : ar) &#123; logger.info(JsonUtils.toJson(basicDBObject));&#125;//输出结果2019-05-06 11:13:11 [com.badou.service.basic.goods.GoodsService]-[INFO] &#123;&quot;_id&quot;:&quot;6430379335205566832&quot;,&quot;count&quot;:1&#125;//分类id和当前分类下商品个数2019-05-06 11:13:11 [com.badou.service.basic.goods.GoodsService]-[INFO] &#123;&quot;_id&quot;:&quot;8732390723817262538&quot;,&quot;count&quot;:2&#125;2019-05-06 11:13:11 [com.badou.service.basic.goods.GoodsService]-[INFO] &#123;&quot;_id&quot;:&quot;4479460853249390323&quot;,&quot;count&quot;:2&#125;2019-05-06 11:13:11 [com.badou.service.basic.goods.GoodsService]-[INFO] &#123;&quot;_id&quot;:&quot;6571438787886532072&quot;,&quot;count&quot;:16&#125;示例三//两个集合关联查询 Aggregation.lookup//若 A集合关联B集合//参数： from B集合名//localField A集合存外键的字段//foreignField B集合与A集合关联的字段//as 重新组合成C集合名称语法db.collection.aggregate([&#123; $lookup: &#123; from: &lt;collection to join&gt;, localField: &lt;field from the input documents&gt;, foreignField: &lt;field from the documents of the &quot;from&quot; collection&gt;, as: &lt;output array field&gt; &#125;&#125;])Criteria criteria = new Criteria();criteria.andOperator(Criteria.where(&quot;_id&quot;).is(&quot;6118446118773213048&quot;));Aggregation aggregation = Aggregation.newAggregation( Aggregation.match(criteria), Aggregation.lookup(&quot;goodsCategory&quot;, &quot;categoryId&quot;, &quot;_id&quot;, &quot;goodsCategory&quot;), Aggregation.unwind(&quot;goodsCategory&quot;), Aggregation.project(&quot;title&quot;,&quot;price&quot;).and(&quot;goodsCategory.name&quot;).as(&quot;categoryName&quot;).and(&quot;goodsCategory.order&quot;).as(&quot;categoryOrder&quot;) );AggregationResults&lt;BasicDBObject&gt; ar = mongoTemplate.aggregate( aggregation, Goods.class, BasicDBObject.class);for (BasicDBObject basicDBObject : ar) &#123; logger.info(JsonUtils.toJson(basicDBObject));&#125;//输出结果2019-05-06 21:17:26 [com.badou.service.basic.goods.GoodsService]-[INFO] &#123;&quot;_id&quot;:&quot;6118446118773213048&quot;,&quot;title&quot;:&quot;小学生必备国学常识&amp;小学生必读国学经典&quot;,&quot;price&quot;:&quot;10.0&quot;,&quot;categoryName&quot;:&quot;小学生&quot;,&quot;categoryOrder&quot;:&quot;1&quot;&#125;示例四(未曾验证)//全文搜索查询//设置索引：db.logInfo.ensureIndex(msg:&quot;text&quot;)Query query = TextQuery.query(new TextCriteria().matching(&quot;coffee&quot;).matching(&quot;-cake&quot;));//-cake表示不匹配不包含cake与notMatching类似//Query query = TextQuery.searching((new TextCriteria().matching(&quot;coffee&quot;).notMatching(&quot;-cake&quot;))List&lt;LogInfoCol&gt; lfs = mongoTemplate.find(query,LogInfoCol.class);if(lfs!=null)&#123; for(LogInfoCol logInfoCol:lfs)&#123; logger.info(JSON.toString(logInfoCol)); &#125;&#125;示例五(未曾验证)//得到范围内数据：以Circle构造函数，第一第二个参数为坐标，第三个参数为距离半径查找符合条件的Circle circle = new Circle(4.2341111,63.00001,0.01);List&lt;PositionCol&gt; positionCols = mongoTemplate.find(new Query(Criteria.where(&quot;location&quot;).within(circle)),PositionCol.class);if(positionCols!=null)&#123; for(PositionCol positionCol:positionCols)&#123; logger.info(JSON.toJSONString(positionCol)) &#125;&#125;示例六(未曾验证)//查找坐标周围10KM内的所有商店Point location = new Point(116.425253,39.925338);NearQuery query = NearQuery.near(location).maxDistance(new Distance(10,Metrice.KILOMETERS));GeoResults&lt;PositionCol&gt; positions = mongoTemplate.geoNear(query,PositionCol.class);if(positionCols!=null)&#123; for(GeoResults&lt;PositionCol&gt;geoResult :positions)&#123; logger.info(geoResult.getContent().getBuissName()+&quot;-&quot;+geoResult.getDistance().getValue()); &#125;&#125;示例七mongo支持的查询参数实例fpi.getParams().put(&quot;state|integer&quot;, &quot;1&quot;); 相当于is 一对一fpi.getParams().put(&quot;ne:state|integer&quot;, &quot;1&quot;); 相当于not is 一对一fpi.getParams().put(&quot;all:keywords&quot;, keyword); keywords为list 多对一fpi.getParams().put(&quot;in:_id|array&quot;, &quot;0,1&quot;); 包含 一对多fpi.getParams().put(&quot;nin:_id|array&quot;, &quot;0,1&quot;); 排除id 一对多fpi.getParams().put(&quot;gte:serialNum&quot;, start); fpi.getParams().put(&quot;lte:serialNum&quot;, end); //日期查询Date currentDate = DateUtil.currentDate();String d = DateUtil.format(currentDate,&quot;yyyy-MM-dd HH:mm:ss&quot;);fpi.getParams().put(&quot;gte:endDate|datetime&quot;, d); 大于当前日期的翻页查询聚合查询1、Criteria idcardCriteria = new Criteria(); idcardCriteria.orOperator(Criteria.where(&quot;idcard&quot;).regex(studentIdcard.toLowerCase()), Criteria.where(&quot;idcard&quot;).regex(studentIdcard.toUpperCase())); criteria.andOperator(idcardCriteria);Mongodb: Sort operation used more than the maximum 33554432 bytes of RAMmongo中所有排序字段加上索引比较好Criteria criteria = new Criteria();criteria.andOperator(Criteria.where(&quot;shakyId&quot;).in(gshakyIds), Criteria.where(&quot;userId&quot;).in(userIds));//筛选条件（限定数据范围）Aggregation aggregation = Aggregation.newAggregation(Aggregation.match(criteria), Aggregation.group(&quot;userId&quot;, &quot;shakyId&quot;).count().as(&quot;count&quot;),Aggregation.sort(Sort.Direction.ASC, &quot;count&quot;));//分组 单个字段分组，直接取count;两个字段key 确定唯一一条记录,后续需要手动统计 AggregationResults ar = mongoTemplate.aggregate(aggregation,Ugclock.class, BasicDBObject.class); 123456789101112131415161718192021222324252627for (BasicDBObject basicDBObject : ar) &#123; String id = basicDBObject.getString(&quot;shakyId&quot;); int count = 1; if (shakyCount.containsKey(id)) &#123; count = shakyCount.get(id); count++; shakyCount.put(id, count); &#125; else &#123; shakyCount.put(id, count); &#125;&#125;2、以id为group by计算num的总值sum（）Aggregation agg = Aggregation.newAggregation( Aggregation.group(new String[] &#123;&quot;_id&quot;&#125;).sum(&quot;num&quot;).as(&quot;num&quot;) );3、以id为group by找出num的最大值max() Aggregation agg = Aggregation.newAggregation( Aggregation.group(new String[] &#123;&quot;_id&quot;&#125;).max(&quot;num&quot;).as(&quot;num&quot;) ); Aggregation agg = Aggregation.newAggregation( Aggregation.group(new String[] &#123;&quot;_id&quot;&#125;).min(&quot;num&quot;).as(&quot;num&quot;) );4、以id为group by计算num的平均值avg(）Aggregation agg = Aggregation.newAggregation( Aggregation.group(new String[] &#123;&quot;_id&quot;&#125;).avg(&quot;num&quot;).as(&quot;num&quot;) ); 翻页项目当中模拟插入了120W条数据，在同一个文档当中单纯查询数据的速度还不错，主要是对查询的文档字段添加了索引，但是对查询结果的前台分页确有问题。具体来说是不设置任何查询条件的时候，会查询出来将近120W条满足条件的结果，使用mongodb的limit()和skip() 来取出来 第一页前20条数据，这样在后台的java程序当中只是这20条数据占用内存。代码具体形式类似于用mongodb客户端执行db.feedbackInfo.find(criteria).skip(0).limit(20) 获得第一页0-20条数据db.feedbackInfo.find(criteria).skip(20).limit(20) 获得第二页20-40条数据……db.feedbackInfo.find(criteria).skip(N).limit(20) 获得第二页N-N+20条数据但问题在于随着不断翻页，skip的值N会越来越大，前台的反应越来越慢。很直接的一个表现就是在前台从第一页直接跳转进入最后一页根本反应不过来。对于这个实际问题，原因就是本书这里所言的skip略过大量结果会带来性能问题，再根源地说是mongodb还不够完善，索引本身还比较简单。具体的这个分页效率的问题，有两种思路：第一，等mongodb升级，优化这个skip的执行效率。第二，不用skip()而实现分页效果。这个思路的基础就是mongodb本身对于where查询和limit()的效率还比较不错，也就是本来分页的那个查询用where和limit速度还可以的前提（一般就是需要建立必要的索引）。假如这个前提不成立，那没法讨论。本书接下来具体讨论了不使用skip对结果分页的实现例子，这个本质是对信息系统增加一个查询中间量——上次查询的业务数值，在逻辑上承担起跟skip相对等价的功能。比如说是第一页查询是按照一个日期date值查询，第一次用db.foo.find().sort({“date”,-1}).limit(20)而点击下一页的时候，事先将上次查询的date的边界值给传递过去，第二页查询的时候就使用新的find条件查询db.foo.find({“date”:{“$gt”:latest.date}}); 而后再对查询结果排序即可这种绕过skip的方式评价：第一，很难比较方便地解决所有的分页问题，简单来说 对于使用正则表达式的查询，根本无法通过记录边界条件来实现。第二，不得不多传递上次查询的那个边界条件，增加了工作量，不够优雅。第三，只能够解决一页一页往下翻页的问题，如果我要从第1页直接跳到100页，就束手无策 翻页的实现一skip实现跳页（比较简单，数据量大了不行） /** *db.getCollection(&apos;user&apos;).find({})是指查询全部。 *sort()设置排序，本示例是指以_id作为条件，正序排序。若将数字1改为-1，则为倒序。 *skip()设置跳页，本示例是指跳过前10条，从第11条开始显示。 *limit()设置每页的显示数量，本例是指每页限制显示10条。 */ db.getCollection(&apos;user&apos;).find({}).sort({&quot;ID&quot;:1}).skip(10).limit(10) 非skip实现跳页原理：以自增_id作为主条件，获取前一页的最后一条记录，查询之后的指定条记录 //根据_id，查询前10条 var a = db. getCollection(&apos;user&apos;).find({}).sort(&quot;_id&quot;,1).limit(10) //定义变量last var last = null //循环遍历 while(a.hasNext()){ last=a.next;//循环到最后，last接收的是最后一条的信息 } //核心是&quot;_id&quot;:{&quot;$gt&quot;:last._id}，即查询大于最后一条的_id的后10条信息 db.getCollection(&apos;slt&apos;).find({&quot;_id&quot;:{&quot;$gt&quot;:last._id}}).sort({_id:1}).limit(10) 翻页的实现二（非skip实现跳页，以自增_id作为主条件）private List&lt;JSONObject&gt; findOrderList(Map&lt;String,Object&gt; paramOptions,int page,int size,String sidx,String sord) throws Exception { //连接数据库 MongoCollection&lt;Document&gt; mongoCollection = getMdbCollection(); //进行第一次查询，条件中没有skip MongoCursor&lt;Document&gt; iterable = mongoCollection.find().limit(size).sort(new BasicDBObject(&quot;_id&quot;, 1)).iterator(); //定义orderItems，用以接收查询的信息 List&lt;JSONObject&gt; orderItems = new ArrayList&lt;JSONObject&gt;(); //如果只有一页或第一页，则走此条件，否，则走else if(page == 1){ //遍历 while (iterable.hasNext()) { Document next = iterable.next(); JSONObject a =JSONObject.parseObject(next.toJson()); orderItems.add(a); } }else{ MongoCursor&lt;Document&gt; iterable2 = mongoCollection.find().limit(size*(page-1)).sort(new BasicDBObject(&quot;_id&quot;, 1)).iterator(); //定义变量last，用以存储每页的最后一条记录 Document last = null; while (iterable2.hasNext()) { last = iterable2.next(); } //定义condition，用以添加【$gt:每页最后一条记录的_id值】，作为查询条件 Map&lt;String, Object&gt; condition = new HashMap&lt;&gt;(); if (null != last.get(&quot;_id&quot;)) { condition.put(&quot;$gt&quot;,last.get(&quot;_id&quot;)); } MongoCursor&lt;Document&gt; iterable3 = mongoCollection.find(condition).limit(size).sort(new BasicDBObject(&quot;_id&quot;, 1)).iterator(); //遍历 while (iterable.hasNext()) { Document next = iterable.next(); JSONObject a =JSONObject.parseObject(next.toJson()); orderItems.add(a); } return orderItems; } 注：上面是按_id正序排列的，如果想要按照倒序排列，则需要将condition.put(&quot;$gt&quot;,last.get(&quot;_id&quot;))改为condition.put(&quot;$lt&quot;,last.get(&quot;_id&quot;))，将sort(new BasicDBObject(&quot;_id&quot;, 1)改为sort(new BasicDBObject(&quot;_id&quot;, -1)。 mongo高级本地搭建副本集mongo的版本 version 3.6.5 副本集文件夹准备mongodb3.6 同目录下建立文件夹，存放数据 data/mongo/27017,data/mongo/27018,data/mongo/27019 同目录下建立文件夹存放日志数据 data/mongo/27017log,data/mongo/27018log,data/mongo/27019log 启动mongod --port 27017 --dbpath /data/mongo/27017 --logpath /data/mongo/27017log/27017.log --replSet rs-local-test --logappend mongod --port 27018 --dbpath /data/mongo/27018 --logpath /data/mongo/27018log/27018.log --replSet rs-local-test --logappend mongod --port 27019 --dbpath /data/mongo/27019 --logpath /data/mongo/27019log/27019.log --replSet rs-local-test --logappend 三个服务器启动完毕之后，不要关闭。另开一个cmd窗口，连接到27017端口的服务器（连接其他端口也可以），每次启动，主服务器可能会不一样，如果连接的是主服务器，前缀会变成如下PRIMARY，如果是从服务器，前缀会变成SECONDARY 创建配置文件创建一个配置文件，在配置文件中列出每一个成员，知道彼此的存在(第二次启动就不需要再配置) use test witched to db test rs.initiate({ &quot;_id&quot;:&quot;rs-local-test01&quot;, &quot;members&quot;:[ {&quot;_id&quot;:0,&quot;host&quot;:&quot;127.0.0.1:27017&quot;}, {&quot;_id&quot;:1,&quot;host&quot;:&quot;127.0.0.1:27018&quot;}, {&quot;_id&quot;:2,&quot;host&quot;:&quot;127.0.0.1:27019&quot;} ] }) { &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1596076771, 1), &quot;$clusterTime&quot; : { &quot;clusterTime&quot; : Timestamp(1596076771, 1), &quot;signature&quot; : { &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot; &quot;keyId&quot; : NumberLong(0) } } } 查看状态信息 rs-local-test:OTHER&gt; rs.status() { &quot;set&quot; : &quot;rs-local-test&quot;, &quot;date&quot; : ISODate(&quot;2020-07-30T02:40:29.243Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;optimes&quot; : { &quot;lastCommittedOpTime&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) }, &quot;readConcernMajorityOpTime&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) }, &quot;appliedOpTime&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) }, &quot;durableOpTime&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) } }, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;name&quot; : &quot;127.0.0.1:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 286, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDate&quot; : ISODate(&quot;2020-07-30T02:40:23Z&quot;), &quot;infoMessage&quot; : &quot;could not find member to sync fr &quot;electionTime&quot; : Timestamp(1596076782, 1), &quot;electionDate&quot; : ISODate(&quot;2020-07-30T02:39:42Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true }, { &quot;_id&quot; : 1, &quot;name&quot; : &quot;127.0.0.1:27018&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 57, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDurable&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDate&quot; : ISODate(&quot;2020-07-30T02:40:23Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2020-07-30T02:40:2 &quot;lastHeartbeat&quot; : ISODate(&quot;2020-07-30T02:40:28.47 &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2020-07-30T02:40:2 ), &quot;pingMs&quot; : NumberLong(0), &quot;syncingTo&quot; : &quot;127.0.0.1:27017&quot;, &quot;configVersion&quot; : 1 }, { &quot;_id&quot; : 2, &quot;name&quot; : &quot;127.0.0.1:27019&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 57, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDurable&quot; : { &quot;ts&quot; : Timestamp(1596076823, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDate&quot; : ISODate(&quot;2020-07-30T02:40:23Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2020-07-30T02:40:2 &quot;lastHeartbeat&quot; : ISODate(&quot;2020-07-30T02:40:28.47 &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2020-07-30T02:40:2 ), &quot;pingMs&quot; : NumberLong(0), &quot;syncingTo&quot; : &quot;127.0.0.1:27018&quot;, &quot;configVersion&quot; : 1 } ], &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1596076823, 1), &quot;$clusterTime&quot; : { &quot;clusterTime&quot; : Timestamp(1596076823, 1), &quot;signature&quot; : { &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot; &quot;keyId&quot; : NumberLong(0) } } } 如果是第二次启动，可以直接查看状态信息，不需要在设置配置文件。查看状态信息，可以看到主服务器和备份服务器： rs-local-test:SECONDARY&gt;use test switched to db test rs-local-test:SECONDARY&gt; db.isMaster() { &quot;hosts&quot; : [ &quot;127.0.0.1:27017&quot;, &quot;127.0.0.1:27018&quot;, &quot;127.0.0.1:27019&quot; ], &quot;setName&quot; : &quot;rs-local-test&quot;, &quot;setVersion&quot; : 1, &quot;ismaster&quot; : false, &quot;secondary&quot; : true, &quot;primary&quot; : &quot;127.0.0.1:27019&quot;, &quot;me&quot; : &quot;127.0.0.1:27017&quot;, &quot;lastWrite&quot; : { &quot;opTime&quot; : { &quot;ts&quot; : Timestamp(1596079583, 1), &quot;t&quot; : NumberLong(5) }, &quot;lastWriteDate&quot; : ISODate(&quot;2020-07-30T03:26:23Z&quot;), &quot;majorityOpTime&quot; : { &quot;ts&quot; : Timestamp(1596079583, 1), &quot;t&quot; : NumberLong(5) }, &quot;majorityWriteDate&quot; : ISODate(&quot;2020-07-30T03:26:23Z&quot;) }, &quot;maxBsonObjectSize&quot; : 16777216, &quot;maxMessageSizeBytes&quot; : 48000000, &quot;maxWriteBatchSize&quot; : 100000, &quot;localTime&quot; : ISODate(&quot;2020-07-30T03:26:25.289Z&quot;), &quot;logicalSessionTimeoutMinutes&quot; : 30, &quot;minWireVersion&quot; : 0, &quot;maxWireVersion&quot; : 6, &quot;readOnly&quot; : false, &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1596079583, 1), &quot;$clusterTime&quot; : { &quot;clusterTime&quot; : Timestamp(1596079583, 1), &quot;signature&quot; : { &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) } } } 读写测试连接主服务器后，可以写入数据：重新启动一个cmd,连接一个备份服务器，查看是否数据被复制：备份节点可能会落后于主节点，可能没有最新写入数据，所以备份节点默认情况下会拒绝读取请求，这是为了保护应用程序，以免意外连接到备份节点，读取到过期数据： rs-local-test:SECONDARY&gt; db.foo.find() Error: error: { &quot;operationTime&quot; : Timestamp(1596079503, 1), &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master and slaveOk=false&quot;, &quot;code&quot; : 13435, &quot;codeName&quot; : &quot;NotMasterNoSlaveOk&quot;, &quot;$clusterTime&quot; : { &quot;clusterTime&quot; : Timestamp(1596079503, 1), &quot;signature&quot; : { &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) } } } 从备份节点读取数据，需要设置标识： db.getMongo().setSlaveOk(); 节点变更在主节点插入数据后，如果有备份节点服务器没有启动，当在该备份启动后，也可以查询到写入的数据：不能对备份节点执行写入操作，备份节点只能通过复制功能写入数据，不接受客户端的写入请求：rs-local-test:SECONDARY&gt; 可以随时添加或删除成员,先按之前的方法创建一个备用服务器，再添加进去：rs.add(‘127.0.0.1’:4444)rs是一个全局变量，其中包含与复制相关的辅助函数。删除成员：rs.remove(‘127.0.0.1’:4444) 添加或删除完可通过 db.isMaster()查看修改结果也可rs.config(); rs-local-test:SECONDARY&gt; rs.config() { &quot;_id&quot; : &quot;rs-local-test&quot;, &quot;version&quot; : 1, &quot;protocolVersion&quot; : NumberLong(1), &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;host&quot; : &quot;127.0.0.1:27017&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : { }, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 }, { &quot;_id&quot; : 1, &quot;host&quot; : &quot;127.0.0.1:27018&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : { }, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 }, { &quot;_id&quot; : 2, &quot;host&quot; : &quot;127.0.0.1:27019&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : { }, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 } ], &quot;settings&quot; : { &quot;chainingAllowed&quot; : true, &quot;heartbeatIntervalMillis&quot; : 2000, &quot;heartbeatTimeoutSecs&quot; : 10, &quot;electionTimeoutMillis&quot; : 10000, &quot;catchUpTimeoutMillis&quot; : -1, &quot;catchUpTakeoverDelayMillis&quot; : 30000, &quot;getLastErrorModes&quot; : { }, &quot;getLastErrorDefaults&quot; : { &quot;w&quot; : 1, &quot;wtimeout&quot; : 0 }, &quot;replicaSetId&quot; : ObjectId(&quot;5f2232e3b3ef9f7db4cb682d&quot;) } } 以上参考自 https://www.cnblogs.com/a-horse-mosaic/p/9284297.html 问题汇总如何开启读写分离默认情况下驱动是从Replica Set 集群中的 Primary 上进行读写的，应用可以在读多写少的场景下开启读写分离，提高效率。 读取偏好(Read Preference)参数 primary 主节点，默认模式，读操作只在主节点，如果主节点不可用，报错或者抛异常。 primaryPreferred 首选主节点，如果主节点不可用，如故障转移，读操作在从节点。 secondary 从节点，读操作只在从节点，如果从节点不可用，报错或者抛异常。 secondaryPreferred 首选从节点，大多情况下读操作在从节点，特殊情况（如单主节点架构）读操作在主节点。 nearest 最邻近节点，读操作在最邻近的成员，可能是主节点或者从节点 在Mongodb中最多能创建多少集合？默认情况下，MongoDB 的每个数据库的命名空间保存在一个 16MB 的 .ns 文件中，平均每个命名占用约 628 字节，也即整个数据库的命名空间的上限约为 24000。每一个集合、索引都将占用一个命名空间。所以，如果每个集合有一个索引（比如默认的 _id 索引），那么最多可以创建 12000 个集合。如果索引数更多，则可创建的集合数就更少了。同时，如果集合数太多，一些操作也会变慢。甚至使得MongoDB集群无法服务的情况发生！ MongoDB有传统数据库的事务和事务回滚么？没有，请不要把它当成关系型数据库来使用，对于MongoDB集群来说，默认情况下数据也不是强一致性的，而是最终一致性。如果对数据一致性比较敏感建议更改WriteConcern级别，但后果是降低了性能，请酌情考虑。 MongoDB有命名规范么？ 不能是空字符串 不能含有.、’’、*、/、\、&lt;、&gt;、:、?、$、\0。建议只使用ASCII码中字母和数字 数据库名区分大小写 数据库名长度最多为64字节 集合名不能包含\0字符，这个字符表示集合名的结束 集合名不能是空字符串”” 集合名不能使用系统集合的保留前缀”system.” 集名名中不建议包含字符’$’，虽然很多驱动程序可以支持包含此字符的集合名 MongoDB有系统保留库名么？ admin local config MongoDB有连接池么？MongoDB驱动中其实已经是一个现成的连接池了，而且线程安全。这个内置的连接池默认初始了100个连接，每一个操作（增删改查等）都会获取一个连接，执行操作后释放连接。【题外话】请务必记得关闭资源，并且设置合理的池子连接数和超时时间。 内置连接池有多个重要参数，分别是 connectionsPerHost：每个主机答应的连接数（每个主机的连接池大小），当连接池被用光时，会被阻塞住，默认值为100 threadsAllowedToBlockForConnectionMultiplier：线程队列数，它和上面connectionsPerHost值相乘的结果就是线程队列最大值。如果连接线程排满了队列就会抛出“Out of semaphores to get db”错误，默认值为5，则最多有500个线程可以等待获取连接 maxWaitTime: 被阻塞线程从连接池获取连接的最长等待时间(ms)。默认值为120,000 connectTimeout：在建立（打开）套接字连接时的超时时间（ms）。默认值为10,000 socketTimeout：套接字超时时间（ms）。默认值为0，无限制（infinite） autoConnectRetry：这个控制是否在连接时，会自动重试，2.13驱动已经【废弃】，请使用connectTimeout代替它 连接池的MaximumPoolSize要有个合理值，否则这个值数据量的连接都被占用，后面再有新的连接创建时就要等待了，而不能超出池上限新建连接。除此之外还要设置合理的连接等待，连接超时时间，以防止一个连接占用时间过长，影响其它连接请求。 connectTimeout 和 socketTimeout 的区别一次完整的请求包括三个阶段： 建立连接 数据传输 断开连接 如果与服务器(这里指数据库)请求建立连接的时间超过ConnectTimeout，就会抛 ConnectionTimeOutException，即服务器连接超时，没有在规定的时间内建立连接。如果与服务器连接成功，就开始数据传输了。如果服务器处理数据用时过长，超过了SocketTimeOut，就会抛出SocketTimeOutExceptin，即服务器响应超时，服务器没有在规定的时间内返回给客户端数据。 所以这该死的超时该怎么配？这里有一份国外写的关于超时的建议：http://blog.mongolab.com/2013/10/do-you-want-a-timeout/上文给出的通常情况下：connectTimeout=5000，socketTimeout=0 关于WriteConcernMongoDB提供了一个配置参数：write concern 来让用户自己衡量性能和写安全。分布式数据库中这样的参数比较常见，记得Cassandra中也有一个类似参数，不过那个好像是要写入几个节点返回成功。其实道理都一样分布式的集群环境考虑到性能因素不能确保每个成员都写入后在返回成功，所以只能交给用户根据实际场景衡量。 Unacknowledged 这个级别也属于比较低的级别，以前这个级别是驱动配置的默认级别，不过后来调整成Acknowledged级别。在这个级别下，这个驱动会根据当前系统的网络配置进行网络问题的检测，不等待Mongd的返回。代码测试：本地网络问题是否有异常？本地网络无问题是远程server问题是否异常？ Acknowledged 这个级别算是中等级别的配置，这个级别能够拿到mongod的返回信息：dupkey Error，以及一些其他的问题。现在这个级别是驱动的默认级别，估计是10gen公司发现好多人评价Mongodb不靠谱后改的。一般系统这个级别也就够用了。由于默认级别是Acknowledged，内部用getLastError方法检查是否写入成功的时候是也不用设置任何参数，对与Replset来说可以在配置中进行getLastErrorDefaults的配置，如果没有的话默认则是Master收到就ok。 Journaled 等到操作记录到Journal Log中才返回操作结果，也就是下一次JournaledLog提交。这种情况可以容忍服务器突然宕机，断电等意外的恢复。出去上边的配置还要在启动mongod的时候加上journaling 参数确保可以使用。commitlog提交间隔时间是可以配置的，单磁盘设备（physical volume, RAID device, or LVM volume）每100ms提交一次，和数据文件刷出相同频率，日志和数据分开磁盘设备的30ms提交一次。在插入数据是如果使用{j:true}则会缩短到已配置的默认设置1/3的时间。 Replica Acknowledged 在副本集中如果w设置为2的话则至少已经吸入到一个secondary中，我猜测写入secondary这个级别是Acknowledged级别，majority是多个secondary已经写入。如果手贱设置w参数大于replset中需要复制的secondarys的话，操作就一直等待直到达到已写入数据的服务器数量符合要求，也可以设置timeout值来指明最长等待时间。{ getLastError: 1, w: 2, wtimeout:5000 } MongoDB的锁机制MongoDB的锁机制和一般关系数据库如 MySQL（InnoDB）, Oracle 有很大的差异，InnoDB 和 Oracle 能提供行级粒度锁，而 MongoDB v2 只能提供库级粒度锁，这意味着当 MongoDB 一个写锁处于占用状态时，其它的读写操作都得干等。 初看起来库级锁在大并发环境下有严重的问题，但是 MongoDB 依然能够保持大并发量和高性能，这是因为 MongoDB 的锁粒度虽然很粗放，但是在锁处理机制和关系数据库锁有很大差异，主要表现在 MongoDB 没有完整事务支持，操作原子性只到单个 document 级别，所以通常操作粒度比较小； MongoDB 锁实际占用时间是内存数据计算和变更时间，通常很快； MongoDB 锁有一种临时放弃机制，当出现需要等待慢速 IO 读写数据时，可以先临时放弃，等 IO 完成之后再重新获取锁。 通常不出问题不等于没有问题，如果数据操作不当，依然会导致长时间占用写锁，比如下面提到的前台建索引操作，当出现这种情况的时候，整个数据库就处于完全阻塞状态，无法进行任何读写操作，情况十分严重。 解决问题的方法，尽量避免长时间占用写锁操作，如果有一些集合操作实在难以避免，可以考虑把这个集合放到一个单独的 MongoDB 库里，因为 MongoDB 不同库锁是相互隔离的，分离集合可以避免某一个集合操作引发全局阻塞问题。 建索引导致数据库阻塞上面提到了 MongoDB 库级锁的问题，建索引就是一个容易引起长时间写锁的问题，MongoDB 在前台建索引时需要占用一个写锁（而且不会临时放弃），如果集合的数据量很大，建索引通常要花比较长时间，特别容易引起问题。 解决的方法很简单，MongoDB 提供了两种建索引的访问，一种是 background 方式，不需要长时间占用写锁，另一种是非 background 方式，需要长时间占用锁。使用 background 方式就可以解决问题。例如，为超大表 posts 建立索引 //千万不用使用 db.posts.ensureIndex({user_id: 1}) //而应该使用 db.posts.ensureIndex({user_id: 1}, {background: 1}) http://www.111com.net/database/165981.htm]]></content>
      <categories>
        <category>mongo</category>
      </categories>
      <tags>
        <tag>mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ffmpeg的使用]]></title>
    <url>%2F2020%2F04%2F04%2Fffmpeg%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[视频格式小科普在开始下面的教程之前有必要先简单科普一下视频格式的知识。 视频格式是一种非常不专业的叫法，事实上，视频有编码格式和容器格式两种。编码格式之于容器格式就像牛奶之于杯子一样。 常见的视频文件有mp4(mpeg4 part 14)，mkv，flv等，这些是视频的容器格式/封装格式(Container format)。它们包含视频流和音频流，mkv支持多条音轨和字幕，因此是目前最受欢迎的容器格式。比如在播放mkv视频的时候，可以选择不同语言的音轨/字幕。 至于视频编码格式(Encoding format)，则是这些容器所包含的视频流/音频流所采用的压缩方式，比如最常用的h.264/aac，mpeg4/mpeg3等。 安装ffmpeg在mac os上安装ffmpeg： brew install ffmpeg –with-fdk-aac –with-libass –with-sdl2与默认安装相比，这里增加了对fdk-aac音频库和ass字幕库的支持，同时安装了ffplay播放器。 在linux上安装ffmpeg最简单的方式是使用官方提供的静态编译安装包，解压后即可使用。 ffmpeg命令通用格式先来认识一下ffmpeg命令行工具的格式： ffmpeg -global_options -input_1_options -i input_1 -input_2_options -i input_2 -output_1_options output_1 …不管你看到的ffmpeg命令多么复杂，万变不离其宗，都可以按照这个格式拆分成几个单独的部分，各个选项的含义可以查阅ffmpeg的documentation。 需要注意的是： 输入输出 至少有一个输入和输出 可以同时有多个输入和输出 输入、输出不一定是文件，可以是rtmp流的地址、摄像头对应的设备文件等 顺序很重要 所有的输入完了然后是输出 * 某个选项只对它后面的输入或输出起作用 1、查看视频的信息ffmpeg -hide_banner -i input.mkv 或者 ffprobe -hide_banner input.mkv 通过ffmpeg可以查看一个视频文件的以下基本信息： 视频/音频的编码格式分辨率(1080p or 1080i, Progressive Interlaced)时长(duration)/码率(bitrate)/帧率(fps,frames per second)音轨(Audio)/字幕(Subtitle) 2、转码(transcode)转码是指对流(视频、音频和字幕)进行解码然后再编码，这个过程非常耗CPU。输出的编码格式通过-codec指定，如果不指定，ffmpeg并不会直接copy，而是采用根据容器格式采用相应的默认编码格式进行重新编码。 为了简单起见，把转换容器格式也归为转码，ffmpeg会通过输出文件的后缀判断输出的容器格式，因此不需要指定(-f)。 视频将mkv转换成mp4 有时候我们需要将mkv转换成mp4： 很多视频网站不支持上传mkv格式大部分视频剪辑软件，比如premiere等不支持导入mkv格式的视频下面的命令将mkv的视频和音频流重新封装成mp4文件，不进行重新编码(无损) ffmpeg -i input.mkv -codec copy output.mp4制作mkv mkv格式能够封装多个音频、字幕轨，是目前最为流行的视频分发格式，通常使用mkvtoolnix工具来编辑制作mkv视频，通过ffmpeg也能完成一些简单的任务。 将mpeg4/mp3编码的视频重新编码为h.264/aac，并和字幕一起封装成mkv文件： ffmpeg -i input.avi -i input.srt -map 0:0 -map 0:1 -map 1:0 -c:v libx264 -c:a aac -c:s srt output.mkv这里使用ffmpeg自带的aac音频编码器，而不是fdkaac。 音频ffmpeg不仅能处理视频，音频文件也不在话下，因此，同样可以用ffmpeg转换音频格式。 将flac无损的音乐转换成高质量(High Quality，即320Kb)的mp3格式： ffmpeg -i &quot;Michael Jackson - Billie Jean.flac&quot; -ab 320k &quot;Michael Jackson - Billie Jean.mp3&quot; 3、截取(cut)从视频中截取出精彩片段是我们最常用的功能。 从第2分钟开始，截取30秒。所有流都直接拷贝。 ffmpeg -ss 00:02:00.0 -i input.mkv -t 30 -c copy output.mkv-ss设置开始时间点，格式是HH:MM:SS.xxx或sec.msec。当作为输入选项时，可以快速seek到指定时间点； -t作为输出选项，设置时长。 截取最大的问题是难以做到精确控制时间点，大部分的视频剪辑软件都存在这样的问题： 视频开头卡顿/不流畅，画面与声音不同步时间误差甚至能达到秒级，比如截取10秒的片段，可能会得到12秒的输出原因是ffmpeg会seek到指定时间点前的一个关键帧，如果是copy视频流，seek点和起点之间的额外部分将会被保留，因此就产生了误差。解决办法是将-ss作为输出选项或进行重新编码： ffmpeg -i input.mkv -ss 00:02:00.0 -t 30 -c copy output.mkv-ss作为输出选项时，解码到指定的时间点，然后开始输出，相当于要扫描前面的所有帧。但是这种方式能获得更精确的输出。 如果还是出现上述问题，使用下面的终极解决方案(使用两次seek)： ffmpeg -ss 00:01:30.0 -i input.mkv -ss 00:00:30.0 -t 30 output.mkv 4、截图(screenshot)截图就是将某个视频帧保存为图片，例如： 在指定时间点截图 ffmpeg -ss 00:30:14.435 -i input.mkv -vframes 1 out.png -ss作为输入选项，可以快速定位到指定时间点；如果作为输出选项，需要读取指定时间点前面所有的帧，但可以获得更精确的位置。 * -vframes设置输出的帧数。 结合fps视频滤镜，可以从视频中截取多张图片。例如： 每1分钟截一张图，在截图文件名中添加当前的时间 ffmpeg -i input.mkv -vf fps=1/60 -strftime 1 out_%Y%m%d%H%M%S.jpg 5、压缩(compress)一张蓝光原盘接近50G，而720p的电影可能只有2G，这是通过压缩实现的。在保证画质的前提下使用更小的文件存储，一直是压缩的目标。然而压缩并非那么简单，比如同样是720p的电影，有的只有2个G，有的却有5个G，然而从视觉上几乎看不出区别来。 这里只介绍压缩的一个简单用途：resize。 将1080p转成720p，宽度自适应 ffmpeg -i input.mkv -c copy -c:v libx264 -vf scale=-2:720 output.mkv-vf是-filter:v的简写，-filter指定滤镜，:v是流选择器，表示对视频流应用滤镜。scale滤镜后面的参数是w:h，w和h可以包含一些变量，比如原始的宽高分别为iw和ih。当其中一个是负数时，假设为-n，ffmpeg自动使用另一个值按照原始的宽高比(aspect ratio)计算出该值，并且保证计算出来的值能被n整除。 因为scale滤镜是针对未编码的原始数据，所以视频流不能用copy，需要进行重新编码。-c copy -c:v libx264表示视频流使用h.264重新编码，其他流直接copy，顺序不能颠倒。 6、字幕(subtitle)对于视频发布者，给视频添加字幕很有必要。如果是mkv，可以采用制作mkv的方法将字幕文件封装到mkv文件里。如果是mp4视频，则需要进行烧录(draw)，即对视频重新编码的过程中将字幕融入视频流。 ffmpeg -i input.mkv -vf subtitles=input.srt output.mp4该过程是通过subtitles滤镜完成的，安装ffmpeg的时候需要启用libass库。subtitles的参数是字幕文件，如果是mkv文件则表示使用mkv的默认字幕流。 如果是ass格式的字幕文件，则使用ass滤镜： ffmpeg -i input.mkv -vf ass=input.ass output.mp4据我所知，通常ass字幕具有更丰富的样式。 另一种常见的方法是使用-c:s mov_text将字幕流封装进mp4里，它不需要重新编码，但是目前大部分播放器对mp4的软字幕支持不好。 7、提取(extract)有时候我们需要从电影中提取插曲或从视频中提取背景音乐，通过ffmpeg可以很容易从视频文件中提取音频： ffmpeg -i cut.mp4 -vn output.mp3-vn表示不输出视频流。 输出的mp3默认的码率是128kb，可以使用ab控制。结合截取中的-ss和-t还可以只提取一个部分的音频。 从mkv文件中提取字幕也是同样的方法。 8、水印(watermark)给视频添加水印是通过overlay滤镜实现的，overlay滤镜的作用是将一个视频覆盖另一个视频上面，它有两个输入，前一个是下层(main)，后一个是上层(overlaid)。 overlay的参数x:y是上层左上角的坐标，0:0表示位于下层的左上角。它可以包含以下参数： W(w):下(上)层的宽H(h):下(上)层的高将水印/logo添加到视频的右上角，并且和边缘保持5像素的距离： ffmpeg -i input.mkv -i input.png -filter_complex &quot;overlay=W-w-5:5&quot; -c copy -c:v libx264 output.mkvinput.png是带透明背景的logo，作为上层。 需要注意的是，视频需要重新编码，其他流(音频、字幕)则直接copy，顺序不能颠倒。 9、混流(Muxing)如果我们拍摄了一段视频，想给它添加背景音乐，最好还是借助视频编辑软件，它能对音乐的开头和结尾分别进行增强和减弱的处理，如果不在意这些细节，完全可以用ffmpeg来代替。 替换音频ffmpeg -i input.mkv -i input.mp3 -map 0:v -map 1:a -c copy -shortest output.mp4-map的作用是手动选择输出的流： -map 0:v – 从输入0(第1个输入，即input.mkv) 选择视频流-map 1:a – 从输入1(第2个输入，即input.mp3) 选择音频流第1个map对应输出的第1个流，第2个map对应输出的第2个流，以此类推。 合并音频另一个常见的形式是将两个音频合并成一个，需要使用amerge滤镜： ffmpeg -i input.mkv -i output.aac -filter_complex &quot;[0:a][1:a]amerge=inputs=2[a]&quot; -map 0:v -map &quot;[a]&quot; -c:v copy -c:a aac -ac 2 -shortest output.mp4 10、制作gif结合前面讲到的截取和压缩，将输出文件的后缀改为gif就可以得到动态图 ffmpeg -ss 30 -t 5 -i input.mp4 -r 10 -vf scale=-1:144 -y output.gif-r指定帧率，原始的帧率是25，降低帧率能减小gif图片的大小。 但是这种方式的效果很差，有一种粗糙的布料的感觉。 原因在于gif限制只能包含256种颜色，而原始视频可能包含数以百万的颜色，ffmpeg默认会使用一种通用的调色板(palettep)，因此导致颜色失真。2015年，ffmpeg通过引入palettegen和paletteuse两个滤镜来改进这个问题，它通过扫描整个视频，输出一个最佳的调色板，然后再转换成gif的过程中应用这个调色板从而避免颜色失真。 下面的命令从input.mp4的第30秒开始截取5秒，生成高144像素的gif动态图： palette=&quot;/tmp/palette.png&quot;filters=&quot;fps=10,scale=-1:144:flags=lanczos&quot; ffmpeg -ss 30 -t 5 -i input.mp4 -vf &quot;$filters,palettegen&quot; -y $paletteffmpeg -ss 30 -t 5 -i input.mp4 -i $palette -filter_complex &quot;$filters [x]; [x][1:v] paletteuse&quot; -y output.gif 与前一个输出相比，质量提升了很多。 另一个有待研究的难题是如何控制输出gif的大小。在不影响画质的前提下，我们希望体积越小越好。比如网上有的gif图很清晰，却不超过1M。而一段500K的视频文件，使用上面的方法转成gif后输出的gif却有1.6M。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式环境下redis的使用优化]]></title>
    <url>%2F2020%2F03%2F10%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E4%B8%8Bredis%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[分布式与集群单机处理到达瓶颈的时候，你就把单机复制几份，这样就构成了一个“集群”。集群中每台服务器就叫做这个集群的一个“节点”，所有节点构成了一个集群。每个节点都提供相同的服务，那么这样系统的处理能力就相当于提升了好几倍（有几个节点就相当于提升了这么多倍）。但问题是用户的请求究竟由哪个节点来处理呢？最好能够让此时此刻负载较小的节点来处理，这样使得每个节点的压力都比较平均。要实现这个功能，就需要在所有节点之前增加一个“调度者”的角色，用户的所有请求都先交给它，然后它根据当前所有节点的负载情况，决定将这个请求交给哪个节点处理。这个“调度者”有个牛逼了名字——负载均衡服务器。集群结构的好处就是系统扩展非常容易。如果随着你们系统业务的发展，当前的系统又支撑不住了，那么给这个集群再增加节点就行了。但是，当你的业务发展到一定程度的时候，你会发现一个问题——无论怎么增加节点，貌似整个集群性能的提升效果并不明显了。这时候，你就需要使用微服务结构了。 从单机结构到集群结构，你的代码基本无需要作任何修改，你要做的仅仅是多部署几台服务器，每台服务器上运行相同的代码就行了。但是，当你要从集群结构演进到微服务结构的时候，之前的那套代码就需要发生较大的改动了。所以对于新系统我们建议，系统设计之初就采用微服务架构，这样后期运维的成本更低。但如果一套老系统需要升级成微服务结构的话，那就得对代码大动干戈了。所以，对于老系统而言，究竟是继续保持集群模式，还是升级成微服务架构，这需要你们的架构师深思熟虑、权衡投入产出比。OK，下面开始介绍所谓的分布式结构。分布式结构就是将一个完整的系统，按照业务功能，拆分成一个个独立的子系统，在分布式结构中，每个子系统就被称为“服务”。这些子系统能够独立运行在web容器中，它们之间通过RPC方式通信。 举个例子，假设需要开发一个在线商城。按照微服务的思想，我们需要按照功能模块拆分成多个独立的服务，如：用户服务、产品服务、订单服务、后台管理服务、数据分析服务等等。这一个个服务都是一个个独立的项目，可以独立运行。如果服务之间有依赖关系，那么通过RPC方式调用。这样的好处有很多：系统之间的耦合度大大降低，可以独立开发、独立部署、独立测试，系统与系统之间的边界非常明确，排错也变得相当容易，开发效率大大提升。系统之间的耦合度降低，从而系统更易于扩展。我们可以针对性地扩展某些服务。假设这个商城要搞一次大促，下单量可能会大大提升，因此我们可以针对性地提升订单系统、产品系统的节点数量，而对于后台管理系统、数据分析系统而言，节点数量维持原有水平即可。服务的复用性更高。比如，当我们将用户系统作为单独的服务后，该公司所有的产品都可以使用该系统作为用户系统，无需重复开发。 集群环境下redis锁的进一步优化死锁的问题 解决： 假如程序被kill了，删除锁同样无法执行得到 解决：给锁增加过期时间 过期时间的确定 假如程序1执行到删除锁的时间大于了过期时间，也就是程序1还没执行完锁就被删掉了，此时另外一个线程也可以拿到锁。当线程1执行完去删除锁的时候删掉的就是线程2的锁了，程序就乱了。 解决：保证当前线程只删除当前线程加的锁 redisson.org的使用&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt; &lt;/dependency&gt; @Bean @ConditionalOnProperty(name=&quot;redisson.address&quot;) RedissonClient redissonSingle() { Config config = new Config(); SingleServerConfig serverConfig = config.useSingleServer() .setAddress(redssionProperties.getAddress()) .setTimeout(redssionProperties.getTimeout()) .setConnectionPoolSize(redssionProperties.getConnectionPoolSize()) .setConnectionMinimumIdleSize(redssionProperties.getConnectionMinimumIdleSize()); if(StringUtils.isNotBlank(redssionProperties.getPassword())) { serverConfig.setPassword(redssionProperties.getPassword()); } return Redisson.create(config); }]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多核并发缓存架构]]></title>
    <url>%2F2020%2F03%2F04%2F%E5%A4%9A%E6%A0%B8%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[计算机架构现代计算机模型是基于-冯诺依曼计算机模型，主要包括五大核心部分，1.控制器，2.运算器，3.存储器，4.输入，5.输出。图如下： CPU的基本结构及其工作原理中央处理器（CPU，Central Processing Unit）是一块超大规模的集成电路，是一台计算机的运算核心（Core）和控制核心（ Control Unit）。它的功能主要是解释计算机指令以及处理计算机软件中的数据。中央处理器主要包括运算器（算术逻辑运算单元，ALU，ArithmeTIc Logic Unit）和高速缓冲存储器（Cache）及实现它们之间联系的数据（Data）、控制及状态的总线（Bus）。它与内部存储器（Memory）和输入/输出（I/O）设备合称为电子计算机三大核心部件。 CPU的基本结构从功能上看，一般CPU的内部结构可分为：控制单元、逻辑运算单元、存储单元（包括内部总线和缓冲器）三大部分。其中控制单元完成数据处理整个过程中的调配工作，逻辑单元则完成各个指令以便得到程序最终想要的结果，存储单元就负责存储原始数据以及运算结果。浑然一体的配合使得CPU拥有了强大的功能，可以完成包括浮点、多媒体等指令在内的众多复杂运算，也为数字时代加入了更多的活力。 逻辑部件 英文Logic components；运算逻辑部件。可以执行定点或浮点算术运算操作、移位操作以及逻辑操作，也可执行地址运算和转换。 寄存器 寄存器部件，包括寄存器、专用寄存器和控制寄存器。 通用寄存器又可分定点数和浮点数两类，它们用来保存指令执行过程中临时存放的寄存器操作数和中间（或最终）的操作结果。 通用寄存器是中央处理器的重要部件之一。 控制部件 英文Control unit；控制部件，主要是负责对指令译码，并且发出为完成每条指令所要执行的各个操作的控制信号。 其结构有两种：一种是以微存储为核心的微程序控制方式；一种是以逻辑硬布线结构为主的控制方式。 微存储中保持微码，每一个微码对应于一个最基本的微操作，又称微指令；各条指令是由不同序列的微码组成，这种微码序列构成微程序。中央处理器在对指令译码以后，即发出一定时序的控制信号，按给定序列的顺序以微周期为节拍执行由这些微码确定的若干个微操作，即可完成某条指令的执行。 简单指令是由（3～5）个微操作组成，复杂指令则要由几十个微操作甚至几百个微操作组成。 CPU的逻辑单元CPU大致可分为如下八个逻辑单元： 指令高速缓存，俗称指令寄存器 ： 它是芯片上的指令仓库，有了它CPU就不必停下来查找计算机内存中的指令，从而大幅提高了CPU的运算速度。 译码单元，俗称指令译码器 ： 它负责将复杂的机器语言指令解译成运算逻辑单元（ALU）和寄存器能够理解的简单格式，就像一位外交官。 控制单元 ： 既然指令可以存入CPU，而且有相应指令来完成运算前的准备工作，背后自然有一个扮演推动作用的角色——它便是负责整个处理过程的操作控制器。根据来自译码单元的指令，它会生成控制信号，告诉运算逻辑单元（ALU）和寄存器如何运算、对什么进行运算以及对结果进行怎样的处理。 寄存器 ： 它对于CPU来说非常的重要，除了存放程序的部分指令，它还负责存储指针跳转信息以及循环操作命令，是运算逻辑单元（ALU）为完成控制单元请求的任务所使用的数据的小型存储区域，其数据来源可以是高速缓存、内存、控制单元中的任何一个。 逻辑运算单元（ALU） ： 它是CPU芯片的智能部件，能够执行加、减、乘、除等各种命令。此外，它还知道如何读取逻辑命令，如或、与、非。来自控制单元的讯息将告诉运算逻辑单元应该做些什么，然后运算单元会从寄存器中间断或连续提取数据，完成最终的任务。 预取单元 ： CPU效能发挥对其依赖非常明显，预取命中率的高低直接关系到CPU核心利用率的高低，进而带来指令执行速度上的不同。根据命令或要执行任务所提出的要求，何时时候，预取单元都有可能从指令高速缓存或计算机内存中获取数据和指令。当指令到达时，预取单元最重要的任务就是确保所有指令均排列正确，然后发送给译码单元。 总线单元 ： 它就像一条高速公路，快速完成各个单元间的数据交换，也是数据从内存流进和流出CPU的地方。 数据高速缓存 ： 存储来自译码单元专门标记的数据，以备逻辑运算单元使用，同时还准备了分配到计算机不同部分的最终结果。 通过以上介绍可以看出CPU虽小，方寸之地却能容纳大世界，内部更像一个发达的装配工厂，环环相扣，层层相套。正因为有了相互间的协作配合，才使得指令最终得以执行，才构成了图文并茂、影像结合的神奇数字世界。 CPU的工作原理CPU的根本任务就是执行指令，对计算机来说最终都是一串由“0”和“1”组成的序列。CPU从逻辑上可以划分成3个模块，分别是控制单元、运算单元和存储单元，这三部分由CPU内部总线连接起来。如下所示： 控制单元：控制单元是整个CPU的指挥控制中心，由指令寄存器IR（InstrucTIon Register）、指令译码器ID（InstrucTIon Decoder）和操作控制器OC（OperaTIon Controller）等，对协调整个电脑有序工作极为重要。它根据用户预先编好的程序，依次从存储器中取出各条指令，放在指令寄存器IR中，通过指令译码（分析）确定应该进行什么操作，然后通过操作控制器OC，按确定的时序，向相应的部件发出微操作控制信号。操作控制器OC中主要包括节拍脉冲发生器、控制矩阵、时钟脉冲发生器、复位电路和启停电路等控制逻辑。 运算单元：是运算器的核心。可以执行算术运算（包括加减乘数等基本运算及其附加运算）和逻辑运算（包括移位、逻辑测试或两个值比较）。相对控制单元而言，运算器接受控制单元的命令而进行动作，即运算单元所进行的全部操作都是由控制单元发出的控制信号来指挥的，所以它是执行部件。 存储单元：包括CPU片内缓存和寄存器组，是CPU中暂时存放数据的地方，里面保存着那些等待处理的数据，或已经处理过的数据，CPU访问寄存器所用的时间要比访问内存的时间短。采用寄存器，可以减少CPU访问内存的次数，从而提高了CPU的工作速度。但因为受到芯片面积和集成度所限，寄存器组的容量不可能很大。寄存器组可分为专用寄存器和通用寄存器。专用寄存器的作用是固定的，分别寄存相应的数据。而通用寄存器用途广泛并可由程序员规定其用途，通用寄存器的数目因微处理器而异。这个是我们以后要介绍这个重点，这里先提一下。 我们将上图细化一下，可以得出CPU的工作原理概括如下： 1、取指令：CPU的控制器从内存读取一条指令并放入指令寄存器。 操作码就是汇编语言里的mov，add，jmp等符号码；操作数地址说明该指令需要的操作数所在的地方，是在内存里还是在CPU的内部寄存器里。 2、指令译码：指令寄存器中的指令经过译码，决定该指令应进行何种操作（就是指令里的操作码）、操作数在哪里（操作数的地址）。 3、 执行指令，分两个阶段“取操作数”和“进行运算”。 4、 修改指令计数器，决定下一条指令的地址。 CPU在运算数据的时候（比如 1+1＝2），会首先从CUP寄存器读取数据（速度最快，因为内置在CPU里面），如果没有，就从三级缓存里读取，如果三级缓存也没有，则会经过系统总线及内存总线，从总存储器中读取（此处的总存储器主要是指主内存）。 首先CPU工作的时候，由控制单元充当大脑，负责协调。让运算单元做运算的时候，会首先从最靠近CPU的寄存器（其实是和CPU一体的）上读取数据，在寄存器上有CPU运行的常用指令，如果寄存器上没有想要的数据，则就从三级缓存的L1级缓存中获取，如果L1取到数据了，会加载到寄存器中，再转输给CPU运算单元。如果L1中没有，则从L2级缓存中读取，同理，如果没有，则从L3中取。如果L3中也没有，这个时候，就比较麻烦了。要从主内存中取。而从主内存中取的时候，会经过系统总线及内存总线。这时因受到总线的限制，速度会大大降低。 缓存一致性问题在多处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存（MainMemory）。基于高速缓存的存储交互很好地解决了处理器与内存的速度矛盾，但是也引入了新的问题：缓存一致性（CacheCoherence）。当多个处理器的运算任务都涉及同一块主内存区域时，将可能导致各自的缓存数据不一致的情况，如果真的发生这种情况，那同步回到主内存时以谁的缓存数据为准呢？为了解决一致性的问题，需要各个处理器访问缓存时都 遵循一些协议，在读写时要根据协议来进行操作。常用的方法是总线加锁或是缓存一致性协议-MESI 这里重点说一个什么是缓存一致性协议mesi:概念：CPU最小存储单元：缓存行MESI代表的四种状态：M：修改E：独享。互斥S：共享I：无效MESI缓存一致性协议原理：假如现在有CPU1和CPU2，主内存有变量X＝ 1 。现在要做 x+1的操作。如果在变量 x = 1 上加上volatile，则就会触发MESI当CPU1从主内存中读取到X＝1时，CPU1会把此变量标记成独享状态并监听总线，是否有其它CPU去读取此变量当CPU2从主内存中读取X＝1变量时，CPU1会通过嗅探机制监听到。此时CPU1的X变量会变成共享状态。继续进行计算，计算完变成X＝2。此时要回写到主内存之前。先锁住缓存行。并标记X变量为修改状态。并向总线发消息。其它CPU2监听总线时，会监听到，并把X标记成无效状态。CPU1把变量X＝2回写到主内存后，会由修改状态变成独享状态。此时，如果CPU2如果想修改X变量时，要重启从主内存中读取。然后开始新的轮回。 指令重排序问题为了使得处理器内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行乱序执 行（Out-Of-Order Execution）优化，处理器会在计算之后将乱序执行的结果重组，保证该 结果与顺序执行的结果是一致的，但并不保证程序中各个语句计算的先后顺序与输入代码中的 顺序一致。因此，如果存在一个计算任务依赖另一个计算任务的中间结果，那么其顺序性并不 能靠代码的先后顺序来保证。与处理器的乱序执行优化类似，Java虚拟机的即时编译器中也有 类似的指令重排序（Instruction Reorder）优化 JMM内存模型java线程内存模型跟cpu缓存模型类似，是基于cpu缓存模型来建立的，java线程内存模型是标准化的，屏蔽掉了底层不同计算机的区别。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2F2020%2F02%2F27%2FJVM%2F</url>
    <content type="text"><![CDATA[Java内存区域前提本文讲的基本都是以Sun HotSpot虚拟机为基础的，Oracle收购了Sun后目前得到了两个【Sun的HotSpot和JRockit(以后可能合并这两个),还有一个是IBM的IBMJVM】 Java内存模型Java程序内存的分配是在JVM虚拟机内存分配机制下完成。 Java内存模型（Java Memory Model,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。 简而言之，JMM是jvm的一种规范，定义了JVM的内存模型。她屏蔽了各种硬件和操作系统的访问差异，不是直接访问硬件内存，相对安全很多，它的主要目的是解决由于多线程通过共享内存进行通信时，存在的本地内存数据不一致，编译器会对代码指令重排序，处理器会对代码乱序执行等带来的问题。可以保证并发编程场景中的原子性、可见性和有序性。 Java数据区域分为为五大数据区域。这些区域各有各的用途，创建及销毁时间。 五大内存区域详解程序计数器程序计数器是一块很小的内存空间，它是线程私有的，可以认作为当前线程的行号指示器。 为什么需要程序计数器？ 我们知道对于一个处理器（如果是多核cpu那就是一核），在一个确定的时刻都只会执行一条线程中的指令，一条线程中有多个指令，为了线程切换可以恢复到正确执行位置，每个线程都需要有一个独立的程序计数器，不同线程之间的程序计数器互不影响，独立存储。 如果线程执行的是个java方法，那么计数器记录虚拟机字节码指令的地址。如果为native方法，那么计数器为空。这块内存区域是虚拟机规范中唯一没有OutOfMemoryError的区域。 Java栈（虚拟机栈）同计数器一样也为线程私有，生命周期与之相同，就是我们平时说的栈，栈描述的是Java方法执行的内存模型。 每个方法被执行的时候都会创建一个栈帧用于存储局部变量表，操作栈，动态链接，方法出口等信息。每一个方法被调用的过程就对应一个栈帧在虚拟机栈中从入栈到出栈的过程。栈遵循先进后出的原则。 栈帧：是用来存储数据和部分过程结果的数据结构。 栈帧的位置：内存-&gt;运行时数据区-&gt;某个线程对应的虚拟机栈-&gt;here 栈帧大小确定时间：编译期确定，不受运行期数据影响。 平时说的栈一般指局部变量表部分。 局部变量表：一片连续的内存空间，用来存放方法参数，以及方法内定义的局部变量，存放着编译期间已知的数据类型（八大基本类型和对象引用reference）,returnAddress类型。它的最小的局部变量表空间单位为Slot,虚拟机没有指明Slot的大小，但在jv’m中，long和double类型数据明确规定为64位，这两个类型占2个Slot,其他基本类型固定占用一个Slot. reference类型：与基本类型不同的是它不等同本身，即使是String，内部也是char数组组成，它可能是指向一个对象起始位置指针，也可能指向一个代表对象的句柄或其他与该对象有关的位置。 returnAddress:指一条字节码指令的地址。 需要注意的是，局部变量表所需要的内存空间在编译期完成分配，当进入一个方法时，这个方法在栈中需要分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表大小。 java虚拟机栈可能出现的两种类型的异常： 1、线程请求的栈深度大于虚拟机允许的栈深度，将抛出StackOverflowError 2、虚拟机栈空间可以动态扩展，当动态扩展是无法申请到足够的空间时，抛出OutOfMemory 本地方法栈本地方法栈是与虚拟机栈发挥的作用十分相似，区别是虚拟机栈执行的是java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的native方法服务，可能底层调用的c或c++,我们打开jdk安装目录可以看到也有很多c编写的文件，可能就是native方法所调用的c代码 堆对于大多数应用来说，堆是java虚拟机管理内存最大的一块内存区域，因为堆存放的对象是线程共享的，所以多线程的时候也需要同步机制。 注意:它是所有线程共享的，它的目的是存放对象实例。同时它也是GC所管理的主要区域，因此常被称为GC堆，又由于现在收集器常使用分代算法，Java堆中还可以细分为新生代和老年代，再细致点还有Eden(伊甸园)空间之类的不做深究。 根据虚拟机规范，Java堆可以存在物理上不连续的内存空间，就像磁盘空间只要逻辑是连续的即可。它的内存大小可以设为固定大小，也可以扩展。 当前主流的虚拟机如HotPot都能按扩展实现(通过设置 -Xmx和-Xms)，如果堆中没有内存完成实例分配，而且堆无法扩展将报OOM错误(OutOfMemoryError) web应用程序new出的对象放在Eden区（8/10），当放满了后会自动开启一个线程执行minor gc 方法区方法区同堆一样，是所有线程共享的内存区域，为了区分堆，又被称为非堆。 用于存储已被虚拟机加载的类信息、常量、静态变量，如static修饰的变量加载类的时候就被加载到方法区中。 运行时常量池 是方法区的一部分，class文件除了有类的字段、接口、方法等描述信息之外，还有常量池用于存放编译期间生成的各种字面量和符号引用。 在老版jdk，方法区也被称为永久代【因为没有强制要求方法区必须实现垃圾回收，HotSpot虚拟机以永久代来实现方法区，从而JVM的垃圾收集器可以像管理堆区一样管理这部分区域，从而不需要专门为这部分设计垃圾回收机制。不过自从JDK7之后，Hotspot虚拟机便将运行时常量池从永久代移除了。】 jdk8真正开始废弃永久代，而使用元空间(Metaspace) GCGC简介GC(Garbage Collection)：即垃圾回收器，诞生于1960年MIT的Lisp语言，主要是用来回收，释放垃圾占用的空间。 java GC泛指java的垃圾回收机制，该机制是java与C/C++的主要区别之一，我们在日常写java代码的时候，一般都不需要编写内存回收或者垃圾清理的代码，也不需要像C/C++那样做类似delete/free的操作。 为什么需要学习GC对象的内存分配在java虚拟机的自动内存分配机制下，一般不容易出现内存泄漏问题。但是写代码难免会遇到一些特殊情况，比如OOM神马的。。尽管虚拟机内存的动态分配与内存回收技术很成熟，可万一出现了这样那样的内存溢出问题，那么将难以定位错误的原因所在。 哪些内存要回收java内存模型中分为五大区域已经有所了解。我们知道程序计数器、虚拟机栈、本地方法栈，由线程而生，随线程而灭，其中栈中的栈帧随着方法的进入顺序的执行的入栈和出栈的操作，一个栈帧需要分配多少内存取决于具体的虚拟机实现并且在编译期间即确定下来【忽略JIT编译器做的优化，基本当成编译期间可知】，当方法或线程执行完毕后，内存就随着回收，因此无需关心。 而Java堆、方法区则不一样。方法区存放着类加载信息，但是一个接口中多个实现类需要的内存可能不太一样，一个方法中多个分支需要的内存也可能不一样【只有在运行期间才可知道这个方法创建了哪些对象需要多少内存】，这部分内存的分配和回收都是动态的，gc关注的也正是这部分的内存。 Java堆是GC回收的“重点区域”。堆中基本存放着所有对象实例，gc进行回收前，第一件事就是确认哪些对象存活，哪些死去[即不可能再被引用] 堆的回收区域为了高效的回收，jvm将堆分为三个区域 1.新生代（Young Generation）NewSize和MaxNewSize分别可以控制年轻代的初始大小和最大的大小 2.老年代（Old Generation） 3.永久代（Permanent Generation）【1.8以后采用元空间，就不在堆中了】 判断对象是否存活算法1.引用计数算法早期判断对象是否存活大多都是以这种算法，这种算法判断很简单，简单来说就是给对象添加一个引用计数器，每当对象被引用一次就加1，引用失效时就减1。当为0的时候就判断对象不会再被引用。优点:实现简单效率高，被广泛使用与如python何游戏脚本语言上。缺点:难以解决循环引用的问题，就是假如两个对象互相引用已经不会再被其它其它引用，导致一直不会为0就无法进行回收。 2.可达性分析算法目前主流的商用语言[如java、c#]采用的是可达性分析算法判断对象是否存活。这个算法有效解决了循环利用的弊端。它的基本思路是通过一个称为“GC Roots”的对象为起始点，搜索所经过的路径称为引用链，当一个对象到GC Roots没有任何引用跟它连接则证明对象是不可用的。 可作为GC Roots的对象有四种 ①虚拟机栈(栈桢中的本地变量表)中的引用的对象，就是平时所指的java对象，存放在堆中 ②方法区中的类静态属性引用的对象，一般指被static修饰引用的对象，加载类的时候就加载到内存中 ③方法区中的常量引用的对象 ④本地方法栈中JNI（native方法)引用的对象 要真正宣告对象死亡需经过两个过程。 1.可达性分析后没有发现引用链 2.查看对象是否有finalize方法，如果有重写且在方法内完成自救[比如再建立引用]，还是可以抢救一下，注意这边一个类的finalize只执行一次，这就会出现一样的代码第一次自救成功第二次失败的情况。[如果类重写finalize且还没调用过，会将这个对象放到一个叫做F-Queue的序列里，这边finalize不承诺一定会执行，这么做是因为如果里面死循环的话可能会使F-Queue队列处于等待，严重会导致内存崩溃，这是我们不希望看到的。] 垃圾收集算法1.标记/清除算法【最基础】 2.复制算法 3.标记/整理算法 jvm采用分代收集算法对不同区域采用不同的回收算法。 新生代采用复制算法新生代中因为对象都是”朝生夕死的”，【深入理解JVM虚拟机上说98%的对象,不知道是不是这么多，总之就是存活率很低】，适用于复制算法【复制算法比较适合用于存活率低的内存区域】。它优化了标记/清除算法的效率和内存碎片问题，且JVM不以5:5分配内存【由于存活率低，不需要复制保留那么大的区域造成空间上的浪费，因此不需要按1:1【原有区域:保留空间】划分内存区域，而是将内存分为一块Eden空间和From Survivor、To Survivor【保留空间】，三者默认比例为8:1:1，优先使用Eden区，若Eden区满，则将对象复制到第二块内存区上。但是不能保证每次回收都只有不多于10%的对象存货，所以Survivor区不够的话，则会依赖老年代年进行分配】。 GC开始时，对象只会存于Eden和From Survivor区域，To Survivor【保留空间】为空。 GC进行时，Eden区所有存活的对象都被复制到To Survivor区，而From Survivor区中，仍存活的对象会根据它们的年龄值决定去向，年龄值达到年龄阈值(默认15是因为对象头中年龄占4bit，新生代每熬过一次垃圾回收，年龄+1)，则移到老年代，没有达到则复制到To Survivor。 老年代采用标记/清除算法或标记/整理算法由于老年代存活率高，没有额外空间给他做担保，必须使用这两种算法。 枚举根节点算法GC Roots 被虚拟机用来判断对象是否存活 可达性分析算法需考虑 1.如果方法区几百兆，一个个检查里面的引用，将耗费大量资源。 2.在分析时，需保证这个对象引用关系不再变化，否则结果将不准确。【因此GC进行时需停掉其它所有java执行线程(Sun把这种行为称为‘Stop the World’)，即使是号称几乎不会停顿的CMS收集器，枚举根节点时也需停掉线程】 解决办法:实际上当系统停下来后JVM不需要一个个检查引用，而是通过OopMap数据结构【HotSpot的叫法】来标记对象引用。 虚拟机先得知哪些地方存放对象的引用，在类加载完时。HotSpot把对象内什么偏移量什么类型的数据算出来，在jit编译过程中，也会在特定位置记录下栈和寄存器哪些位置是引用，这样GC在扫描时就可以知道这些信息。【目前主流JVM使用准确式GC】 OopMap可以帮助HotSpot快速且准确完成GC Roots枚举以及确定相关信息。但是也存在一个问题，可能导致引用关系变化。 这个时候有个safepoint(安全点)的概念。 HotSpot中GC不是在任意位置都可以进入，而只能在safepoint处进入。 GC时对一个Java线程来说，它要么处在safepoint,要么不在safepoint。 safepoint不能太少，否则GC等待的时间会很久 safepoint不能太多，否则将增加运行GC的负担 安全点主要存放的位置 1:循环的末尾 2:方法临返回前/调用方法的call指令后 3:可能抛异常的位置 Minor GC、Major GC、FULL GCMinor GC:在年轻代Young space(包括Eden区和Survivor区)中的垃圾回收称之为 Minor GC,Minor GC只会清理年轻代. Major GC:Major GC清理老年代(old GC)，但是通常也可以指和Full GC是等价，因为收集老年代的时候往往也会伴随着升级年轻代，收集整个Java堆。所以有人问的时候需问清楚它指的是full GC还是old GC。 Full GC:full gc是对新生代、老年代、永久代【jdk1.8后没有这个概念了】统一的回收。jvm调优 目的减少full gc次数，full gc的时候会stw,停掉所有线程，影响用户体验的 Linux查看某个服务JVM的GC和堆内存使用情况使用 jps 命令查看配置了JVM的服务Jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java JVM进程的一些简单情况。 jps 列出pid和java主类名 jps -l 列出pid和java主类全称 [root@airthink ~]# jps 23393 jar 5401 jenkins.war 29675 Bootstrap 31692 Bootstrap [root@airthink ~]# jps -l 23393 bubu-0.0.1-SNAPSHOT.jar 5401 /usr/lib/jenkins/jenkins.war 24746 sun.tools.jps.Jps 29675 org.apache.catalina.startup.Bootstrap 31692 org.apache.catalina.startup.Bootstrap jps -lm 列出皮带、主类全称和应用程序参数 [root@airthink ~]# jps -lm 23393 bubu-0.0.1-SNAPSHOT.jar --server.port=7070 5401 /usr/lib/jenkins/jenkins.war --logfile=/var/log/jenkins/jenkins.log --webroot=/var/cache/jenkins/war --daemon --httpPort=9090 --debug=5 --handlerCountMax=100 --handlerCountMaxIdle=20 29675 org.apache.catalina.startup.Bootstrap start 24908 sun.tools.jps.Jps -lm 31692 org.apache.catalina.startup.Bootstrap start jps -v 列出pid和JVM参数 [root@airthink ~]# jps -v 23393 jar 24964 Jps -Denv.class.path=.:/usr/local/jdk1.8.0_162/lib:/usr/local/jdk1.8.0_162/jre/lib: -Dapplication.home=/usr/local/jdk1.8.0_162 -Xms8m 5401 jenkins.war -Dcom.sun.akuma.Daemon=daemonized -Djava.awt.headless=true -DJENKINS_HOME=/var/lib/jenkins 29675 Bootstrap -Djava.util.logging.config.file=/airthink/oauth-apache-tomcat-8.5.46/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -Dcatalina.base=/airthink/oauth-apache-tomcat-8.5.46 -Dcatalina.home=/airthink/oauth-apache-tomcat-8.5.46 -Djava.io.tmpdir=/airthink/oauth-apache-tomcat-8.5.46/temp 31692 Bootstrap -Djava.util.logging.config.file=/airthink/apache-tomcat-8.0.50/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dignore.endorsed.dirs= -Dcatalina.base=/airthink/apache-tomcat-8.0.50 -Dcatalina.home=/airthink/apache-tomcat-8.0.50 -Djava.io.tmpdir=/airthink/apache-tomcat-8.0.50/temp 查看某个进程JVM的GC使用情况jstat -gc 71614 5000 jstat -gc 进程号 刷新时间 S0C：年轻代中第一个survivor（幸存区）的容量 (字节) S1C：年轻代中第二个survivor（幸存区）的容量 (字节) S0U：年轻代中第一个survivor（幸存区）目前已使用空间 (字节) S1U：年轻代中第二个survivor（幸存区）目前已使用空间 (字节) EC：年轻代中Eden（伊甸园）的容量 (字节) EU：年轻代中Eden（伊甸园）目前已使用空间 (字节) OC：Old代的容量 (字节) OU：Old代目前已使用空间 (字节) YGC：从应用程序启动到采样时年轻代中gc次数 YGCT：从应用程序启动到采样时年轻代中gc所用时间(s) FGC：从应用程序启动到采样时old代(全gc)gc次数 FGCT：从应用程序启动到采样时old代(全gc)gc所用时间(s) GCT：从应用程序启动到采样时gc用的总时间(s) 查看堆内存使用情况jmap -heap 进程号 Heap Configuration: #堆配置情况 MinHeapFreeRatio = 40 #堆最小使用比例 MaxHeapFreeRatio = 70 #堆最大使用比例 MaxHeapSize = 482344960 (460.0MB) #堆最大空间 NewSize = 10485760 (10.0MB) #新生代初始化大小 MaxNewSize = 160759808 (153.3125MB) #新生代可使用最大容量大小 OldSize = 20971520 (20.0MB) #老生代大小 NewRatio = 2 #新生代比例 SurvivorRatio = 8 #新生代与suvivor的占比 MetaspaceSize = 21807104 (20.796875MB) #元数据空间初始大小 CompressedClassSpaceSize = 1073741824 (1024.0MB) #类指针压缩空间大小, 默认为1G MaxMetaspaceSize = 17592186044415 MB #元数据空间的最大值, 超过此值就会触发 GC溢出( JVM会动态地改变此值) G1HeapRegionSize = 0 (0.0MB) #区块的大小 Heap Usage: New Generation (Eden + 1 Survivor Space): #新生代大小 capacity = 14876672 (14.1875MB) #区块最大可使用大小 used = 2722520 (2.5963973999023438MB) #区块已使用内存 free = 12154152 (11.591102600097656MB) #区块空闲内存 18.30059841340859% used #区块使用比例 Eden Space: # Eden区空间 capacity = 13238272 (12.625MB) used = 2630736 (2.5088653564453125MB) free = 10607536 (10.116134643554688MB) 19.87220084313119% used From Space: #Survivor0区 capacity = 1638400 (1.5625MB) used = 91784 (0.08753204345703125MB) free = 1546616 (1.4749679565429688MB) 5.60205078125% used To Space: #Survivor1区 capacity = 1638400 (1.5625MB) used = 0 (0.0MB) free = 1638400 (1.5625MB) 0.0% used tenured generation: #老年代 capacity = 33013760 (31.484375MB) used = 26392512 (25.16986083984375MB) free = 6621248 (6.31451416015625MB) 79.94397487593052% used linux服务器或tomcat项目启动，进行jvm参数调优设置首先执行命令：free -h，查询当前的内存占用情况 [root@airthink ~]# free -h total used free shared buff/cache available Mem: 1.8G 1.1G 111M 680K 553M 498M Swap: 0B 0B 0B 开始进行优化，执行命令：top，查看各个应用的内存占用情况，选取内存占用过高的pid进程； 然后获取pid号31692，根据pid查询对应的进程以及项目路径，执行命令： ps -aux |grep -v grep|grep 31692 [root@airthink ~]# ps -aux |grep -v grep|grep 31692 root 31692 0.1 17.2 2575436 324396 ? Sl Feb24 16:53 /usr/local/jdk1.8.0_162/jre/bin/java -Djava.util.logging.config.file=/airthink/apache-tomcat-8.0.50/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dignore.endorsed.dirs= -classpath /airthink/apache-tomcat-8.0.50/bin/bootstrap.jar:/airthink/apache-tomcat-8.0.50/bin/tomcat-juli.jar -Dcatalina.base=/airthink/apache-tomcat-8.0.50 -Dcatalina.home=/airthink/apache-tomcat-8.0.50 -Djava.io.tmpdir=/airthink/apache-tomcat-8.0.50/temp org.apache.catalina.startup.Bootstrap start 定位到项目跟路径之后，开始设置项目启动jvm内存占用，不同项目可分配不同的内存 如果是springboot项目jar启动，则在启动的时候指定jvm的内存分配： java -Xms128m -Xmx256m -jar xxx.jar --server.port =8080 如果是tomcat项目启动，则在bin目录下，执行命令：vim catalina.sh，然后在顶部加上： JAVA_OPTS=&quot;-Xms128m -Xmx256m&quot;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[uni-app]]></title>
    <url>%2F2020%2F02%2F18%2Funi-app%2F</url>
    <content type="text"><![CDATA[uni-app的基本使用课程介绍： 基础部分： 环境搭建 页面外观配置 数据绑定 uni-app的生命周期 组件的使用 uni-app中样式学习 在uni-app中使用字体图标和开启scss 条件注释跨端兼容 uni中的事件 导航跳转 组件创建和通讯，及组件的生命周期 uni-app中使用uni-ui库 项目：黑马商城项目 uni-app介绍 官方网页uni-app 是一个使用 Vue.js 开发所有前端应用的框架，开发者编写一套代码，可发布到iOS、Android、H5、以及各种小程序（微信/支付宝/百度/头条/QQ/钉钉）等多个平台。 即使不跨端，uni-app同时也是更好的小程序开发框架。 具有vue和微信小程序的开发经验，可快速上手uni-app 为什么要去学习uni-app？ 相对开发者来说，减少了学习成本，因为只学会uni-app之后，即可开发出iOS、Android、H5、以及各种小程序的应用，不需要再去学习开发其他应用的框架，相对公司而言，也大大减少了开发成本。 环境搭建安装编辑器HbuilderX 下载地址 HBuilderX是通用的前端开发工具，但为uni-app做了特别强化。 下载App开发版，可开箱即用 安装微信开发者工具 下载地址 利用HbuilderX初始化项目 点击HbuilderX菜单栏文件&gt;项目&gt;新建 选择uni-app,填写项目名称，项目创建的目录 运行项目在菜单栏中点击运行，运行到浏览器，选择浏览器即可运行 在微信开发者工具里运行：进入hello-uniapp项目，点击工具栏的运行 -&gt; 运行到小程序模拟器 -&gt; 微信开发者工具，即可在微信开发者工具里面体验uni-app 在微信开发者工具里运行：进入hello-uniapp项目，点击工具栏的运行 -&gt; 运行到手机或模拟器 -&gt; 选择调式的手机 注意： 如果是第一次使用，需要先配置小程序ide的相关路径，才能运行成功 微信开发者工具在设置中安全设置，服务端口开启 介绍项目目录和文件作用pages.json 文件用来对 uni-app 进行全局配置，决定页面文件的路径、窗口样式、原生的导航栏、底部的原生tabbar 等 manifest.json 文件是应用的配置文件，用于指定应用的名称、图标、权限等。 App.vue是我们的跟组件，所有页面都是在App.vue下进行切换的，是页面入口文件，可以调用应用的生命周期函数。 main.js是我们的项目入口文件，主要作用是初始化vue实例并使用需要的插件。 uni.scss文件的用途是为了方便整体控制应用的风格。比如按钮颜色、边框风格，uni.scss文件里预置了一批scss变量预置。 就是打包目录，在这里有各个平台的打包文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647```pages``` 所有的页面存放目录```static``` 静态资源目录，例如图片等```components``` 组件存放目录为了实现多端兼容，综合考虑编译速度、运行性能等因素，`uni-app` 约定了如下开发规范：- 页面文件遵循 [Vue 单文件组件 (SFC) 规范](https://vue-loader.vuejs.org/zh/spec.html)- 组件标签靠近小程序规范，详见[uni-app 组件规范](https://uniapp.dcloud.io/component/README)- 接口能力（JS API）靠近微信小程序规范，但需将前缀 `wx` 替换为 `uni`，详见[uni-app接口规范](https://uniapp.dcloud.io/api/README)- 数据绑定及事件处理同 `Vue.js` 规范，同时补充了App及页面的生命周期- 为兼容多端运行，建议使用flex布局进行开发#### 全局配置和页面配置##### 通过globalStyle进行全局配置用于设置应用的状态栏、导航条、标题、窗口背景色等。[详细文档](https://uniapp.dcloud.io/collocation/pages?id=globalstyle)| 属性 | 类型 | 默认值 | 描述 || ---------------------------- | -------- | ------- | ---------------------------------------- || navigationBarBackgroundColor | HexColor | #F7F7F7 | 导航栏背景颜色（同状态栏背景色） || navigationBarTextStyle | String | white | 导航栏标题颜色及状态栏前景颜色，仅支持 black/white || navigationBarTitleText | String | | 导航栏标题文字内容 || backgroundColor | HexColor | #ffffff | 窗口的背景色 || backgroundTextStyle | String | dark | 下拉 loading 的样式，仅支持 dark / light || enablePullDownRefresh | Boolean | false | 是否开启下拉刷新，详见[页面生命周期](https://uniapp.dcloud.io/use?id=%e9%a1%b5%e9%9d%a2%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f)。 || onReachBottomDistance | Number | 50 | 页面上拉触底事件触发时距页面底部距离，单位只支持px，详见[页面生命周期](https://uniapp.dcloud.io/use?id=%e9%a1%b5%e9%9d%a2%e7%94%9f%e5%91%bd%e5%91%a8%e6%9c%9f) |##### 创建新的message页面右键pages新建message目录，在message目录下右键新建.vue文件,并选择基本模板```html&lt;template&gt; &lt;view&gt; 这是信息页面 &lt;/view&gt;&lt;/template&gt;&lt;script&gt;&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 通过pages来配置页面 属性 类型 默认值 描述 path String 配置页面路径 style Object 配置页面窗口表现，配置项参考 pageStyle pages数组数组中第一项表示应用启动页 1234567891011"pages": [ 、 &#123; "path":"pages/message/message" &#125;, &#123; "path": "pages/index/index", "style": &#123; "navigationBarTitleText": "uni-app" &#125; &#125; ] 通过style修改页面的标题和导航栏背景色，并且设置h5下拉刷新的特有样式 12345678910111213141516"pages": [ //pages数组中第一项表示应用启动页，参考：https://uniapp.dcloud.io/collocation/pages &#123; "path":"pages/message/message", "style": &#123; "navigationBarBackgroundColor": "#007AFF", "navigationBarTextStyle": "white", "enablePullDownRefresh": true, "disableScroll": true, "h5": &#123; "pullToRefresh": &#123; "color": "#007AFF" &#125; &#125; &#125; &#125; ] 配置tabbar如果应用是一个多 tab 应用，可以通过 tabBar 配置项指定 tab 栏的表现，以及 tab 切换时显示的对应页。 Tips 当设置 position 为 top 时，将不会显示 icon tabBar 中的 list 是一个数组，只能配置最少2个、最多5个 tab，tab 按数组的顺序排序。 属性说明： 属性 类型 必填 默认值 描述 平台差异说明 color HexColor 是 tab 上的文字默认颜色 selectedColor HexColor 是 tab 上的文字选中时的颜色 backgroundColor HexColor 是 tab 的背景色 borderStyle String 否 black tabbar 上边框的颜色，仅支持 black/white App 2.3.4+ 支持其他颜色值 list Array 是 tab 的列表，详见 list 属性说明，最少2个、最多5个 tab position String 否 bottom 可选值 bottom、top top 值仅微信小程序支持 其中 list 接收一个数组，数组中的每个项都是一个对象，其属性值如下： 属性 类型 必填 说明 pagePath String 是 页面路径，必须在 pages 中先定义 text String 是 tab 上按钮文字，在 5+APP 和 H5 平台为非必填。例如中间可放一个没有文字的+号图标 iconPath String 否 图片路径，icon 大小限制为40kb，建议尺寸为 81px * 81px，当 postion 为 top 时，此参数无效，不支持网络图片，不支持字体图标 selectedIconPath String 否 选中时的图片路径，icon 大小限制为40kb，建议尺寸为 81px * 81px ，当 postion 为 top 时，此参数无效 案例代码： 12345678910111213141516171819202122"tabBar": &#123; "list": [ &#123; "text": "首页", "pagePath":"pages/index/index", "iconPath":"static/tabs/home.png", "selectedIconPath":"static/tabs/home-active.png" &#125;, &#123; "text": "信息", "pagePath":"pages/message/message", "iconPath":"static/tabs/message.png", "selectedIconPath":"static/tabs/message-active.png" &#125;, &#123; "text": "我们", "pagePath":"pages/contact/contact", "iconPath":"static/tabs/contact.png", "selectedIconPath":"static/tabs/contact-active.png" &#125; ] &#125; condition启动模式配置启动模式配置，仅开发期间生效，用于模拟直达页面的场景，如：小程序转发后，用户点击所打开的页面。 属性说明： 属性 类型 是否必填 描述 current Number 是 当前激活的模式，list节点的索引值 list Array 是 启动模式列表 list说明： 属性 类型 是否必填 描述 name String 是 启动模式名称 path String 是 启动页面路径 query String 否 启动参数，可在页面的 onLoad 函数里获得 组件的基本使用uni-app提供了丰富的基础组件给开发者，开发者可以像搭积木一样，组合各种组件拼接称自己的应用 uni-app中的组件，就像 HTML 中的 div 、p、span 等标签的作用一样，用于搭建页面的基础结构 text文本组件的用法001 - text 组件的属性 属性 类型 默认值 必填 说明 selectable boolean false 否 文本是否可选 space string . 否 显示连续空格，可选参数：ensp、emsp、nbsp decode boolean false 否 是否解码 text 组件相当于行内标签、在同一行显示 除了文本节点以外的其他节点都无法长按选中 002 - 代码案例1234567891011121314151617181920212223242526&lt;view&gt; &lt;!-- 长按文本是否可选 --&gt; &lt;text selectable='true'&gt;来了老弟&lt;/text&gt;&lt;/view&gt;&lt;view&gt; &lt;!-- 显示连续空格的方式 --&gt; &lt;view&gt; &lt;text space='ensp'&gt;来了 老弟&lt;/text&gt; &lt;/view&gt; &lt;view&gt; &lt;text space='emsp'&gt;来了 老弟&lt;/text&gt; &lt;/view&gt; &lt;view&gt; &lt;text space='nbsp'&gt;来了 老弟&lt;/text&gt; &lt;/view&gt;&lt;/view&gt;&lt;view&gt; &lt;text&gt;skyblue&lt;/text&gt;&lt;/view&gt;&lt;view&gt; &lt;!-- 是否解码 --&gt; &lt;text decode='true'&gt;&amp;nbsp; &amp;lt; &amp;gt; &amp;amp; &amp;apos; &amp;ensp; &amp;emsp;&lt;/text&gt;&lt;/view&gt; view视图容器组件的用法 View 视图容器， 类似于 HTML 中的 div 001 - 组件的属性 002 - 代码案例12345&lt;view class="box2" hover-class="box2_active"&gt; &lt;view class='box1' hover-class='active' hover-stop-propagation :hover-start-time="2000" :hover-stay-time='2000'&gt; &lt;/view&gt;&lt;/view&gt; button按钮组件的用法001 - 组件的属性 属性名 类型 默认值 说明 size String default 按钮的大小 type String default 按钮的样式类型 plain Boolean false 按钮是否镂空，背景色透明 disabled Boolean false 是否按钮 loading Boolean false 名称是否带 loading t图标 button 组件默认独占一行，设置 size 为 mini 时可以在一行显示多个 002 - 案例代码12345&lt;button size='mini' type='primary'&gt;前端&lt;/button&gt;&lt;button size='mini' type='default' disabled='true'&gt;前端&lt;/button&gt;&lt;button size='mini' type='warn' loading='true'&gt;前端&lt;/button&gt; image组件的使用image图片。 属性名 类型 默认值 说明 平台差异说明 src String 图片资源地址 mode String ‘scaleToFill’ 图片裁剪、缩放的模式 Tips &lt;image&gt; 组件默认宽度 300px、高度 225px； src 仅支持相对路径、绝对路径，支持 base64 码； 页面结构复杂，css样式太多的情况，使用 image 可能导致样式生效较慢，出现 “闪一下” 的情况，此时设置 image{will-change: transform} ,可优化此问题。 uni-app中的样式 rpx 即响应式px，一种根据屏幕宽度自适应的动态单位。以750宽的屏幕为基准，750rpx恰好为屏幕宽度。屏幕变宽，rpx 实际显示效果会等比放大。 使用@import语句可以导入外联样式表，@import后跟需要导入的外联样式表的相对路径，用;表示语句结束 支持基本常用的选择器class、id、element等 在 uni-app 中不能使用 * 选择器。 page 相当于 body 节点 定义在 App.vue 中的样式为全局样式，作用于每一个页面。在 pages 目录下 的 vue 文件中定义的样式为局部样式，只作用在对应的页面，并会覆盖 App.vue 中相同的选择器。 uni-app 支持使用字体图标，使用方式与普通 web 项目相同，需要注意以下几点： 字体文件小于 40kb，uni-app 会自动将其转化为 base64 格式； 字体文件大于等于 40kb， 需开发者自己转换，否则使用将不生效； 字体文件的引用路径推荐使用以 ~@ 开头的绝对路径。 1234@font-face &#123; font-family: test1-icon; src: url(&apos;~@/static/iconfont.ttf&apos;);&#125; 如何使用scss或者less uni-app中的数据绑定在页面中需要定义数据，和我们之前的vue一摸一样，直接在data中定义数据即可 1234567export default &#123; data () &#123; return &#123; msg: 'hello-uni' &#125; &#125;&#125; 插值表达式的使用 利用插值表达式渲染基本数据 1&lt;view&gt;&#123;&#123;msg&#125;&#125;&lt;/view&gt; 在插值表达式中使用三元运算 1&lt;view&gt;&#123;&#123; flag ? '我是真的':'我是假的' &#125;&#125;&lt;/view&gt; 基本运算 1&lt;view&gt;&#123;&#123;1+1&#125;&#125;&lt;/view&gt; v-bind动态绑定属性在data中定义了一张图片，我们希望把这张图片渲染到页面上 1234567export default &#123; data () &#123; return &#123; img: 'http://destiny001.gitee.io/image/monkey_02.jpg' &#125; &#125;&#125; 利用v-bind进行渲染 1&lt;image v-bind:src="img"&gt;&lt;/image&gt; 还可以缩写成: 1&lt;image :src="img"&gt;&lt;/image&gt; v-for的使用data中定以一个数组，最终将数组渲染到页面上 12345678910data () &#123; return &#123; arr: [ &#123; name: '刘能', age: 29 &#125;, &#123; name: '赵四', age: 39 &#125;, &#123; name: '宋小宝', age: 49 &#125;, &#123; name: '小沈阳', age: 59 &#125; ] &#125;&#125; 利用v-for进行循环 1&lt;view v-for="(item,i) in arr" :key="i"&gt;名字：&#123;&#123;item.name&#125;&#125;---年龄：&#123;&#123;item.age&#125;&#125;&lt;/view&gt; uni中的事件事件绑定在uni中事件绑定和vue中是一样的，通过v-on进行事件的绑定，也可以简写为@ 1&lt;button @click="tapHandle"&gt;点我啊&lt;/button&gt; 事件函数定义在methods中 12345methods: &#123; tapHandle () &#123; console.log('真的点我了') &#125;&#125; 事件传参 默认如果没有传递参数，事件函数第一个形参为事件对象 12345678// template&lt;button @click=&quot;tapHandle&quot;&gt;点我啊&lt;/button&gt;// scriptmethods: &#123; tapHandle (e) &#123; console.log(e) &#125;&#125; 如果给事件函数传递参数了，则对应的事件函数形参接收的则是传递过来的数据 12345678// template&lt;button @click=&quot;tapHandle(1)&quot;&gt;点我啊&lt;/button&gt;// scriptmethods: &#123; tapHandle (num) &#123; console.log(num) &#125;&#125; 如果获取事件对象也想传递参数 12345678// template&lt;button @click=&quot;tapHandle(1,$event)&quot;&gt;点我啊&lt;/button&gt;// scriptmethods: &#123; tapHandle (num,e) &#123; console.log(num,e) &#125;&#125; uni的生命周期应用的生命周期生命周期的概念：一个对象从创建、运行、销毁的整个过程被成为生命周期。 生命周期函数：在生命周期中每个阶段会伴随着每一个函数的触发，这些函数被称为生命周期函数 uni-app 支持如下应用生命周期函数： 函数名 说明 onLaunch 当uni-app 初始化完成时触发（全局只触发一次） onShow 当 uni-app 启动，或从后台进入前台显示 onHide 当 uni-app 从前台进入后台 onError 当 uni-app 报错时触发 页面的生命周期uni-app 支持如下页面生命周期函数： 函数名 说明 平台差异说明 最低版本 onLoad 监听页面加载，其参数为上个页面传递的数据，参数类型为Object（用于页面传参），参考示例 onShow 监听页面显示。页面每次出现在屏幕上都触发，包括从下级页面点返回露出当前页面 onReady 监听页面初次渲染完成。 onHide 监听页面隐藏 onUnload 监听页面卸载 下拉刷新开启下拉刷新在uni-app中有两种方式开启下拉刷新 需要在 pages.json 里，找到的当前页面的pages节点，并在 style 选项中开启 enablePullDownRefresh 通过调用uni.startPullDownRefresh方法来开启下拉刷新 通过配置文件开启创建list页面进行演示 123456789101112131415161718192021&lt;template&gt; &lt;view&gt; 杭州学科 &lt;view v-for="(item,index) in arr" :key="index"&gt; &#123;&#123;item&#125;&#125; &lt;/view&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; data () &#123; return &#123; arr: ['前端','java','ui','大数据'] &#125; &#125; &#125;&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 通过pages.json文件中找到当前页面的pages节点，并在 style 选项中开启 enablePullDownRefresh 123456&#123; "path":"pages/list/list", "style":&#123; "enablePullDownRefresh": true &#125;&#125; 通过API开启api文档 1uni.startPullDownRefresh() 监听下拉刷新通过onPullDownRefresh可以监听到下拉刷新的动作 123456789101112131415export default &#123; data () &#123; return &#123; arr: ['前端','java','ui','大数据'] &#125; &#125;, methods: &#123; startPull () &#123; uni.startPullDownRefresh() &#125; &#125;, onPullDownRefresh () &#123; console.log('触发下拉刷新了') &#125;&#125; 关闭下拉刷新uni.stopPullDownRefresh() 停止当前页面下拉刷新。 案例演示 12345678910111213141516171819202122232425262728293031&lt;template&gt; &lt;view&gt; &lt;button type="primary" @click="startPull"&gt;开启下拉刷新&lt;/button&gt; 杭州学科 &lt;view v-for="(item,index) in arr" :key="index"&gt; &#123;&#123;item&#125;&#125; &lt;/view&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; data () &#123; return &#123; arr: ['前端','java','ui','大数据'] &#125; &#125;, methods: &#123; startPull () &#123; uni.startPullDownRefresh() &#125; &#125;, onPullDownRefresh () &#123; this.arr = [] setTimeout(()=&gt; &#123; this.arr = ['前端','java','ui','大数据'] uni.stopPullDownRefresh() &#125;, 1000); &#125; &#125;&lt;/script&gt; 上拉加载通过在pages.json文件中找到当前页面的pages节点下style中配置onReachBottomDistance可以设置距离底部开启加载的距离，默认为50px 通过onReachBottom监听到触底的行为 12345678910111213141516171819202122232425262728&lt;template&gt; &lt;view&gt; &lt;button type="primary" @click="startPull"&gt;开启下拉刷新&lt;/button&gt; 杭州学科 &lt;view v-for="(item,index) in arr" :key="index"&gt; &#123;&#123;item&#125;&#125; &lt;/view&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; data () &#123; return &#123; arr: ['前端','java','ui','大数据','前端','java','ui','大数据'] &#125; &#125;, onReachBottom () &#123; console.log('触底了') &#125; &#125;&lt;/script&gt;&lt;style&gt; view&#123; height: 100px; line-height: 100px; &#125;&lt;/style&gt; 网络请求在uni中可以调用uni.request方法进行请求网络请求 需要注意的是：在小程序中网络相关的 API 在使用前需要配置域名白名单。 发送get请求 12345678910111213141516171819&lt;template&gt; &lt;view&gt; &lt;button @click="sendGet"&gt;发送请求&lt;/button&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; methods: &#123; sendGet () &#123; uni.request(&#123; url: 'http://localhost:8082/api/getlunbo', success(res) &#123; console.log(res) &#125; &#125;) &#125; &#125; &#125;&lt;/script&gt; 发送post请求 数据缓存uni.setStorage官方文档 将数据存储在本地缓存中指定的 key 中，会覆盖掉原来该 key 对应的内容，这是一个异步接口。 代码演示 123456789101112131415161718192021222324&lt;template&gt; &lt;view&gt; &lt;button type="primary" @click="setStor"&gt;存储数据&lt;/button&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; methods: &#123; setStor () &#123; uni.setStorage(&#123; key: 'id', data: 100, success () &#123; console.log('存储成功') &#125; &#125;) &#125; &#125; &#125;&lt;/script&gt;&lt;style&gt;&lt;/style&gt; uni.setStorageSync将 data 存储在本地缓存中指定的 key 中，会覆盖掉原来该 key 对应的内容，这是一个同步接口。 代码演示 123456789101112131415161718&lt;template&gt; &lt;view&gt; &lt;button type="primary" @click="setStor"&gt;存储数据&lt;/button&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; methods: &#123; setStor () &#123; uni.setStorageSync('id',100) &#125; &#125; &#125;&lt;/script&gt;&lt;style&gt;&lt;/style&gt; uni.getStorage从本地缓存中异步获取指定 key 对应的内容。 代码演示 123456789101112131415161718192021222324&lt;template&gt; &lt;view&gt; &lt;button type="primary" @click="getStorage"&gt;获取数据&lt;/button&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; data () &#123; return &#123; id: '' &#125; &#125;, methods: &#123; getStorage () &#123; uni.getStorage(&#123; key: 'id', success: res=&gt;&#123; this.id = res.data &#125; &#125;) &#125; &#125; &#125;&lt;/script&gt; uni.getStorageSync从本地缓存中同步获取指定 key 对应的内容。 代码演示 123456789101112131415&lt;template&gt; &lt;view&gt; &lt;button type="primary" @click="getStorage"&gt;获取数据&lt;/button&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; methods: &#123; getStorage () &#123; const id = uni.getStorageSync('id') console.log(id) &#125; &#125; &#125;&lt;/script&gt; uni.removeStorage从本地缓存中异步移除指定 key。 代码演示 12345678910111213141516171819&lt;template&gt; &lt;view&gt; &lt;button type="primary" @click="removeStorage"&gt;删除数据&lt;/button&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; methods: &#123; removeStorage () &#123; uni.removeStorage(&#123; key: 'id', success: function () &#123; console.log('删除成功') &#125; &#125;) &#125; &#125; &#125;&lt;/script&gt; uni.removeStorageSync从本地缓存中同步移除指定 key。 代码演示 1234567891011121314&lt;template&gt; &lt;view&gt; &lt;button type="primary" @click="removeStorage"&gt;删除数据&lt;/button&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; methods: &#123; removeStorage () &#123; uni.removeStorageSync('id') &#125; &#125; &#125;&lt;/script&gt; 上传图片、预览图片上传图片uni.chooseImage方法从本地相册选择图片或使用相机拍照。 案例代码 12345678910111213141516171819202122232425262728&lt;template&gt; &lt;view&gt; &lt;button @click="chooseImg" type="primary"&gt;上传图片&lt;/button&gt; &lt;view&gt; &lt;image v-for="item in imgArr" :src="item" :key="index"&gt;&lt;/image&gt; &lt;/view&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; data () &#123; return &#123; imgArr: [] &#125; &#125;, methods: &#123; chooseImg () &#123; uni.chooseImage(&#123; count: 9, success: res=&gt;&#123; this.imgArr = res.tempFilePaths &#125; &#125;) &#125; &#125; &#125;&lt;/script&gt; 预览图片结构 123&lt;view&gt; &lt;image v-for="item in imgArr" :src="item" @click="previewImg(item)" :key="item"&gt;&lt;/image&gt;&lt;/view&gt; 预览图片的方法 123456previewImg (current) &#123; uni.previewImage(&#123; urls: this.imgArr, current &#125;)&#125; 条件注释实现跨段兼容条件编译是用特殊的注释作为标记，在编译时根据这些特殊的注释，将注释里面的代码编译到不同平台。 写法：以 #ifdef 加平台标识 开头，以 #endif 结尾。 平台标识 值 平台 参考文档 APP-PLUS 5+App HTML5+ 规范 H5 H5 MP-WEIXIN 微信小程序 微信小程序 MP-ALIPAY 支付宝小程序 支付宝小程序 MP-BAIDU 百度小程序 百度小程序 MP-TOUTIAO 头条小程序 头条小程序 MP-QQ QQ小程序 （目前仅cli版支持） MP 微信小程序/支付宝小程序/百度小程序/头条小程序/QQ小程序 组件的条件注释代码演示 123456789101112131415&lt;!-- #ifdef H5 --&gt;&lt;view&gt; h5页面会显示&lt;/view&gt;&lt;!-- #endif --&gt;&lt;!-- #ifdef MP-WEIXIN --&gt;&lt;view&gt; 微信小程序会显示&lt;/view&gt;&lt;!-- #endif --&gt;&lt;!-- #ifdef APP-PLUS --&gt;&lt;view&gt; app会显示&lt;/view&gt;&lt;!-- #endif --&gt; api的条件注释代码演示 12345678onLoad () &#123; //#ifdef MP-WEIXIN console.log('微信小程序') //#endif //#ifdef H5 console.log('h5页面') //#endif&#125; 样式的条件注释 代码演示 1234567891011121314/* #ifdef H5 */view&#123; height: 100px; line-height: 100px; background: red;&#125;/* #endif *//* #ifdef MP-WEIXIN */view&#123; height: 100px; line-height: 100px; background: green;&#125;/* #endif */ uni中的导航跳转利用navigator进行跳转navigator详细文档：文档地址 跳转到普通页面 123&lt;navigator url="/pages/about/about" hover-class="navigator-hover"&gt; &lt;button type="default"&gt;跳转到关于页面&lt;/button&gt;&lt;/navigator&gt; 跳转到tabbar页面 123&lt;navigator url="/pages/message/message" open-type="switchTab"&gt; &lt;button type="default"&gt;跳转到message页面&lt;/button&gt;&lt;/navigator&gt; 利用编程式导航进行跳转导航跳转文档) 利用navigateTo进行导航跳转 保留当前页面，跳转到应用内的某个页面，使用uni.navigateBack可以返回到原页面。 1&lt;button type="primary" @click="goAbout"&gt;跳转到关于页面&lt;/button&gt; 通过navigateTo方法进行跳转到普通页面 12345goAbout () &#123; uni.navigateTo(&#123; url: '/pages/about/about', &#125;)&#125; 通过switchTab跳转到tabbar页面 跳转到tabbar页面 1&lt;button type="primary" @click="goMessage"&gt;跳转到message页面&lt;/button&gt; 通过switchTab方法进行跳转 12345goMessage () &#123; uni.switchTab(&#123; url: '/pages/message/message' &#125;)&#125; redirectTo进行跳转 关闭当前页面，跳转到应用内的某个页面。 12345678&lt;!-- template --&gt;&lt;button type="primary" @click="goMessage"&gt;跳转到message页面&lt;/button&gt;&lt;!-- js --&gt;goMessage () &#123; uni.switchTab(&#123; url: '/pages/message/message' &#125;)&#125; 通过onUnload测试当前组件确实卸载 123onUnload () &#123; console.log('组件卸载了')&#125; 导航跳转传递参数在导航进行跳转到下一个页面的同时，可以给下一个页面传递相应的参数，接收参数的页面可以通过onLoad生命周期进行接收 传递参数的页面 12345goAbout () &#123; uni.navigateTo(&#123; url: '/pages/about/about?id=80', &#125;);&#125; 接收参数的页面 1234567&lt;script&gt; export default &#123; onLoad (options) &#123; console.log(options) &#125; &#125;&lt;/script&gt; #### uni-app中组件的创建在uni-app中，可以通过创建一个后缀名为vue的文件，即创建一个组件成功，其他组件可以将该组件通过impot的方式导入，在通过components进行注册即可 创建login组件，在component中创建login目录，然后新建login.vue文件 1234567891011&lt;template&gt; &lt;view&gt; 这是一个自定义组件 &lt;/view&gt;&lt;/template&gt;&lt;script&gt;&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 在其他组件中导入该组件并注册 1import login from &quot;@/components/test/test.vue&quot; 注册组件 1components: &#123;test&#125; 使用组件 1&lt;test&gt;&lt;/test&gt; 组件的生命周期函数 beforeCreate 在实例初始化之后被调用。详见 created 在实例创建完成后被立即调用。详见 beforeMount 在挂载开始之前被调用。详见 mounted 挂载到实例上去之后调用。详见 注意：此处并不能确定子组件被全部挂载，如果需要子组件完全挂载之后在执行操作可以使用$nextTickVue官方文档 beforeUpdate 数据更新时调用，发生在虚拟 DOM 打补丁之前。详见 仅H5平台支持 updated 由于数据更改导致的虚拟 DOM 重新渲染和打补丁，在这之后会调用该钩子。详见 仅H5平台支持 beforeDestroy 实例销毁之前调用。在这一步，实例仍然完全可用。详见 destroyed Vue 实例销毁后调用。调用后，Vue 实例指示的所有东西都会解绑定，所有的事件监听器会被移除，所有的子实例也会被销毁。详见 组件的通讯父组件给子组件传值通过props来接受外界传递到组件内部的值 1234567891011121314&lt;template&gt; &lt;view&gt; 这是一个自定义组件 &#123;&#123;msg&#125;&#125; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; props: [&apos;msg&apos;] &#125;&lt;/script&gt;&lt;style&gt;&lt;/style&gt; 其他组件在使用login组件的时候传递值 123456789101112131415161718&lt;template&gt; &lt;view&gt; &lt;test :msg=&quot;msg&quot;&gt;&lt;/test&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; import test from &quot;@/components/test/test.vue&quot; export default &#123; data () &#123; return &#123; msg: &apos;hello&apos; &#125; &#125;, components: &#123;test&#125; &#125;&lt;/script&gt; 子组件给父组件传值通过$emit触发事件进行传递参数 123456789101112131415161718192021222324252627&lt;template&gt; &lt;view&gt; 这是一个自定义组件 &#123;&#123;msg&#125;&#125; &lt;button type="primary" @click="sendMsg"&gt;给父组件传值&lt;/button&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; export default &#123; data () &#123; return &#123; status: '打篮球' &#125; &#125;, props: &#123; msg: &#123; type: String, value: '' &#125; &#125;, methods: &#123; sendMsg () &#123; this.$emit('myEvent',this.status) &#125; &#125; &#125;&lt;/script&gt; 父组件定义自定义事件并接收参数 12345678910111213141516171819202122&lt;template&gt; &lt;view&gt; &lt;test :msg="msg" @myEvent="getMsg"&gt;&lt;/test&gt; &lt;/view&gt;&lt;/template&gt;&lt;script&gt; import test from "@/components/test/test.vue" export default &#123; data () &#123; return &#123; msg: 'hello' &#125; &#125;, methods: &#123; getMsg (res) &#123; console.log(res) &#125; &#125;, components: &#123;test&#125; &#125;&lt;/script&gt; 兄弟组件通讯uni-ui的使用uni-ui文档 1、进入Grid宫格组件 2、使用HBuilderX导入该组件 3、导入该组件 12import uniGrid from "@/components/uni-grid/uni-grid.vue"import uniGridItem from "@/components/uni-grid-item/uni-grid-item.vue" 4、注册组件 1components: &#123;uniGrid,uniGridItem&#125; 5、使用组件 1234567891011&lt;uni-grid :column="3"&gt; &lt;uni-grid-item&gt; &lt;text class="text"&gt;文本&lt;/text&gt; &lt;/uni-grid-item&gt; &lt;uni-grid-item&gt; &lt;text class="text"&gt;文本&lt;/text&gt; &lt;/uni-grid-item&gt; &lt;uni-grid-item&gt; &lt;text class="text"&gt;文本&lt;/text&gt; &lt;/uni-grid-item&gt;&lt;/uni-grid&gt; 模拟器 adb kill-server //结束adb服务 adb start-server //启动adb服务 adb devices //获取adb设备列表]]></content>
      <categories>
        <category>uni-app</category>
      </categories>
      <tags>
        <tag>uni-app</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10学linux]]></title>
    <url>%2F2020%2F02%2F16%2Fwin10%E5%AD%A6linux%2F</url>
    <content type="text"><![CDATA[Linux环境win10家庭版自带虚拟机Hyper-V的设置一般win10家庭版并不带Hyper-V虚拟机，解决方法： 新建Hyper-V.bat文件，内容为 pushd &quot;%~dp0&quot; dir /b %SystemRoot%\servicing\Packages\*Hyper-V*.mum &gt;hyper-v.txt for /f %%i in (&apos;findstr /i . hyper-v.txt 2^&gt;nul&apos;) do dism /online /norestart /add-package:&quot;%SystemRoot%\servicing\Packages\%%i&quot; del hyper-v.txt Dism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL 右键-以管理员身份运行，这时就会自动的安装虚拟机功能。 下载linux镜像下载linux镜像，运行Hyper-V管理器，并加载linux镜像]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx]]></title>
    <url>%2F2020%2F02%2F13%2Fnginx%2F</url>
    <content type="text"><![CDATA[nginx介绍Nginx(“engine x”)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 Nginx 安装下载nginx下载地址：http://nginx.org/download/nginx-1.6.2.tar.gz [root@bogon src]# cd /usr/local/src/ [root@bogon src]# wget http://nginx.org/download/nginx-1.6.2.tar.gz 解压安装包[root@bogon src]# tar zxvf nginx-1.6.2.tar.gz 编译安装[root@bogon nginx-1.6.2]# ./configure --prefix=/usr/local/webserver/nginx --with-http_stub_status_module --with-http_ssl_module --with-pcre=/usr/local/src/pcre-8.35 [root@bogon nginx-1.6.2]# make [root@bogon nginx-1.6.2]# make install 查看nginx版本[root@bogon nginx-1.6.2]# /usr/local/webserver/nginx/sbin/nginx -v Nginx 配置创建 Nginx 运行使用的用户 www[root@bogon conf]# /usr/sbin/groupadd www [root@bogon conf]# /usr/sbin/useradd -g www www 配置nginx.conf ，将/usr/local/webserver/nginx/conf/nginx.conf替换为以下内容[root@bogon conf]# cat /usr/local/webserver/nginx/conf/nginx.conf user www www; worker_processes 2; #设置值和CPU核心数一致 error_log /usr/local/webserver/nginx/logs/nginx_error.log crit; #日志位置和日志级别 pid /usr/local/webserver/nginx/nginx.pid; #Specifies the value for maximum file descriptors that can be opened by this process. worker_rlimit_nofile 65535; events { use epoll; worker_connections 65535; } http { include mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; $http_x_forwarded_for&apos;; #charset gb2312; server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 8m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/plain application/x-javascript text/css application/xml; gzip_vary on; #limit_zone crawler $binary_remote_addr 10m; #下面是server虚拟主机的配置 server { listen 80;#监听端口 server_name localhost;#域名 index index.html index.htm index.php; root /usr/local/webserver/nginx/html;#站点目录 location ~ .*\.(php|php5)?$ { #fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } location ~ .*\.(gif|jpg|jpeg|png|bmp|swf|ico)$ { expires 30d; # access_log off; } location ~ .*\.(js|css)?$ { expires 15d; # access_log off; } access_log off; } } 检查配置文件nginx.conf的正确性命令[root@bogon conf]# /usr/local/webserver/nginx/sbin/nginx -t 启动 Nginx[root@bogon conf]# /usr/local/webserver/nginx/sbin/nginx Nginx 其他命令/usr/local/webserver/nginx/sbin/nginx -s reload # 重新载入配置文件 /usr/local/webserver/nginx/sbin/nginx -s reopen # 重启 Nginx /usr/local/webserver/nginx/sbin/nginx -s stop # 停止 Nginx 详细配置#user nobody; #配置用户或者组，默认为nobody nobody。 worker_processes auto; #number | auto;默认为1，最好配置成auto,自动匹配进程数 #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emerg #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #pid logs/nginx.pid; #指定nginx进程运行文件存放地址 events { worker_connections 1024; #最大连接数，默认为512 } http { include mime.types; #文件扩展名与文件类型映射表 default_type application/octet-stream; #默认文件类型，默认为text/plain #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #允许sendfile方式传输文件，默认为off #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #连接超时时间，默认为75s #gzip on; #启用Gizp压缩 #include /usr/local/nginx/conf/vhost/*; server { listen 80; server_name #监听地址; #charset koi8-r; #access_log logs/host.access.log main; location / { #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。 proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表 #设置禁止浏览器缓存，每次都从服务器请求 add_header Cache-Control no-cache; add_header Cache-Control private;#表明响应只能被单个用户缓存，不能作为共享缓存（即代理服务器不能缓存它）,可以缓存响应内容。响应只作为私有的缓存，不能在用户间共享。如果要求HTTP认证，响应会自动设置为private。 deny 127.0.0.1; #拒绝的ip allow 172.18.5.54; #允许的ip } location /material/ { proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表 #设置禁止浏览器缓存，每次都从服务器请求 add_header Cache-Control no-cache; add_header Cache-Control private;#表明响应只能被单个用户缓存，不能作为共享缓存（即代理服务器不能缓存它）,可以缓存响应内容。响应只作为私有的缓存，不能在用户间共享。如果要求HTTP认证，响应会自动设置为private。 } } server { listen 80; server_name #监听地址; #charset koi8-r; #access_log logs/host.access.log main; location / { #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。 proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表 #设置禁止浏览器缓存，每次都从服务器请求 add_header Cache-Control no-cache; add_header Cache-Control private;#表明响应只能被单个用户缓存，不能作为共享缓存（即代理服务器不能缓存它）,可以缓存响应内容。响应只作为私有的缓存，不能在用户间共享。如果要求HTTP认证，响应会自动设置为private。 deny 127.0.0.1; #拒绝的ip allow 172.18.5.54; #允许的ip } location /material/ { proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表 #设置禁止浏览器缓存，每次都从服务器请求 add_header Cache-Control no-cache; add_header Cache-Control private;#表明响应只能被单个用户缓存，不能作为共享缓存（即代理服务器不能缓存它）,可以缓存响应内容。响应只作为私有的缓存，不能在用户间共享。如果要求HTTP认证，响应会自动设置为private。 } } #server { # listen 80; # server_name xxx.com; # # #charset koi8-r; # # #access_log logs/host.access.log main; # # location / { # root html; # index index.html index.htm; # } # # #error_page 404 /404.html; # # # redirect server error pages to the static page /50x.html # # # error_page 500 502 503 504 /50x.html; # location = /50x.html { # root html; # } # # # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # # # #location ~ \.php$ { # # proxy_pass http://127.0.0.1; # #} # # # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # # # #location ~ \.php$ { # # root html; # # fastcgi_pass 127.0.0.1:9000; # # fastcgi_index index.php; # # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # # include fastcgi_params; # #} # # # deny access to .htaccess files, if Apache&apos;s document root # # concurs with nginx&apos;s one # # # #location ~ /\.ht { # # deny all; # #} #} # another virtual host using mix of IP-, name-, and port-based configuration # #server { # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / { # root html; # index index.html index.htm; # } #} # HTTPS server # #server { # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / { # root html; # index index.html index.htm; # } #} } NGINX之反向代理与负载均衡反向代理： 从图中，我们可以知道，对于浏览器来说，他会发一个http://www.a.com/uri请求到Nginx服务器，对于他来说，他认为数据就是从http://www.a.com/uri域中返回的，事实上，当http://www.a.com/uri到达Nginx服务器后，Nginx服务器会将其转发给http://www.b.com/uri,从http://www.b.com/uri域中取得数据并将其返回给浏览器，这个步骤浏览器是不知道的，也就是说，浏览器并不知道http://www.b.com/uri该域的存在，同理，http://www.b.com/uri所在的域（图中的Tomcat）也并不知道浏览器的存在，他也只对Nginx负责。Nginx的这么一个过程便称为反向代理。 那么，Nginx服务器是如何实现这一步的呢，事实上也很简单，只需要在location中做一下简单的配置即可，命令大概如下图所示：（配置完命令记得reload重新加载才能生效） 重点在于location处，这样的配置代表的是，所有来自浏览器的请求，在Nginx收到之后，都会代理到http://192.168.1.62:8080所在的地方 比如，我浏览器上发起http://192.168.1.61/a/index.html；Nginx收到之后，将会发出http:// 192.168.1.62:8080/a/index.html这么一个请求到所连接的服务器上，如上图的Tomcat。 接下来我们做这样一个假设，假如后端连接着几台。几十台服务器呢，这个时候Nginx也是做同样的代理吗，答案是肯定的。图示如下：那么，在这么多台服务器上，Nginx的转发又是基于怎样的策略呢？这个时候就涉及在负载均衡了，说白了就是，应该怎样的分发，才能做到资源的最大限度的利用？ 负载均衡策略 （ 我们这里假设三台服务器的IP地址分别为 http:// 192.168.1.62:8080 http:// 192.168.1.63:8080 http:// 192.168.1.64:8080 ） 这里我们把后台所有的服务器放入upstream中，并在代理中进行引用。 其他的配置备份与停机状态：server 192.168.1.64 backup;//备份，不参与转发，只有当所有服务器都挂掉时才参与转发； server 192.168.1.65 down;//临时停机维护，不参与任何转发，是关闭状态， down存在的意义在于，有时我们需要对服务器做临时停机更新维护，假如我们直接关闭服务器的话，那么对于Nginx来说，他还是会把请求发到该服务器上的，因为他并不知道服务器已关，而设置down后，Nginx则不会再发到该服务器上了，避免造成无用的请求浪费。 max_fails: 达到指定次数后认为服务器挂掉 fail_timeout：挂掉多久后再次测试是否已经挂掉 配置命令 server 192.168.1.66 max_fails=2 fail_timeout=60s;]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[奇迹男孩]]></title>
    <url>%2F2020%2F02%2F11%2F%E5%A5%87%E8%BF%B9%E7%94%B7%E5%AD%A9%2F</url>
    <content type="text"><![CDATA[零My mom always said…”if you don’t like where you are…just picture where you wanna be.”（by Auggie） 我妈妈常说，“如果你不喜欢这里，就想象一个你想去的地方。”(奥吉) 壹“When given the choice between being right or being kind , choose kind.”(Mr Brown’s September Precet) “如果要从正确和善良中，做出选择，请选择善良。”（布朗老师的九月箴言） 叁Because I’m your mom , it counts the most because I know you the most.(By Isabel) 正因为我是你妈妈说话才算数，因为我最了解你。（伊莎贝尔） 肆Because school sucks. And people change.So if you wanna be a normal kid,Auggie,the those are the rules.(By Via) 上学就是这么糟糕。人也是会变的。如果你想当普通小孩奥吉，这就是规则。（维娅） 伍While nothing justifies striking another student…I know goos friends are worth defending.(By Mr. Tushman) 虽然殴打同学的行为不能通融，但我知道友情是值得捍卫的。（图始曼先生） 六Maybe if we knew what other people were thinking, we’d know that no one’s ordinary.And we all deserve a standing ovation at least once in our lives.(By Auggie) 如果我们了解别人的想法，就会知道，没有人是普通的。每个人都值得大家站起来为他鼓掌一次。 柒Be kind,for everyone is fighting a hard battle.And if you really wanna see what people are… all you have to do ..is look.(By Auggie) 善良一点，因为每个人都在与人生苦战。如果你想真正了解他人，你只需要用心…去看。（奥吉）]]></content>
      <categories>
        <category>电影台词</category>
      </categories>
      <tags>
        <tag>电影台词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[圣经解惑]]></title>
    <url>%2F2019%2F12%2F14%2F%E5%9C%A3%E7%BB%8F%E8%A7%A3%E6%83%91%2F</url>
    <content type="text"><![CDATA[约拿的神迹路加福音11章29节中讲道“这世代是一个邪恶的世代。他们求看神迹，除了约拿的神迹外，再没有神迹给他们看了。约拿怎样为尼尼微人成了神迹，人子也要照这样为这世代的人成为神迹。” 约拿的神迹是什么？约拿的神迹就是悔改的神迹。尼尼微人因为悔改，而是神改变了要毁灭他们的计划。照样耶稣来世界上就是要让人悔改归向上帝，而脱离灭亡。尼尼微城的最终毁灭，是在约拿之后的几百年后，照样上帝也在宽容和等待世人的悔改。]]></content>
      <categories>
        <category>信仰</category>
      </categories>
      <tags>
        <tag>信仰</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[财务自由体系-三元策略]]></title>
    <url>%2F2019%2F12%2F12%2F%E8%B4%A2%E5%8A%A1%E8%87%AA%E7%94%B1%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[现金流现金流指通过薪资，实业，租金等获取长期的稳定的收入。 低风险投资低风险套利：将现金流进行低风险的证券，债券套利； 优质增值资产（房地产等）]]></content>
      <categories>
        <category>财务</category>
      </categories>
      <tags>
        <tag>财务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《天道》]]></title>
    <url>%2F2019%2F12%2F11%2F%E5%A4%A9%E9%81%93%2F</url>
    <content type="text"><![CDATA[壹“悟，悟道休言天命；修行勿取真经。一悲一喜一枯荣，哪个前生注定。袈裟本无清净，红尘不染性空，幽幽古刹千年钟，都是痴人说梦。” 贰“你是一块玉，但我不是匠人，我不过是一个略懂投机之道的混子。充其量挣几个打发凡夫俗子的铜板，你要求的是一种雄性文化的魂，我不能因为你没有说出来而假装不知道，接受你就接受了一种高度，我没有这个自信。” 叁“透视社会依次有三个层面，技术、制度、文化。小到一个人，大到一个国家一个民族任何一种命运归根到底都是那种文化属性的产物。强势文化造就强者，弱势文化造就弱者，这是规律。也可以理解为天道，不以人的意志为转移。” 肆“天下之道论到极致，百姓的柴米油盐，人生冷暖论到极致，男人女人的一个情字。” 伍“所谓真经，就是能够达到寂空涅槃的究竟法门。可悟不可修。修为成佛，在求。悟为明性，在知。修行以行制性。悟道以性施行。觉着由心生律；修者以律制心，不落恶果者有信无证，住因住果，住念住心，如是生灭。不昧因果者无住而住。无欲不欲。无戒无不戒。如是涅槃。” 陆但知行好事，莫要问前程。 柒“强盗的本质是破格获取，破格获取和直接获取是两个不同的概念。你们没有自信与强者在同一个规则下竞争，这只能说明你是弱者，因为弱势文化所追求的最高价值就是破格获取。所以，强盗的逻辑从本质上讲是懦弱的生存哲学，所以你不算好汉。” 捌神就是道，道就是规律，规律如来，容不得你思议，按规律办事的人就是神。 玖要想做点事，别把自己太当人，别把别人不当人。 拾“如果一个民族的文化从骨子里就是弱势文化属性，怎么可能去承载强势文化的政治、经济衡量一种文化属性不是看他的积淀的时间长短二十看他与客观规律的距离。五千年的文化是光辉，是灿烂。这个没有问题。但是，传统和习俗得过过客观规律的筛子。” 十一“女人是形式逻辑的典范，是辩证逻辑的障碍，我无意摧残女人，也不想被女人摧残。” 十二过于沉静的外表，恰恰诠释着他内心的沉重。 十三“强势文化就是遵循事物规律的文化，弱势文化就是依赖强者得道的期望破格获取的文化，也是期望救主的文化。强势文化在武学上被称为秘笈， 而弱势文化由于易学、 易懂、易用、成了流行品种。” 十四“你的生存状态不是病态，用佛教得话说是自性，无所挂碍，是自在。自在是什么？就是解脱。” 十五“红颜知己自古有之，这还得看男人是不是一杯好酒，自古又有几个男人能把自己酿到淡而又淡得名贵，这不是为之而可为的事，能混就混吧。” 十六“这就是圆融世故，不显山不露水，各得其所。可品行这个东西今天缺个角，明天裂个缝，也就离坍陷不远了。” 十七“这世上原来就没有什么神话。所谓的神话，不过是常人的思维所不易理解的平常事。” 十八“中国的传统文化是皇恩浩荡的文化，他的实用是以皇天在上为先决条件，中国为什么穷，穷就穷在幼稚的思维，穷在期望救主，期望救恩的文化上，这是一个渗透到民族骨子里的价值判断体系。太可怕了！” 十九“马克思主义得道理归根到底一句话，客观规律不以人的意志为转移，什么是客观规律，归根到底也是一句话：一切以时间，地点和条件为转移。” 二十“我们这个民族总是以有文化自居，却忘了问一句，是有什么文化，是真理真相的文化，还是弱势文化，是符合事物规律的文化还是违背事物规律的文化，归根到底都是那种文化属性的产物，不以人的意志为转移。” 二十壹女人与男人的对话方式只有两个，要么躺着，要么站着。所以，我总愿意把你想象成一个流浪街头的醉汉，想收留你，却不敢想象收留你的门槛有多高。你说过，给你扔块馒头就行，可你要的这块馒头太大了，我这个穷家养不活你。 二十贰“佛说，看山是山，看水是水。我只是依佛法如实观照，看摩登女郎实摩登女郎，看红颜知己实红颜知己。” 二十叁“当有人笑话耶稣是傻子的时候，其实谁都不傻，仅仅是两种价值观不兼容。” 二十肆“比如说文化产业。文学、影视是扒拉灵魂的艺术。如果文学影视的创作能破解更高思维空间的文化密码，那么他的功效就是启迪人的觉悟，震撼人的灵魂，这是众生所需，就是功德、市场、 名利、精神拯救的暴利与毒品麻醉完全等值。而且不必像贩毒那样耍花招没有心理成本和法律风险。” 二十伍“股票的暴利并不产生上产经营，而是产生于股票市场本身的投机性。他的运作动力是把你口袋里的钱装到我的口袋里去，他的规律是把大多数的肉填到极少数狼的嘴里。私募基金是从狼嘴里夹肉。这就要求你得比狼更黑更狠，但是心理成本也更高，而且又多了一重股市之外的风险。 所以，得适可而止。” 二十六“灵魂归宿感， 这是人性本能的需要，是人性你帮他找块干净的地方归宿灵魂。” 二十七有道无术，术尚可求，有术无道，止于术。神即道，道法自然,如来! 二十八从心理学角度分析，越是头脑简 单的人越需要点缀和填充，而头脑复杂的 人，则对简洁有着特殊的心理需求。 二十九更高的哲人独处着，不是他们享受孤独，而是在他们身边找不到同类！]]></content>
      <categories>
        <category>书摘</category>
      </categories>
      <tags>
        <tag>书摘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支付模块梳理]]></title>
    <url>%2F2019%2F12%2F11%2F%E6%94%AF%E4%BB%98%E6%A8%A1%E5%9D%97%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"><![CDATA[支付宝和微信1、支付宝app支付 2、支付宝二维码支付 3、跳转支付地址支付 4、支付宝WAP支付（H5） 5、微信app支付 6、微信二维码支付 7、微信公众号JSAPI支付 8、微信WAP支付（H5） 9、微信小程序支付 实现构造pay对象pay对象包含属性有：用户id、订单id(List)、支付金额、支付状态、支付类型（aliqr、aliurl、aliwap、aliapp）、支付订单的类型（具体由业务定义）、支付标题，即支付时显示的商品名称、商品信息、服务器异步通知页面路径（回调地址）、页面跳转同步通知页面路径 若是微信公众号支付，需加上openid，ip(微信支付所需的客户端ip),prepayId（用于发送模板消息） 入参支付将pay对象传入，组装业务参数，调取第三方sdk进行支付。 回调回调结果，支付成功，pay对象写入支付状态1并返回。 业务数据更新根据回调结果以及是否返回pay对象，验证支付成功，进行业务数据更新。主要是订单的记录,创建order并保存。业务数据更新成功后，pay对象写入支付状态2]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>微信、支付宝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《我喜欢辽阔的地方》-毕淑敏]]></title>
    <url>%2F2019%2F12%2F08%2F%E6%88%91%E5%96%9C%E6%AC%A2%E8%BE%BD%E9%98%94%E7%9A%84%E5%9C%B0%E6%96%B9%2F</url>
    <content type="text"><![CDATA[零我们那时只有十六七岁，虽说也感到轻微的不适，但都像否认有偷窃行为一样否认有高原反应。那还是一个以为否认就能挽救一切的年纪。 壹她会长大，跨过岁月的栅栏，向远方跑去。我知道她会跌很多跤，瘀很多伤，淌很多泪和汗，有时候也会滴血。。。她将欲火沐风九蒸九焙，从一颗娇嫩的青豆磨练成珍珠。她的目光将不复天真，但依然保持真诚。她的面庞光洁不再，但笑容依旧。她能一直不倦地跑下去，因为她17岁的时候，就已经坚守岗位和职责，就已经懂得了奉献和光荣。 叁病使我们血气方刚，不再原谅，不再敦厚，不再费厄泼赖。 肆也许是离死亡近了，看爱情就更纯正永恒。大的爱也如大的死一般，是宽广和柔软的，云雾似的包容天地。 伍我们不必像小的时候，总要把整碗面都吃光，才知道碗底下并没有卧个鸡蛋。我们以为是碗欺骗了我们，其实是缺少经验。 陆读书是精神的一次牙牙学语。 柒面对苍凉旷远的高原，俯冲而下乜视的鹰眼，散乱在山之巅的病态脏器和牧羊人颜面表皮层永恒的笑容，在那一瞬间，我明白了什么叫作生命。 捌身后，是熟悉的一切，尽管它有令人不悦不满以致腐朽发臭的地方，但我们曾长久地浸泡其中，习惯成自然了。即使是令人痛苦的体验，我们也已经承受并忍耐，熬过了。向前，一切是陌生和昏暗暧昧的。。。 玖回头是土，向前是金。 拾文化这个东西，像胃一样，换不掉的。 十一旅途就是这样，我们会在某个地方以出乎意料的方式遇到某个人，彼此一点儿都不了解，却说了太多的话。 十二日子是一天天地走，书要一页页地读。清风朗月水滴石穿，一年几年一辈子地读下去。书就像微波，从内向外震荡着我们的心，徐徐地加热，精神分子的结构就改变了，成熟了，书的效力最终凸显出来。 十三年龄是生命的坐标，懂得渐渐消失并欣然迎接老迈是一种成熟的光荣。 真实有时候简陋的可怕。 黑色如荒草蔓延滋生。]]></content>
      <categories>
        <category>书摘</category>
      </categories>
      <tags>
        <tag>书摘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《时间简史》]]></title>
    <url>%2F2019%2F12%2F08%2F%E6%97%B6%E9%97%B4%E7%AE%80%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[零我们看到的宇宙之所以如此，乃是因为我们的存在–人存原理 壹为何宇宙是我们看到的这种样子？答案很简单：如果它不是这样样子，我们就不会在这里。]]></content>
      <categories>
        <category>书摘</category>
      </categories>
      <tags>
        <tag>书摘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《且以优雅过一生:杨绛传》]]></title>
    <url>%2F2019%2F12%2F08%2F%E4%B8%94%E4%BB%A5%E4%BC%98%E9%9B%85%E8%BF%87%E4%B8%80%E7%94%9F-%E6%9D%A8%E7%BB%9B%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[零我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容。。。我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系。 壹我和谁都不争，和谁争我都不屑；我爱大自然，其次就是艺术；我双手烤着生命之火取暖；火萎了，我也准备走了。 叁人那么壮大，权位、生死、爱恨、名利却动摇它。权位、生死、爱恨、名利那么壮大，时间却消磨它。–时间最壮大吗？不，是“心”，当心空无一物，它便无边无涯。 肆时间是洪水猛兽，也是解开问题的答案。]]></content>
      <categories>
        <category>书摘</category>
      </categories>
      <tags>
        <tag>书摘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《围城》-钱钟书]]></title>
    <url>%2F2019%2F12%2F07%2F%E5%9B%B4%E5%9F%8E-%E9%92%B1%E9%92%9F%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[零围在城里的人想逃出来，城外的人想冲进去，对婚姻也罢，职业也罢，人生的愿望大都如此。 壹天生人是教他们孤独的，一个个该各归各，老死不相往来。身体里容不下的东西，或消化，或排泄，是个人的事；为什么心里容不下的情感，要找同伴来分摊？聚在一起，动不动自己冒犯人，或者人开罪自己，好像一只刺猬，只好保持着彼此间的距离，要亲密团结，不是你刺痛我的肉，就是我擦破你的皮。（引自第199页） 贰我们一天要想到不知多少人，亲戚、朋友、仇人，以及不相干的见过面的人。真正想一个人，记挂着他，希望跟他接近，这少得很。人事太忙了，不许我们全神贯注，无间断地怀念一个人。我们一生对于最亲爱的人的想念，加起来恐怕不到一点钟，此外不过是念头在他身上瞥过，想到而已。（引自第161页） 叁写信的时候总觉得这是慰情聊胜于无，比不上见面，到见了面，许多话倒将不出来，想还不如写信。见面有瘾的；最初，约着见一面就能使见面的前后几天都沾着光，变成好日子渐渐地恨不能天天见面了；到后来，恨不能刻刻见面了。写好信发出，他总担心这信像支火箭，到落地时，火已熄了，对方收到的只是一段枯炭。（引自第79页） 肆像咱们这种旅行，最实验得出一个人的品性。旅行是最劳顿，最麻烦，叫人本相毕现的时候。经过长期苦旅行而彼此不讨厌的人，才可以结交作朋友–且慢，你听我说–结婚以后的蜜月旅行是次序颠倒的，应该先同旅行一个月，一个月舟车仆仆以后，双方还没有彼此看破，彼此厌恶，还没有吵嘴翻脸，还要维持原来的婚约，这种夫妇保证不会离婚（引自第188页） 伍年龄是个自然历程里不能超越的事实，就像饮食男女，像死亡。有时，这种年辈意识比阶级意识更鲜明。随你政见、学说或趣味如何相同，年辈的老少总替你隐隐分了界限，仿佛瓷器上的裂纹，平时一点没有什么，一旦受着震动，这条裂纹先扩大成裂缝。（引自第248页） 陆忠厚老实人的恶毒，像饭菜里的沙砾或者出骨鱼片里未净的刺，会给人一种不期待的伤。（引自第5页） 柒心里仿佛黑牢里的禁锢者摸索着一根火柴，刚划亮，火柴就熄了，眼前没看清的一片又滑回黑暗里。譬如黑夜里两条船相迎擦过，一个在这条船上，瞥见对面船舱的灯光正是自己梦寐不忘的脸，没来得及叫唤，彼此早距离远了。这一刹那的接近，反见得睽隔的渺茫。 捌在旅行的时候，人生的地平线移近；坐汽车只几个钟点，而乘客仿佛下半世全在车里消磨的，只要坐定了，身心像得到归宿，一劳永逸地看书、看报、抽烟、吃东西、瞌睡，路程以外的事暂时等于身后身外的事。 玖对于丑人，细看是一种残忍–除非他是坏人，你要惩罚他。 拾天下只有两种人，譬如一串葡萄到手，一种人挑最好的先吃，另一种人把最好的留在最后吃。照例第一种人应该乐观，因为他每吃一颗都是吃剩的葡萄里最好的；第二种应该悲观，因为他每吃一颗都是吃剩的葡萄里最坏的。只不过事实上适得其反，缘故是第二种人还有希望，第一种人只有回忆。从恋爱到白头偕老，好比一串葡萄，总有最好的一颗，最好的只有一颗，留着希望，多么好？你希望的好葡萄在后面呢，我们是坏葡萄，别倒了你的胃口。 十一一个人的缺点正像猴子的尾巴，猴子蹲在地面的时候，尾巴是看不见的，直到他向树上爬，就把后部供大众瞻仰，可是这红臀长尾巴本来就有，并非地位爬高了的新标识。 十二我们在社会上一切说话全像戏院子的入场券，一边印着“过期作废”，可是那一边并不注明什么日期，随我们都的便可以提早或延迟。 十三这就是生离死别比百年团聚好的地方，它能使人不老。不但鬼不会长大，不见了好久的朋友，在我们心目里，还是当年的风采，尽管我们自己已经老了。 十四他个人的天地忽然从世人公共生活的天地里分出来，宛如与活人幽明隔绝的孤鬼。瞧着阳世的乐事，自己插不进，瞧着阳世的太阳，自己晒不到。 十五吃的饭并不能使他们不饿，只滋养栽培了饿，使饿在他们身体里长存，而他们不至于饿死了不再饿。]]></content>
      <categories>
        <category>书摘</category>
      </categories>
      <tags>
        <tag>书摘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成小程序]]></title>
    <url>%2F2019%2F05%2F18%2F%E9%9B%86%E6%88%90%E5%B0%8F%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[web.xml&lt;servlet&gt; &lt;servlet-name&gt;WeixinMiniServiceServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/weixinmini-context.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;WeixinMiniServiceServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/wm/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; src/main/resources增加weixinmimi-context.xml配置，添加对modulewxmini包的扫描 &lt;context:component-scan base-package=&quot;com.material.modulewxmini&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.web.bind.annotation.ControllerAdvice&quot; /&gt; &lt;/context:component-scan&gt; 添加过滤器WeixinMiniInterceptor weixin包里增加WxMiniVoucherService及WxMinigetInfo WeMiniVoucher类]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>系统集成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用命令]]></title>
    <url>%2F2019%2F05%2F04%2F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[项目拷贝1.项目右键 –&gt; properties –&gt; Web Project Settings –&gt; 修改Context root 2.pom.xml 3.工作空间中找到当前项目下.project文件 4.工作空间中找到当前项目，打开.settings文件夹，找到org.eclipse.wst.common.component文件 5..component hexo 记录1.hexo new ‘页面名称’ 2.hexo clean 清除 3.hexo g 4.hexo d 5.hexo server 本地起服务 git备份hexo源码1、cd 本地分支里12345678910111213141516171819202122232425git branch* hexo master git remote –v若什么都没有，则和上游已断联系，拉不了代码也推不了代码加关联git remote add origin https://github.com/ikangbow/ikangbow.github.iogit fetch origin再次检查远程仓库，显示对应的clone地址git remote –vorigin https://github.com/ikangbow/ikangbow.github.io (fetch)origin https://github.com/ikangbow/ikangbow.github.io (push)本地git 文件添加git add .本地git 提交git commit -m "备注" 2、进入H:\hexo\ikangbow.github.io目录右键 点击黄框最后一个按钮push 依次点击黄框的按钮即可提交最新代码到到远程仓库]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS]]></title>
    <url>%2F2019%2F03%2F03%2FJS%2F</url>
    <content type="text"><![CDATA[js委托事件概述什么叫事件委托呢？它还有一个名字叫事件代理，JavaScript高级程序设计上讲：事件委托就是利用事件冒泡，只指定一个事件处理程序，就可以管理某一类型的所有事件。举个栗子：有三个同事预计会在周一收到快递。为签收快递，有两种办法：一是三个人在公司门口等快递；二是委托给前台MM代为签收。现实当中，我们大都采用委托的方案（公司也不会容忍那么多员工站在门口就为了等快递）。前台MM收到快递后，她会判断收件人是谁，然后按照收件人的要求签收，甚至代为付款。这种方案还有一个优势，那就是即使公司里来了新员工（不管多少），前台MM也会在收到寄给新员工的快递后核实并代为签收。 这里其实还有2层意思的： 第一，现在委托前台的同事是可以代为签收的，即程序中的现有的dom节点是有事件的； 第二，新员工也是可以被前台MM代为签收的，即程序中新添加的dom节点也是有事件的。 为什么要用事件委托一般来说，dom需要有事件处理程序，我们都会直接给它设事件处理程序就好了，那如果是很多的dom需要添加事件处理呢？比如我们有100个li，每个li都有相同的click点击事件，可能我们会用for循环的方法，来遍历所有的li，然后给它们添加事件，那这么做会存在什么影响呢？ 在JavaScript中，添加到页面上的事件处理程序数量将直接关系到页面的整体运行性能，因为需要不断的与dom节点进行交互，访问dom的次数越多，引起浏览器重绘与重排的次数也就越多，就会延长整个页面的交互就绪时间，这就是为什么性能优化的主要思想之一就是减少DOM操作的原因；如果要用事件委托，就会将所有的操作放到js程序里面，与dom的操作就只需要交互一次，这样就能大大的减少与dom的交互次数，提高性能； 每个函数都是一个对象，是对象就会占用内存，对象越多，内存占用率就越大，自然性能就越差了（内存不够用，是硬伤，哈哈），比如上面的100个li，就要占用100个内存空间，如果是1000个，10000个呢，那只能说呵呵了，如果用事件委托，那么我们就可以只对它的父级（如果只有一个父级）这一个对象进行操作，这样我们就需要一个内存空间就够了，是不是省了很多，自然性能就会更好。 事件委托原理事件委托是利用事件的冒泡原理来实现的，何为事件冒泡呢？就是事件从最深的节点开始，然后逐步向上传播事件，举个例子：页面上有这么一个节点树，div&gt;ul&gt;li&gt;a;比如给最里面的a加一个click点击事件，那么这个事件就会一层一层的往外执行，执行顺序a&gt;li&gt;ul&gt;div，有这样一个机制，那么我们给最外面的div加点击事件，那么里面的ul，li，a做点击事件的时候，都会冒泡到最外层的div上，所以都会触发，这就是事件委托，委托它们父级代为执行事件。 实现给指定li 加上class属性 class=”liShow” $(&quot;.test&quot;).on(&quot;click&quot;,&quot;.liShow&quot;,function() { //获取到当前点击事件的属性id var thisId = $(this).attr(&apos;data-id&apos;); //执行事件 })]]></content>
      <categories>
        <category>JS</category>
      </categories>
      <tags>
        <tag>JS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云web应用部署]]></title>
    <url>%2F2019%2F02%2F24%2F%E9%98%BF%E9%87%8C%E4%BA%91web%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[阿里云服务器 阿里云服务器的注册及申请 添加安全组规则。安全组在云端提供类似虚拟防火墙功能，用于设置单个或多个 ECS 实例的网络访问控制，它是重要的安全隔离手段。在创建 ECS 实例时，必须选择一个安全组。您还可以添加安全组规则，对该安全组下的所有 ECS 实例的出方向和入方向进行网络控制。 若没有配置安全组规则，直接在本地ping服务器，结果ping不通，ssh也连不上。 默认安全组中的默认规则仅设置针对ICMP协议、SSH 22端口、RDP 3389端口、HTTP 80端口和HTTPS 443端口的入方向规则。网络类型不同，安全且规则不同。 安全组常用配置 例如：redis端口6379/mongo数据库端口27017/mysql端口3306/ 服务器环境部署 使用shell工具上传 JDK；安装svn，安装maven；新建项目文件夹上传tomcat、mongo、redis安装包。 配置环境变量 java环境变量 JAVA_HOME=/usr/local/jdk1.8.0_162 export JRE_HOME=/usr/local/jdk1.8.0_162/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$JAVA_HOME:$PATH:$MAVEN_HOME/bin export MAVEN_HOME=/usr/local/maven/apache-maven-3.5.4 redis配置 找到redis.conf文件将bind 127.0.0.1注释，改为bind 0.0.0.0这样就可以支持远程连接 找到requirepass 设置redis登录密码 mongo配置 找到mongo.conf配置auth先设置false port=27017 logpath=/airthink/mongodb-linux-x86_64-3.6.3/mongod.log pidfilepath=/airthink/mongodb-linux-x86_64-3.6.3/mongod.pid logappend=true fork=true maxConns=3000 dbpath=/airthink/mongodb-linux-x86_64-3.6.3/data auth=false ./mongodb-linux-x86_64-3.6.3/bin/mongo 进入mongo shell 创建超级管理员admin账户 use admin db.createUser( { user: &quot;admin&quot;, pwd: &quot;123456&quot;, roles: [ { role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; } ] } ) 现在启用auth 将配置文件中auth=false 改为 auth=true use admin db.auth(&apos;admin&apos;,&apos;123456&apos;);授权 创建数据库 db.createUser({user:&quot;用户名&quot;,pwd:&quot;密码&quot;,roles:[{role:&quot;readWrite&quot;,db:&quot;项目名&quot;}]}) 部署web项目在项目文件夹下使用svn命令svn checkout svn://路径(目录或文件的全路径) 首次checkout出项目后，运行自动发布脚本，删除开发配置文件，替换生产配置文件，使用mvn打包，将war包复制到tomcat的webapp下，重启tomcat。 问题Linux执行.sh文件，提示No such file or directory的问题的解决方法： 原因：在windows中写好shell脚本测试正常，但是上传到 Linux 上以脚本方式运行命令时提示No such file or directory错误，那么一般是文件格式是dos格式的缘故，改成unix 格式即可。一般有如下几种修改办法。 用vim打开该sh文件，输入：:set ff回车，显示fileformat=dos，重新设置下文件格式：:set ff=unix保存退出::wq再执行 免费ssh证书申请及配置参考https://help.aliyun.com/knowledge_detail/95496.html?spm=a2c4g.11186623.2.11.53674c07M5nwXN]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java异常及使用]]></title>
    <url>%2F2019%2F02%2F02%2FJava%E5%BC%82%E5%B8%B8%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Java异常Java异常类层次结构图java 异常是程序运行过程中出现的错误。Java把异常当作对象来处理，并定义一个基类java.lang.Throwable作为所有异常的超类。在Java API中定义了许多异常类,分为两大类，错误Error和异常Exception。其中异常类Exception又分为运行时异常(RuntimeException)和非运行时异常(非runtimeException)，也称之为不检查异常（Unchecked Exception）和检查异常（Checked Exception）。 Error与ExceptionError是程序无法处理的错误，比如OutOfMemoryError、ThreadDeath等。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。Exception是程序本身可以处理的异常，这种异常分两大类运行时异常和非运行时异常。程序中应当尽可能去处理这些异常。 运行时异常和非运行时异常1.运行时异常: 都是RuntimeException类及其子类异常： IndexOutOfBoundsException：索引越界异常 ArithmeticException：数学计算异常 NullPointerException：空指针异常 ArrayOutOfBoundsException：数组索引越界异常 ClassNotFoundException：类文件未找到异常 ClassCastException：造型异常（类型转换异常） 这些异常是不检查异常（Unchecked Exception），程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的。 2.非运行时异常:是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如： IOException：文件读写异常 FileNotFoundException：文件未找到异常 EOFException：读写文件尾异常 MalformedURLException：URL格式错误异常 SocketException：Socket异常 SQLException：SQL数据库异常 异常的捕获和处理Java异常的捕获和处理是一个不容易把握的事情，如果处理不当，不但会让程序代码的可读性大大降低， 而且导致系统性能低下，甚至引发一些难以发现的错误。 1.try、catch方式： try{ //（尝试运行的）程序代码 }catch(异常类型 异常的变量名){ //异常处理代码 }finally{ //异常发生，方法返回之前，总是要执行的代码 } try、catch、finally三个语句块应注意的问题第一、try、catch、finally三个语句块均不能单独使用，三者可以组成 try…catch…finally、try…catch、 try…finally三种结构，catch语句可以有一个或多个，finally语句最多一个。第二、try、catch、finally三个代码块中变量的作用域为代码块内部，分别独立而不能相互访问。如果要在三个块中都可以访问，则需要将变量定义到这些块的外面。第三、多个catch块时候，只会匹配其中一个异常类并执行catch块代码，而不会再执行别的catch块，并且匹配catch语句的顺序是由上到下。 2.抛异常给上一级方式： public static void demo() throws Exception{ //抛出一个检查异常 throw new Exception(&quot;方法demo中的Exception&quot;); } 上面的代码可以看到两个关键字，throw和throws关键字throw：是用于方法体内部，用来抛出一个Throwable类型的异常。throws：是用于方法体外部的方法声明部分，用来声明方法可能会抛出某些异常。仅当抛出了检查异常，该方法的调用者才必须处理或者重新抛出该异常。当方法的调用者无力处理该异常的时候，应该继续抛出。异常处理是为了程序的健壮性。异常能处理就处理，不能处理就抛出，最终没有处理的异常JVM会进行处理。对于一个应用系统来说，应该有自己的一套异常处理框架，这样当异常发生时，也能得到统一的处理风格，将优雅的异常信息反馈给用户。 Java异常使用在开发业务系统中，目前绝大多数采用MVC模式。考虑如下场景：系统提供一个API，用于修改用户信息，服务器端采用json数据交互.首先我们定义BaseException，用来表示业务逻辑受理失败，它仅表示我们处理业务的时候发现无法继续执行下去。 /** * 业务受理失败异常 */ public class BaseException extends RuntimeException{ public ServiceException() { super(); } public BaseException(String arg0, Throwable arg1) { super(arg0, arg1); } public BaseException(String arg0) { super(arg0); } public BaseException(Throwable arg0) { super(arg0); } } 接下来Controller层 @RequestMapping pulic ReturnStatus updateUser(User user){ userService.updateUser(user); ReturnStatus status = new ReturnStatus(ture,&quot;更新成功&quot;); status.setEntity(user) return status; } 关于上述Controller写法乍一看会有一些冗余，如果无法理解，请仔细研读MVC设计模式. 先不管service，我们来考虑下。 一个业务系统不可能不对用户提交的数据进行验证，验证包括两方面：有效性和合法性 有效性: 比如用户所在岗位,是否属于数据库有记录的岗位ID,如果不存在,无效. 合法性: 比如用户名只允许输入最多12个字符,用户提交了20个字符,不合法. 有效性检查，可以交给java的校验框架执行，比如JSR303。假设用户提交的数据经过验证都合法，还是有一些情况是不能调用修改逻辑的。 要修改的用户ID不存在. 用户被锁定,不允许修改. 乐观锁机制发现用户已经被被人修改过. 由于某种原因,我们的程序无法保存到数据库. 一些程序员错误的开发了代码,导致保存过程中出现异常,比如NPE. 对于前3种，我们认为是有效性检查失败，第4种属与我们无法处理的异常，第5种就是程序员bug。 现在的问题是，前三种情况我们如何通知用户呢? 1.在ccontroller 调用userService的checkUserExist()方法.2.在controller直接书写业务逻辑.3.在service响应一个状态码机制,比如1 2 3表示错误信息,0 表示没有任何错误. 显然前2种方法都不可取，因为MVC设计模式告诉我们,controller是用来接收页面参数，并且调用逻辑处理，最后组织页面响应的地方。我们不可以在controller进行逻辑处理，controller只应该负责用户API入口和响应的处理(如若不然,思考一下如果有一天service的代码打包成jar放到另一个平台，没有controller了,该怎么办？) 状态码机制是个不错的选择，可是如此一来，用户保存逻辑变了，比如增加一个情况，不允许修改已经离职的用户，那么我们还需要修改controller的代码，代码量增加，维护成本增高，并且还耦合了service，不符合MVC设计模式。 那么怎么办呢？现在我们来看下service代码如何编写 public void updateUser(User user){ User userOrig = userDao.getUserById(user.getUserById); if(null == userOrig){ throw new BaseException(&quot;用户不存在&quot;); } if(userOrig.isLocked()){ throw new BaseException(&quot;用户被锁定,不允许修改&quot;); } if(!user.getVersion.equals(userOrig.getVersion)){ throw new BaseException(&quot;用户已经被别人修改,请刷新&quot;); } //TODO保存用户数据... } 这样一来只要我们检查到不允许保存的项目，我们就可以直接throw 一个新的异常，异常机制会帮助我们中断代码执行。 接下来有2种选择: 在controller 使用try-catch进行处理. 直接把异常抛给上层框架统一处 第1种方式是不可取的，注意我们抛出的BaseException，它仅仅逻辑处理异常,并且我们的方法前面没有声明throws BaseException，这表示他是一个非受查异常.controller也没有关心会发生什么异常。 为什么不定义成受查异常呢？如果是一个受查异常，那么意味着controller必须要处理你的异常。并且如果有一天你的业务逻辑变了，可能多一种检查项，就需要增加一个异常，反之需要删除一个异常，那么你的方法签名也需要改变，controller也随之要改变,这又变成了紧耦合，这和用状态码123表示处理结果没有什么不同。 我们可以为每一种检查项定义一个异常吗？可以，但是那样显得太多余了。因为业务逻辑处理失败的时候，根据我们需求，我们只需要通知用户失败的原因(通常应该是一段字符串)，以及服务器受理失败的一个状态码(有时可能不需要状态码,这要看你的设计了)，这样这需要一个包含原因属性的异常即可满足我们需求。 最后我们决定这个异常继承自RuntimeException。并且包含一个接受一个错误原因的构造器，这样controller层也不需要知道异常，只要全局捕获到BaseException做统一的处理即可，这无论是在struct1,2时代，还是springMVC中，甚至servlet年代，都是极为容易的！ 异常不提供无参构造器，因为绝对不允许你抛出一个逻辑处理异常，但是不指明原因，想想看，你是必须要告诉用户为什么受理失败的！ 如此一来，我们只需要全局统一处理下 BaseException 就可以了，很好，spring为我们提供了ControllerAdvice机制，有关ControllerAdvice，可以查阅springMVC使用文档，下面是一个简单的示例： @ControllerAdvice(basePackages = {&quot;com.xxx.xxx.bussiness.xxx&quot;}) public class ModuleControllerAdvice{ public static final Logger LOGGER = LoggerFactory.getLogger(ModuleControllerAdvice.class); public static final Logger BASE_LOGGER = LoggerFactory.getLogger(BaseException.class); /** * 业务受理失败 */ @ResponseBody @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR) @ExceptionHandler(BaseException.class) private ReturnStatus handleServiceException(BaseException exception){ String message = &quot;业务受理失败,原因：&quot;+exception.getLocalizedMessage(); SERVICE_LOGGER.info(message); ReturnStatus status = new ReturnStatus(MErrorCode.e2001.code(), MErrorCode.e2001.message());//自定义枚举异常类 return status; } } 在这个时候，我们就可以很轻松的处理各种情况了. 注意一点，在这个类中，我们定义了2个log对象，分别指向 BaseException.class 和 ModuleControllerAdvice.class。并且处理 BaseException的时候使用了info级别的日志输出，这是很有用的。 首先,BaseException一定要和其他的代码错误分离,不应该混为一谈.其次,BaseException并不一定要记录日志,我们应该提供独立的log对象,方便开关.接下来你可以在修改用户的时候想客户端响应这样的JSON { code : e2001, message : &quot;用户不存在&quot; } 如此一来没有任何地方需要关心异常，或者业务逻辑校验失败的情况。用户也可以得到很友好的错误提示。如果你只需要一句概括，那么直接定义一个简单的异常，用于中断处理，并且与用户保持友好交互即可。如果不可能一句话描述清楚，并且包含附加信息，比如需要在日志或者数据库记录消息ID，此时可能专门针对这种重要/复杂业务创建独立异常。上述两种情况因为web系统，是用户发起请求之后需要等待程序给予响应结果的。 如果是后台作业，或者复杂业务需要追溯性。这种通常用流程判断语句控制，要用异常处理。我们认为这些流程判断一定在一个原子性处理中。并且检查到(不是遇到)的问题(不是异常)需要记录到用户可友好查看的日志。这种情况属于处理反馈，并不叫异常。 综上，笔者通常分为如下几类:逻辑异常，这类异常用于描述业务无法按照预期的情况处理下去，属于用户制造的意外。代码错误，这类异常用于描述开发的代码错误，例如NPE，ILLARG，都属于程序员制造的BUG。专有异常，多用于特定业务场景，用于描述指定作业出现意外情况无法预先处理。各类异常必须要有单独的日志记录，或者分级，分类可管理。有的时候仅仅想给三方运维看到逻辑异常。 注意 异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。上面这句话出自&lt;java编程思想&gt;，但是我们思考如下几点：业务逻辑检查，也是意外情况UnknownHostException，表示找不到这样的主机，这个异常和NoUserException有什么区别么？换言之，没有这样的主机是异常，没有这样的用户不是异常了么？所以一定要弄明白什么是用异常来控制逻辑，什么是定义程序异常。 异常处理效率很低 书中所示的例子，是在循环中大量使用try-catch进行检查，但是业务系统，用户发起请求的次数与该场景天壤地别。淘宝的11`11是个很好的反例。但是请你的系统上到这个级别再考虑这种问题。系统有千万并发，不可能还去考虑这些中规中矩的按部就班的方式，别忘了MVC本来就浪费很多资源，代码量增加很多。业务系统也存在很多巨量任务处理的情况。但是那些任务都是原子性的，现在MVC中的controller和service可不是原子性的，不然为什么要区分这么多层呢。如果那么在乎效率，考虑下重写Throwable的fillStackTrace方法。你要知道异常的开销大到底大在什么地方，fillStackTrace是一个native方法，会填充异常类内部的运行轨迹。 不要用异常进行业务逻辑处理 我们先来看一个例子: public void processMessage(Message&lt;String&gt; message){ try{ //处理消息验证 //处理消息解析 //处理消息入库 }catch(ValidateException e){ //验证失败 }catch(ParseException e){ //解析失败 }catch(PersistException e){ //入库失败 } } 上述代码就是典型的使用异常来处理业务逻辑。这种方式需要严重的禁止！上述代码最大的问题在于，我们如何利用异常来自动处理事务呢？然而这和我们的异常中断service没有什么冲突.也并不是一回事.我们提倡在 业务处理 的时候，如果发现无法处理直接抛出异常即可。而并不是在 逻辑处理 的时候，用异常来判断逻辑进行的状况。改正后的逻辑 public void processMessage(Message&lt;String&gt; message){ if(!message.isValud){ MessageLogService.log(&quot;消息验证失败&quot;+message.errors()); return; } ... //TODO ... }]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker系列]]></title>
    <url>%2F2019%2F01%2F16%2Fdocker%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"><![CDATA[CentOS Docker 安装Docker支持以下的CentOS版本： CentOS 7 (64-bit) CentOS 6.5 (64-bit) 或更高的版本 使用 yum 安装（CentOS 7下） 通过 uname -r 命令查看你当前的内核版本 [root@airthink ~]# uname -r 3.10.0-327.el7.x86_64 只需通过以下命令即可安装 Docker 软件： yum -y install docker-io 可使用以下命令，查看 Docker 是否安装成功 docker version Client: Version: 18.09.1 API version: 1.39 Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:35:01 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.1 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:06:30 2019 OS/Arch: linux/amd64 Experimental: false 若输出了 Docker 的版本号，则说明安装成功了，可通过以下命令启动 Docker 服务： service docker start 下载镜像docker search &quot;镜像名&quot;，搜索镜像，第一个字段为imagename（镜像名字），第四个字段为official，是否为官方镜像，个人也可以把自己制作的镜像放到docker hub上，找到自己想要下载的镜像名字就可以拉取了 docker pull &quot;镜像名&quot;，下载镜像 docker rmi &quot;镜像名&quot;，删除镜像 运行一个新的container docker run [-i -t -d -p -P -c] [--name]:在容器内运行一个应用程序 -t :在新容器内指定一个伪终端或终端 -i:允许你对容器内的标准输入进行交互 -d：以进程方式运行容器，让容器在后台运行 -p：设置端口 -P：将容器内部使用的网络端口映射到我们使用的主机，就是让我们访问我们使用的主机就等同于访问到容器内部 -c：command,后面接命令 --name container name：指定容器名字 docker ps -a 列出创建的所有容器 docker ps -s 列出正在运行的容器 docker start [container ID] docker exec -it [container ID] /bin/bash 即可进入一个已经在运行的容器 exit 可退出容器 docker stop [container ID] 停用容器 Docker 1.13版本以后，可以使用 docker containers prune 命令，删除孤立的容器。 docker container prune docker rmi -f image_ID//删除镜像 Dockerfile部分命令首先，每个指令的前缀必须大写 指令：FROM 功能描述：设置基础镜像 语法：FROM &lt; image&gt;[:&lt; tag&gt; | @&lt; digest&gt;] 提示：镜像都是从一个基础镜像（操作系统或其他镜像）生成，可以在一个Dockerfile中添加多条FROM指令，一次生成多个镜像 注意：如果忽略tag选项，会使用latest镜像 指令：MAINTAINER 功能描述：设置镜像作者 语法：MAINTAINER &lt; name&gt; 指令：RUN 功能描述： 语法：RUN &lt; command&gt; RUN [“executable”,”param1”,”param2”] 提示：RUN指令会生成容器，在容器中执行脚本，容器使用当前镜像，脚本指令完成后，Docker Daemon会将该容器提交为一个中间镜像，供后面的指令使用 补充：RUN指令第一种方式为shell方式，使用/bin/sh -c &lt; command&gt;运行脚本，可以在其中使用\将脚本分为多行 RUN指令第二种方式为exec方式，镜像中没有/bin/sh或者要使用其他shell时使用该方式，其不会调用shell命令 例子：RUN source $HOME/.bashrc;\ echo $HOME RUN [“/bin/bash”,”-c”,”echo hello”] RUN [“sh”,”-c”,”echo”,”$HOME”] 使用第二种方式调用shell读取环境变量 指令：CMD 功能描述：设置容器的启动命令 语法：CMD [“executable”,”param1”,”param2”] CMD [“param1”,”param2”] CMD &lt; command&gt; 提示：CMD第一种、第三种方式和RUN类似，第二种方式为ENTRYPOINT参数方式，为entrypoint提供参数列表 注意：Dockerfile中只能有一条CMD命令，如果写了多条则最后一条生效 指令：LABEL 功能描述：设置镜像的标签 延伸：镜像标签可以通过docker inspect查看 格式：LABEL &lt; key&gt;=&lt; value&gt; &lt; key&gt;=&lt; value&gt; … 提示：不同标签之间通过空格隔开 注意：每条指令都会生成一个镜像层，Docker中镜像最多只能有127层，如果超出Docker Daemon就会报错，如LABEL ..=.. &lt;假装这里有个换行&gt; LABEL ..=..合在一起用空格分隔就可以减少镜像层数量，同样，可以使用连接符\将脚本分为多行 镜像会继承基础镜像中的标签，如果存在同名标签则会覆盖 指令：EXPOSE 功能描述：设置镜像暴露端口，记录容器启动时监听哪些端口 语法：EXPOSE &lt; port&gt; &lt; port&gt; … 延伸：镜像暴露端口可以通过docker inspect查看 提示：容器启动时，Docker Daemon会扫描镜像中暴露的端口，如果加入-P参数，Docker Daemon会把镜像中所有暴露端口导出，并为每个暴露端口分配一个随机的主机端口（暴露端口是容器监听端口，主机端口为外部访问容器的端口） 注意：EXPOSE只设置暴露端口并不导出端口，只有启动容器时使用-P/-p才导出端口，这个时候才能通过外部访问容器提供的服务 指令：ENV 功能描述：设置镜像中的环境变量 语法：ENV &lt; key&gt;=&lt; value&gt;…|&lt; key&gt; &lt; value&gt; 注意：环境变量在整个编译周期都有效，第一种方式可设置多个环境变量，第二种方式只设置一个环境变量 提示：通过${变量名}或者 $变量名使用变量，使用方式${变量名}时可以用${变量名:-default} ${变量名:+cover}设定默认值或者覆盖值 ENV设置的变量值在整个编译过程中总是保持不变的 指令：ADD 功能描述：复制文件到镜像中 语法：ADD &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”] 注意：当路径中有空格时，需要使用第二种方式 当src为文件或目录时，Docker Daemon会从编译目录寻找这些文件或目录，而dest为镜像中的绝对路径或者相对于WORKDIR的路径 提示：src为目录时，复制目录中所有内容，包括文件系统的元数据，但不包括目录本身 src为压缩文件，并且压缩方式为gzip,bzip2或xz时，指令会将其解压为目录 如果src为文件，则复制文件和元数据 如果dest不存在，指令会自动创建dest和缺失的上级目录 指令：COPY 功能描述：复制文件到镜像中 语法：COPY &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”] 提示：指令逻辑和ADD十分相似，同样Docker Daemon会从编译目录寻找文件或目录，dest为镜像中的绝对路径或者相对于WORKDIR的路径 指令：ENTRYPOINT 功能描述：设置容器的入口程序 语法：ENTRYPOINT [“executable”,”param1”,”param2”] ENTRYPOINT command param1 param2（shell方式） 提示：入口程序是容器启动时执行的程序，docker run中最后的命令将作为参数传递给入口程序 入口程序有两种格式：exec、shell，其中shell使用/bin/sh -c运行入口程序，此时入口程序不能接收信号量 当Dockerfile有多条ENTRYPOINT时只有最后的ENTRYPOINT指令生效 如果使用脚本作为入口程序，需要保证脚本的最后一个程序能够接收信号量，可以在脚本最后使用exec或gosu启动传入脚本的命令 注意：通过shell方式启动入口程序时，会忽略CMD指令和docker run中的参数 为了保证容器能够接受docker stop发送的信号量，需要通过exec启动程序；如果没有加入exec命令，则在启动容器时容器会出现两个进程，并且使用docker stop命令容器无法正常退出（无法接受SIGTERM信号），超时后docker stop发送SIGKILL，强制停止容器 指令：VOLUME 功能描述：设置容器的挂载点 语法：VOLUME [“/data”] VOLUME /data1 /data2 提示：启动容器时，Docker Daemon会新建挂载点，并用镜像中的数据初始化挂载点，可以将主机目录或数据卷容器挂载到这些挂载点 指令：USER 功能描述：设置RUN CMD ENTRYPOINT的用户名或UID 语法：USER &lt; name&gt; 指令：WORKDIR 功能描述：设置RUN CMD ENTRYPOINT ADD COPY指令的工作目录 语法：WORKDIR &lt; Path&gt; 提示：如果工作目录不存在，则Docker Daemon会自动创建 Dockerfile中多个地方都可以调用WORKDIR，如果后面跟的是相对位置，则会跟在上条WORKDIR指定路径后（如WORKDIR /A WORKDIR B WORKDIR C，最终路径为/A/B/C） 指令：ARG 功能描述：设置编译变量 语法：ARG &lt; name&gt;[=&lt; defaultValue&gt;] 注意：ARG从定义它的地方开始生效而不是调用的地方，在ARG之前调用编译变量总为空，在编译镜像时，可以通过docker build –build-arg &lt; var&gt;=&lt; value&gt;设置变量，如果var没有通过ARG定义则Daemon会报错 可以使用ENV或ARG设置RUN使用的变量，如果同名则ENV定义的值会覆盖ARG定义的值，与ENV不同，ARG的变量值在编译过程中是可变的，会对比使用编译缓存造成影响（ARG值不同则编译过程也不同） 例子：ARG CONT_IMAG_VER &lt;换行&gt; RUN echo $CONT_IMG_VER ARG CONT_IMAG_VER &lt;换行&gt; RUN echo hello 当编译时给ARG变量赋值hello，则两个Dockerfile可以使用相同的中间镜像，如果不为hello，则不能使用同一个中间镜 示例：Dockerfile以阿里中间件大赛给的debian-jdk8镜像为例，Dockerfile文件如下： FROM debian:stretch ARG DEBIAN_FRONTEND=noninteractive ARG JAVA_VERSION=8 ARG JAVA_UPDATE=172 ARG JAVA_BUILD=11 ARG JAVA_PACKAGE=jdk ARG JAVA_HASH=a58eab1ec242421181065cdc37240b08 ENV LANG C.UTF-8 ENV JAVA_HOME=/opt/jdk ENV PATH=${PATH}:${JAVA_HOME}/bin RUN set -ex \ &amp;&amp; apt-get update \ &amp;&amp; apt-get -y install ca-certificates wget unzip \ &amp;&amp; wget -q --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; \ -O /tmp/java.tar.gz \ http://download.oracle.com/otn-pub/java/jdk/${JAVA_VERSION}u${JAVA_UPDATE}-b${JAVA_BUILD}/${JAVA_HASH}/${JAVA_PACKAGE}-${JAVA_VERSION}u${JAVA_UPDATE}-linux-x64.tar.gz \ &amp;&amp; CHECKSUM=$(wget -q -O - https://www.oracle.com/webfolder/s/digest/${JAVA_VERSION}u${JAVA_UPDATE}checksum.html | grep -E &quot;${JAVA_PACKAGE}-${JAVA_VERSION}u${JAVA_UPDATE}-linux-x64\.tar\.gz&quot; | grep -Eo &apos;(sha256: )[^&lt;]+&apos; | cut -d: -f2 | xargs) \ &amp;&amp; echo &quot;${CHECKSUM} /tmp/java.tar.gz&quot; &gt; /tmp/java.tar.gz.sha256 \ &amp;&amp; sha256sum -c /tmp/java.tar.gz.sha256 \ &amp;&amp; mkdir ${JAVA_HOME} \ &amp;&amp; tar -xzf /tmp/java.tar.gz -C ${JAVA_HOME} --strip-components=1 \ &amp;&amp; wget -q --header &quot;Cookie: oraclelicense=accept-securebackup-cookie;&quot; \ -O /tmp/jce_policy.zip \ http://download.oracle.com/otn-pub/java/jce/${JAVA_VERSION}/jce_policy-${JAVA_VERSION}.zip \ &amp;&amp; unzip -jo -d ${JAVA_HOME}/jre/lib/security /tmp/jce_policy.zip \ &amp;&amp; rm -rf ${JAVA_HOME}/jar/lib/security/README.txt \ /var/lib/apt/lists/* \ /tmp/* \ /root/.wget-hsts 体验一下如何用Dockerfile打包一个镜像，新建一个空目录，假设就是~/debian-jdk8吧，cd进这个目录，新建一个Dockerfile，然后把上面的内容copy进去，然后执行下面的命令： docker build -t debian-jdk8:v1.0 .其中-t debian-jdk8:v1.0表示打包的镜像名为debian-jdk，tag为v1.0（前面说过，tag是可以任意命名的，不一定要是这种格式），注意命令的最后有一个.，这个表示打包的上下文（其实就是Dockerfile所在目录）是在当前目录，然后目录下的Dockerfile就会被编译执行。执行完毕后运行docker images就会发现多了一个debian-jdk8镜像。下面来解释一下Dockerfile的结构，那些字母全部大写的每行第一个单词都是Dockerfile的指令，可以看出这个Dockefile中包括的指令有FROM、ARG、ENV、RUNFROM：FROM debian:stretch表示以debian:stretch作为基础镜像进行构建RUN 可以看出RUN后面跟的其实就是一些shell命令，通过&amp;&amp;将这些脚本连接在了一行执行，这么做的原因是为了减少镜像的层数，每多一行RUN都会给镜像增加一层，所以这里选择将所有命令联结在一起执行以减少层数ARG 特地将这个指令放在RUN之后讲解，这个指令可以进行一些宏定义，比如我定义ENV JAVAHOME=/opt/jdk，之后RUN后面的shell命令中的${JAVAHOME}都会被/opt/jdk代替ENV 可以看出这个指令的作用是在shell中设置一些环境变量（其实就是export） Docker部署web程序为了搭建 Java Web 运行环境，我们需要安装 JDK 、数据库、redis与 Tomcat，首先要再宿主机上传JDK 、数据库、redis与 Tomcat压缩包，以/mnt/software/为例。下面的过程均在容器内部进行。我们不妨选择/opt/目录作为安装目录，首先需要通过cd /opt/命令进入该目录。 首先，解压程序包 tar -zxf /mnt/software/jdk-7u67-linux-x64.tar.gz tar -zxf /mnt/software/apache-tomcat-7.0.55.tar.gz tar -zxf /mnt/software/redis tar -zxf /mnt/software/mongo 设置环境变量 首先，编辑.bashrc文件 vi ~/.bashrc 然后，在该文件末尾添加如下配置： export JAVA_HOME=/opt/jdk export PATH=$PATH:$JAVA_HOME 最后，需要使用source命令，让环境变量生效： source ~/.bashrc 编写运行脚本 vi /root/run.sh 然后，编辑脚本内容如下： source ~/.bashrc sh /opt/tomcat/bin/catalina.sh run 注意：这里必须先加载环境变量，然后使用 Tomcat 的运行脚本来启动 Tomcat 服务。 最后，为运行脚本添加执行权限： chmod u+x /root/run.sh 退出容器当以上步骤全部完成后，可使用exit命令，退出容器。 随后，可使用如下命令查看正在运行的容器： docker ps//未输出 docker ps -a//输出 输出如下内容： CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 57c312bbaad1 docker.cn/docker/centos:centos6 &quot;/bin/bash&quot; 27 minutes ago Exited (0) 19 seconds ago naughty_goldstine 记住以上CONTAINER ID（容器 ID），随后我们将通过该容器，创建一个可运行 Java Web 的镜像。 创建 Java Web 镜像使用以下命令，根据某个“容器 ID”来创建一个新的“镜像”： docker commit 57c312bbaad1 ikangbow/javaweb:0.1 该容器的 ID 是“57c312bbaad1”，所创建的镜像名是“ikangbow/javaweb:0.1”，随后可使用镜像来启动 Java Web 容器。 启动 Java Web 容器首先使用docker images命令，查看当前所有的镜像： REPOSITORY TAG IMAGE ID CREATED SIZE ikangbow/javaweb 0.1 95b5f6b09ca6 6 hours ago 1.51GB centos latest 1e1148e4cc2c 5 weeks ago 202MB 可见，此时已经看到了最新创建的镜像“ikangbow/javaweb:0.1”，其镜像 ID 是“95b5f6b09ca6”。正如上面所描述的那样，我们可以通过“镜像名”或“镜像 ID”来启动容器，与上次启动容器不同的是，我们现在不再进入容器的命令行，而是直接启动容器内部的 Tomcat 服务。此时，需要使用以下命令： docker run -d -p 58080:8080 --name javaweb ikangbow/javaweb:0.1 /root/run.sh 稍作解释： -d：表示以“守护模式”执行/root/run.sh脚本，此时 Tomcat 控制台不会出现在输出终端上。 -p：表示宿主机与容器的端口映射，此时将容器内部的 8080 端口映射为宿主机的 58080 端口，这样就向外界暴露了 58080 端口，可通过 Docker 网桥来访问容器内部的 8080 端口了。 –name：表示容器名称，用一个有意义的名称命名即可。 关于 Docker 网桥的内容，需要补充说明一下。实际上 Docker 在宿主机与容器之间，搭建了一座网络通信的桥梁，我们可通过宿主机 IP 地址与端口号来映射容器内部的 IP 地址与端口号， 在一系列参数后面的是“镜像名”或“镜像 ID”，怎么方便就怎么来。最后是“初始命令”，它是上面编写的运行脚本，里面封装了加载环境变量并启动 Tomcat 服务的命令。 当运行以上命令后，会立即输出一长串“容器 ID”，我们可通过docker ps命令来查看当前正在运行的容器。 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 82f47923f926 ikangbow/javaweb:0.1 &quot;/root/run.sh&quot; 4 seconds ago Up 3 seconds 0.0.0.0:58080-&gt;8080/tcp javaweb 此处实测有问题，需要继续研究。 附常用命令： 删除容器实例 寻找已经停止运行的container docker ps -a docker rm 容器id 删除实例 docker ps -a 查看实例已经删除 docker rm $(docker ps -a -q) 删除所有container 2.删除镜像 停止所有的container，这样才能够删除其中的images： docker stop $(docker ps -a -q) docker rmi &lt;image id&gt; docker images 查看所有镜像 docker rmi 镜像id 删除镜像 docker images 查看镜像 发现已经删除 实际应用docker stop application//停止应用 docker rm application//删除镜像 docker run -i -t -d -p 88:8080 -v /badou/badou/badou1.war:/usr/local/tomcat/webapps/badou.war -v /badou/conf/apiclient_cert.p12:/usr/local/tomcat/apiclient_cert.p12 -v /badou/badou/index.html:/usr/local/tomcat/webapps/ROOT/index.html --name=application andylaun/tomcat:v5 -v 挂载文件目录 docker运行nginxwhich nginx 查看nginx安装在哪个目录 docker网络docker具有隔离性,网络也是个隔离性的一部分，linux使用了命名空间来进行资源的隔离,比如pid namespace就是用来隔离进程的,mount namespace是用来隔离文件系统的,network namespace 是用来隔离网络的.每一个network namespace都提供了一个独立的网络环境,包括网卡路由iptable规则等等,都是与以其它的network space隔离的 docker容器在默认情况下,一般会分配一个独立的network-namespace,也就是网络类型中的Bridge模式，在使用Bridge时就涉及到了一个问题,既然它有独立的namesapce,这就需要一种技术使容器内的端口可以在主机上访问到,这种技术就是端口映射,docker可以指定你想把容器内的某一个端口可以在容器所在主机上的某一个端口它俩之间做一个映射,当你在访问主机上的端口时,其实就是访问容器里面的端口。 docker部署web 在宿主机系统下载所需要的的jdk版本gz文件 docker cp gz文件 容器ID:/root 复制gz文件到容器中的root的用户目录下 docker cp jdk-8u121-linux-x64.tar.gz ea49f55dde3d :/root 进入运行中的容器 docker exec -it ea49f55dde3d /bin/bash 在容器中解压gz文件 tar -zxvf jdk-8u121-linux-x64.tar.gz 建立容器系统的JAVA_HOME目录 mkdir /usr/lib/jvm 移动jdk目录到JAVA_HOME目录 mv ~/jdk1.8.0_121 /usr/lib/jvm 安装vim（可选） apt-get update apt-get install vim 设置环境变量 vim ~/.bashrc export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_121 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH 使得环境变量马上生效 source ~/.bashrc 验证JDK是否安装成功 java -version 安装tomcat 查找tomcat信息 docker search tomcat 下载官方的镜像Starts最高的那个 docker pull docker.io/tomcat 查看docker镜像 docker images 后台运行tomcat镜像 docker run -d –name tomcat -p 8081:8080 hub.c.163.com/library/tomcat 若端口被占用，可以指定容器和主机的映射端口，前者是外围访问端口，后者是容器内部端口。-d参数：容器会在后台运行并不会把输出的结果打印到宿主机上面。使用 -d 参数启动后会返回一个唯一的 id。 部署web项目 把宿主机的war包丢到docker容器tomcat/webapps下： docker cp lsz.war ab6bce2c5826:/usr/local/tomcat/webapps 进入docker容器中 docker exec -it ab6bce2c5826 /bin/bash 查看webapps中的web项目 退出docker容器Ctrl+p+q 重新运行tomcat容器docker restart ab6bce2c5826 停止tomcat容器docker stop ab6bce2c5826]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http基础]]></title>
    <url>%2F2019%2F01%2F07%2Fhttp%2F</url>
    <content type="text"><![CDATA[HTTP 简介HTTP协议（HyperText Transfer Protocol，超文本传输协议）是因特网上应用最为广泛的一种网络传输协议，所有的WWW文件都必须遵守这个标准。 HTTP是一个基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。 工作原理HTTP协议工作于客户端-服务端架构上。浏览器作为HTTP客户端通过URL向HTTP服务端即WEB服务器发送所有请求。 Web服务器有：Apache服务器，IIS服务器（Internet Information Services）等。 Web服务器根据接收到的请求后，向客户端发送响应信息。 HTTP默认端口号为80，但是你也可以改为8080或者其他端口。 HTTP三点注意事项： 1.HTTP是无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 2.HTTP是媒体独立的：这意味着，只要客户端和服务器知道如何处理的数据内容，任何类型的数据都可以通过HTTP发送。客户端以及服务器指定使用适合的MIME-type内容类型。 3.HTTP是无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 HTTP 消息结构HTTP是基于客户端/服务端（C/S）的架构模型，通过一个可靠的链接来交换信息，是一个无状态的请求/响应协议。 一个HTTP”客户端”是一个应用程序（Web浏览器或其他任何客户端），通过连接到服务器达到向服务器发送一个或多个HTTP的请求的目的。 一个HTTP”服务器”同样也是一个应用程序（通常是一个Web服务，如Apache Web服务器或IIS服务器等），通过接收客户端的请求并向客户端发送HTTP响应数据。 HTTP使用统一资源标识符（Uniform Resource Identifiers, URI）来传输数据和建立连接。 一旦建立连接后，数据消息就通过类似Internet邮件所使用的格式[RFC5322]和多用途Internet邮件扩展（MIME）[RFC2045]来传送。 实例下面实例是一点典型的使用GET来传递数据的实例： 客户端请求： GET /hello.txt HTTP/1.1User-Agent: curl/7.16.3 libcurl/7.16.3 OpenSSL/0.9.7l zlib/1.2.3Host: www.example.comAccept-Language: en, mi 服务端响应: HTTP/1.1 200 OKDate: Mon, 27 Jul 2009 12:28:53 GMTServer: ApacheLast-Modified: Wed, 22 Jul 2009 19:15:56 GMTETag: “34aa387-d-1568eb00”Accept-Ranges: bytesContent-Length: 51Vary: Accept-EncodingContent-Type: text/plain HTTP请求方法根据HTTP标准，HTTP请求可以使用多种请求方法。 HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。 HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。 GET: 请求指定的页面信息，并返回实体主体。 HEAD: 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 POST: 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT: 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE: 请求服务器删除指定的页面。 CONNECT: HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS: 允许客户端查看服务器的性能。 TRACE: 回显服务器收到的请求，主要用于测试或诊断。 HTTP 响应头信息 Allow: 服务器支持哪些请求方法（如GET、POST等）。 Content-Encoding: 文档的编码（Encode）方法。只有在解码之后才可以得到Content-Type头指定的内容类型。利用gzip压缩文档能够显著地减少HTML文档 的下载时间。Java的GZIPOutputStream可以很方便地进行gzip压缩，但只有Unix上的Netscape和Windows上的IE 4、IE 5才支持它。因此，Servlet应该通过查看Accept-Encoding头（即request.getHeader(“Accept- Encoding”)）检查浏览器是否支持gzip，为支持gzip的浏览器返回经gzip压缩的HTML页面，为其他浏览器返回普通页面。 Content-Length: 表示内容长度。只有当浏览器使用持久HTTP连接时才需要这个数据。如果你想要利用持久连接的优势，可以把输出文档写入 ByteArrayOutputStream，完成后查看其大小，然后把该值放入Content-Length头，最后通过 byteArrayStream.writeTo(response.getOutputStream()发送内容。 Content-Type: 表示后面的文档属于什么MIME类型。Servlet默认为text/plain，但通常需要显式地指定为text/html。由于经常要设置 Content-Type，因此HttpServletResponse提供了一个专用的方法setContentType。 Date: 当前的GMT时间。你可以用setDateHeader来设置这个头以避免转换时间格式的麻烦。 Expires: 应该在什么时候认为文档已经过期，从而不再缓存它？ Last-Modified: 文档的最后改动时间。客户可以通过If-Modified-Since请求头提供一个日期，该请求将被视为一个条件GET，只有改动时间迟于指定时间的文 档才会返回，否则返回一个304（Not Modified）状态。Last-Modified也可用setDateHeader方法来设置。 Location: 表示客户应当到哪里去提取文档。Location通常不是直接设置的，而是通过HttpServletResponse的sendRedirect方法，该方法同时设置状态代码为302。 Refresh: 表示浏览器应该在多少时间之后刷新文档，以秒计。除了刷新当前文档之外，你还可以通过setHeader(“Refresh”, “5; URL=http://host/path&quot;)让浏览器读取指定的页面。 注意这种功能通常是通过设置HTML页面HEAD区的＜METAHTTP-EQUIV=”Refresh” CONTENT=”5;URL=http://host/path&quot;＞实现，这是因为，自动刷新或重定向对于那些不能使用CGI或Servlet的HTML编写者十分重要。但是，对于Servlet来说，直接设置Refresh头更加方便。注意Refresh的意义是&quot;N秒之后刷新本页面或访问指定页面&quot;，而不是&quot;每隔N秒刷新本页面或访问指定页面&quot;。因此，连续刷新要求每次都发送一 个Refresh头，而发送204状态代码则可以阻止浏览器继续刷新，不管是使用Refresh头还是＜META HTTP-EQUIV=”Refresh” …＞。Refresh头不属于HTTP 1.1正式规范的一部分，而是一个扩展，但Netscape和IE都支持它 Server: 服务器名字。Servlet一般不设置这个值，而是由Web服务器自己设置。 Set-Cookie: 设置和页面关联的Cookie。Servlet不应使用response.setHeader(“Set-Cookie”, …)，而是应使用HttpServletResponse提供的专用方法addCookie。参见下文有关Cookie设置的讨论。 WWW-Authenticate: 客户应该在Authorization头中提供什么类型的授权信息？在包含401（Unauthorized）状态行的应答中这个头是必需的。例 如，response.setHeader(“WWW-Authenticate”, “BASIC realm=＼”executives＼””)。 注意Servlet一般不进行这方面的处理，而是让Web服务器的专门机制来控制受密码保护页面的访问（例如.htaccess）。 HTTP状态码当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 HTTP状态码的英文为HTTP Status Code。 HTTP状态码分类 200 OK 请求成功。一般用于GET与POST请求 401 Unauthorized 请求要求用户的身份认证 500 Internal Server Error 服务器内部错误，无法完成请求 404 Not Found 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置”您所请求的资源无法找到”的个性页面 502 Bad Gateway 作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 HTTP content-typeContent-Type，内容类型，一般是指网页中存在的Content-Type，用于定义网络文件的类型和网页的编码，决定浏览器将以什么形式、什么编码读取这个文件]]></content>
      <categories>
        <category>https</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[final和static的作用]]></title>
    <url>%2F2019%2F01%2F06%2Ffinal%E4%BF%AE%E9%A5%B0%E7%AC%A6%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[final关键字的作用 被final修饰的类不可以被继承 被final修饰的方法不可以被重写 被final修饰的变量不可被改变 重点 被fianl修饰不可变的是变量的引用，而不是引用指向的内容，引用指向的内容是可以改变的。 被fina修饰的常量，在编译阶段会存入调用类的常量池中。 static关键字的作用 被static修饰的变量属于类变量，可以通过 类名.变量名 直接引用，而不需要new出一个类。 被static修饰的方法属于类方法，可以通过 类名.变量名 直接引用，而不需要new出一个类。 被static修饰的变量和方法属于类的静态资源，是类实例之间共享的。JDK把不同的静态资源放在了不同的类中而不把所有的静态资源放在一个类里面是因为： 不同的类有自己的静态资源，这可以实现静态资源分类。比如和数学相关的静态资源放在java.lang.Math中，和日历相关的静态资源放在java.util.Calendar中，这样比较清晰 避免重名。不同的类之间有重名的静态变量名，静态方法名也是很正常。 避免静态资源类无限膨胀。 问题静态方法能不能引用非静态资源？静态方法里面能不能引用静态资源？非静态方法里面能不能引用静态资源？比如就以这段代码为例，是否有错？ public class A{ private int i = 1; public static void main(String[] args){ i = 1;//错误 } } 静态资源属于类，但是是独立于类存在的。从JVM类的加载机制的角度讲，静态资源是类初始化的时候加载的，而非静态资源是类new的时候加载的。类的初始化早于类的new,比如Class.forName(“xxx”),就是初始化了一个类，但是并没有new它，只是加载这个类的静态资源。所以对于静态资源来说，它是不可能知道一个类中有哪些非静态资源的；但是对于非静态资源来说就不一样了，它是new出来之后产生的，因此属于类的它都认识。所以结论是： 静态方法不能引用非静态资源。非静态资源在new对象的时候才会产生，晚于一初始化就存在的静态资源。 静态方法里面可以引用静态资源。都是类初始化的时候加载的。 非静态方法可以引用静态资源。非静态方法是new之后产生的，静态资源是类一初始化就存在的。 静态块静态块是static的重要应用之一。主要用于初始化一个类的时候做操作用的，和静态变量，静态方法一样，静态块里面的代码只执行一次，且只在类初始化的时候执行。 public class A{ private static int a = B(); static{ System.out.println(&quot;Enter A.static block&quot;); } public static void main(String[] args){ new A(); } public static int B(){ System.out.println(&quot;Enter A.B()&quot;); return 1; } } 打印结果： Enter A.B() Enter A.static block 结论： 静态资源的加载顺序是严格安装静态资源的定义顺序加载的。 public class A{ static { c = 3; System.out.println(c);//Cannot reference a field before it is defined } private static int c; } 结论： 静态代码块对于定义在它之后的静态变量，可以赋值，但是不能访问。 public class A{ static { System.out.println(&quot;A.static block&quot;); } public A() { System.out.println(&quot;A.constructor()&quot;); } } public class B extends A{ static { System.out.println(&quot;B.static block&quot;); } public B() { System.out.println(&quot;B.constructor()&quot;); } public static void main(String[] args) { new B(); new B(); } } 打印结果： A.static block B.static block A.constructor() B.constructor() A.constructor() B.constructor() 结论： 静态代码块是严格按照父类静态代码块》子类静态代码块的顺序加载的，且只加载一次。 项目中用到的项目中的角色类型通常用static final修饰 public static final String TYPE_A = &quot;a&quot;; public static final String TYPE_B = &quot;b&quot;; public:使接口的实现类可以使用这个常量 static:static修饰的表示是属于类的，随着类的加载而存在。如果是非static的，就表示属于对象的，只有建立对象时才有它，而接口是不能建立对象的，所以接口的常量必须定义为satic final:fina修饰保证接口定义的常量不能被实现类去修改，如果没有final的话，任由子类随意去修改的话，接口建立这个常量就没有意义了。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优雅的写代码]]></title>
    <url>%2F2018%2F12%2F19%2F%E4%BC%98%E9%9B%85%E7%9A%84%E5%86%99%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[代码优化的目标1.减小代码的体积。 2.提高代码运行效率 代码规范 所有文件的开头都要有Java文档的注释（/* /） 常量应该全部大写，单词之间由下划线分隔(例如，MAX_WORK_HOURS) 数组标识：要用”int[] packets”，而不是”int packets[]”，后一种永远也不要用 不要在循环中构造和释放对象 不要在循环中频繁查询数据库 方法要通盘考虑，尽量做到复用。（多想） 代码优化细节尽量重用对象特别是String对象的使用，出现字符串连接时应该使用StringBuilder/StringBuffer代替。由于Java虚拟机不仅要花时间生成对象，以后可能还需要花时间对这些对象进行垃圾回收和处理，因此，生成过多的对象将会给程序的性能带来很大的影响。 string类是final类，不能被继承，并且它的成员方法都默认为final方法。在java中，被final修饰的类是不允许被继承的，并且该类中的成员方法都默认为final方法。在早期的JVM实现版本中，被final修饰的方法会被转为内嵌调用以提升执行效率。从Java SE5/6开始，就渐渐摒弃这种方式了。因此在现在版本中并不需要考虑用final去提升方法调用效率。只有在确定不想让该方法被覆盖时，才将方法设置为final. String类其实是通过char数组来保存字符串的。所以无论是substring、concat、replace操作都不是在原有的字符串上进行的，而是重新生成了一个新的字符串对象。所以结论是：对String对象的任何改变都不影响到原对象，相关的任何change操作都会生成新的对象。 通过new关键字来生成对象是在堆区进行的，所以通过new来创建对象，创建出的一定是不同的对象，即使字符串的内容是相同的。 在class文件中有一部分来存储编译期间生成的字面常量以及符合的引用，这部分叫做class文件常量池，在运行期间对应着方法区的运行时常量池。下面代码 str1和str3都存储在常量池中，只存了一份。 public class Main { public static void main(String[] args) { String str1 = &quot;hello world&quot;; String str2 = new String(&quot;hello world&quot;); String str3 = &quot;hello world&quot;; String str4 = new String(&quot;hello world&quot;); System.out.println(str1==str2); System.out.println(str1==str3); System.out.println(str2==str4); } } 输出结果为false true false 既然已经有了String类，那为什么还需要StringBuilder、StringBuffer类呢 public class Main { public static void main(String[] args) { String string = &quot;&quot;; for(int i=0;i&lt;10000;i++){ string += &quot;hello&quot;; } } } 这句 string += “hello”;的过程相当于将原有的string变量指向的对象内容取出与”hello”作字符串相加操作再存进另一个新的String对象当中，再让string变量指向新生成的对象。结论：使用String进行字符串拼接会造成内存资源浪费。 public class Main { public static void main(String[] args) { StringBuilder stringBuilder = new StringBuilder(); for(int i=0;i&lt;10000;i++){ stringBuilder.append(&quot;hello&quot;); } } } 使用StringBuilder new操作只进行了一次，也就是说只生成了一个对象，append操作是在原有对象的基础上进行的。因此在循环了10000次之后，这段代码所占的资源要比上面小得多。 事实上，StringBuilder和StringBuffer类拥有的成员属性以及成员方法基本相同，区别是StringBuffer类的成员方法前面多了一个关键字：synchronized，不用多说，这个关键字是在多线程访问时起到安全保护作用的,也就是说StringBuffer是线程安全的。 StringBuilder的insert方法： public StringBuilder insert(int index, char str[], int offset, int len){ super.insert(index, str, offset, len); return this; } StringBuffer的insert方法： public synchronized StringBuffer insert(int index, char str[], int offset, int len) { super.insert(index, str, offset, len); return this; } 结论： 1.对于直接相加字符串，效率很高，因为在编译器便确定了它的值，也就是说形如”I”+”love”+”java”; 的字符串相加，在编译期间便被优化成了”Ilovejava”。这个可以用javap -c命令反编译生成的class文件进行验证。 对于间接相加（即包含字符串引用），形如s1+s2+s3; 效率要比直接相加低，因为在编译器不会对引用变量进行优化。 2.String、StringBuilder、StringBuffer三者的执行效率： StringBuilder &gt; StringBuffer &gt; String 当然这个是相对的，不一定在所有情况下都是这样。 比如String str = “hello”+ “world”的效率就比 StringBuilder st = new StringBuilder().append(“hello”).append(“world”)要高。 因此，这三个类是各有利弊，应当根据不同的情况来进行选择使用： 当字符串相加操作或者改动较少的情况下，建议使用 String str=”hello”这种形式； 当字符串相加操作较多的情况下，建议使用StringBuilder，如果采用了多线程，则使用StringBuffer。 尽可能使用局部变量调用方法时传递的参数以及在调用中创建的临时变量都保存在栈中，速度较快，其他变量，如静态变量、实例变量等，都在堆中创建，速度较慢。另外，栈中创建的变量，随着方法的运行结束，这些内容就没了，不需要额外的垃圾回收。 及时关闭流Java编程过程中，进行数据库连接、I/O流操作时务必小心，在使用完毕后，及时关闭以释放资源。因为对这些大对象的操作会造成系统大的开销，稍有不慎，将会导致严重的后果。尽量避免在for循环中进行数据库的操作。 尽量减少对变量的重复计算明确一个概念，对方法的调用，即使方法中只有一句语句，也是有消耗的，包括创建栈帧、调用方法时保护现场、调用方法完毕时恢复现场等。所以例如下面的操作： for (int i = 0; i &lt; list.size(); i++) {...} 替换为 for (int i = 0, length = list.size(); i &lt; length; i++) {...} 这样，在list.size()很大的时候，就减少了很多的消耗 尽量采用懒加载的策略，即在需要的时候才创建例如： String str = &quot;aaa&quot;; if (i == 1) { list.add(str); } 建议替换为： if (i == 1) { String str = &quot;aaa&quot;; list.add(str); } 慎用异常异常对性能不利。抛出异常首先要创建一个新的对象，Throwable接口的构造函数调用名为fillInStackTrace()的本地同步方法，fillInStackTrace()方法检查堆栈，收集调用跟踪信息。只要有异常被抛出，Java虚拟机就必须调整调用堆栈，因为在处理过程中创建了一个新的对象。异常只能用于错误处理，不应该用来控制程序流程。 如果能估计到待添加的内容长度，为底层以数组方式实现的集合、工具类指定初始长度比如ArrayList、LinkedLlist、StringBuilder、StringBuffer、HashMap、HashSet等等，以StringBuilder为例： StringBuilder() // 默认分配16个字符的空间 StringBuilder(int size) // 默认分配size个字符的空间 StringBuilder(String str) // 默认分配16个字符+str.length()个字符空间 可以通过类（这里指的不仅仅是上面的StringBuilder）的构造函数来设定它的初始化容量，这样可以明显地提升性能。比如StringBuilder吧，length表示当前的StringBuilder能保持的字符数量。因为当StringBuilder达到最大容量的时候，它会将自身容量增加到当前的2倍再加2，无论何时只要StringBuilder达到它的最大容量，它就不得不创建一个新的字符数组然后将旧的字符数组内容拷贝到新字符数组中—-这是十分耗费性能的一个操作。试想，如果能预估到字符数组中大概要存放5000个字符而不指定长度，最接近5000的2次幂是4096，每次扩容加的2不管，那么： 在4096 的基础上，再申请8194个大小的字符数组，加起来相当于一次申请了12290个大小的字符数组，如果一开始能指定5000个大小的字符数组，就节省了一倍以上的空间 把原来的4096个字符拷贝到新的的字符数组中去。 这样，既浪费内存空间又降低代码运行效率。所以，给底层以数组实现的集合、工具类设置一个合理的初始化容量是错不了的，这会带来立竿见影的效果。但是，注意，像HashMap这种是以数组+链表实现的集合，别把初始大小和你估计的大小设置得一样，因为一个table上只连接一个对象的可能性几乎为0。初始大小建议设置为2的N次幂，如果能估计到有2000个元素，设置成new HashMap(128)、new HashMap(256)都可以。 循环内不要不断创建对象引用例如： for (int i = 1; i &lt;= count; i++){ Object obj = new Object(); } 这种做法会导致内存中有count份Object对象引用存在，count很大的话，就耗费内存了，建议为改为： Object obj = null; for (int i = 0; i &lt;= count; i++) { obj = new Object(); } 基于效率和类型检查的考虑，应该尽可能使用array，无法确定数组大小时才使用ArrayList，尽量使用HashMap、ArrayList、StringBuilder，除非线程安全需要，否则不推荐使用Hashtable、Vector、StringBuffer，后三者由于使用同步机制而导致了性能开销。实现RandomAccess接口的集合比如ArrayList，应当使用最普通的for循环而不是foreach循环来遍历if (list instanceof RandomAccess){ for (int i = 0; i &lt; list.size(); i++){ ... } } else { Iterator&lt;?&gt; iterator = list.iterable(); while (iterator.hasNext()){ iterator.next() } } 这是JDK推荐给用户的。JDK API对于RandomAccess接口的解释是：实现RandomAccess接口用来表明其支持快速随机访问，此接口的主要目的是允许一般的算法更改其行为，从而将其应用到随机或连续访问列表时能提供良好的性能。实际经验表明，实现RandomAccess接口的类实例，假如是随机访问的，使用普通for循环效率将高于使用foreach循环；反过来，如果是顺序访问的，则使用Iterator会效率更高。 使用带缓冲的输入输出流进行IO操作带缓冲的输入输出流，即BufferedReader、BufferedWriter、BufferedInputStream、BufferedOutputStream，这可以极大地提升IO效率 字符串变量和字符串常量equals的时候将字符串常量写在前面把一个基本数据类型转为字符串，基本数据类型.toString()是最快的方式、String.valueOf(数据)次之、数据+””最慢public static void main(String[] args){ int loopTime = 50000; Integer i = 0; long startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++){ String str = String.valueOf(i); } System.out.println(&quot;String.valueOf()：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++){ String str = i.toString(); } System.out.println(&quot;Integer.toString()：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++){ String str = i + &quot;&quot;; } System.out.println(&quot;i + \&quot;\&quot;：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); } 运行结果为： String.valueOf()：11ms Integer.toString()：5ms i + &quot;&quot;：25ms 三者对比下来，明显是2最快、1次之、3最慢。结论： 以后遇到把一个基本数据类型转为String的时候，优先考虑使用toString()方法。至于为什么，很简单： String.valueOf()方法底层调用了Integer.toString()方法，但是会在调用前做空判断 Integer.toString()方法就不说了，直接调用了 i + “”底层使用了StringBuilder实现，先用append方法拼接，再用toString()方法获取字符串 使用最有效率的方式去遍历Mappublic static void main(String[] args){ HashMap&lt;String, String&gt; hm = new HashMap&lt;String, String&gt;(); hm.put(&quot;111&quot;, &quot;222&quot;); Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = hm.entrySet(); Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iter = entrySet.iterator(); while (iter.hasNext()) { Map.Entry&lt;String, String&gt; entry = iter.next(); System.out.println(entry.getKey() + &quot;\t&quot; + entry.getValue()); } } 如果你只是想遍历一下这个Map的key值，那用”Set keySet = hm.keySet();”会比较合适一些]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis相关及实践]]></title>
    <url>%2F2018%2F12%2F15%2FRedis%E7%9B%B8%E5%85%B3%E5%8F%8A%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[定义redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。Redis 是一个高性能的key-value数据库。 redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部 分场合可以对关系数据库起到很好的补充作用。它提供了Java，C/C++，C#，PHP，JavaScript，Perl，Object-C，Python，Ruby，Erlang等客户端，使用很方便。Redis支持主从同步。数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器。这使得Redis可执行单层树复制。存盘可以有意无意的对数据进行写操作。由于完全实现了发布/订阅机制，使得从数据库在任何地方同步树时，可订阅一个频道并接收主服务器完整的消息发布记录。同步对读取操作的可扩展性和数据冗余很有帮助。redis的官网地址，非常好记，是redis.io。（特意查了一下，域名后缀io属于国家域名，是british Indian Ocean territory，即英属印度洋领地）目前，Vmware在资助着redis项目的开发和维护。详见百度百科 RedisTemplate常用集合-opsForValue1.set(K key, V value)新增一个字符串类型的值,key是键，value是值。 redisTemplate.opsForValue().set(&quot;stringValue&quot;,&quot;bbb&quot;); 2.get(Object key)获取key键对应的值 String stringValue = redisTemplate.opsForValue().get(&quot;stringValue&quot;)+&quot;&quot;; System.out.println(&quot;通过get(Object key)方法获取set(K key, V value)方法新增的字符串值:&quot; + stringValue); 3.append(K key, String value)在原有的值基础上新增字符串到末尾。 redisTemplate.opsForValue().append(&quot;stringValue&quot;,&quot;aaa&quot;); String stringValueAppend = redisTemplate.opsForValue().get(&quot;stringValue&quot;)+&quot;&quot;; ` System.out.println(&quot;通过append(K key, String value)方法修改后的字符串:&quot;+stringValueAppend); 4.get(K key, long start, long end)截取key键对应值得字符串，从开始下标位置开始到结束下标的位置(包含结束下标)的字符串。 String cutString = redisTemplate.opsForValue().get(&quot;stringValue&quot;,0,3); System.out.println(&quot;通过get(K key, long start, long end)方法获取截取的字符串:&quot;+cutString); 5.getAndSet(K key, V value)获取原来key键对应的值并重新赋新值。 String oldAndNewStringValue = redisTemplate.opsForValue().getAndSet(&quot;stringValue&quot;,&quot;ccc&quot;)+&quot;&quot;; System.out.print(&quot;通过getAndSet(K key, V value)方法获取原来的&quot; + oldAndNewStringValue + &quot;,&quot;); String newStringValue = redisTemplate.opsForValue().get(&quot;stringValue&quot;)+&quot;&quot;; System.out.println(&quot;修改过后的值:&quot;+newStringValue); 6.setBit(K key, long offset, boolean value)key键对应的值value对应的ascii码,在offset的位置(从左向右数)变为value。 redisTemplate.opsForValue().setBit(&quot;stringValue&quot;,1,false); newStringValue = redisTemplate.opsForValue().get(&quot;stringValue&quot;)+&quot;&quot;; System.out.println(&quot;通过setBit(K key,long offset,boolean value)方法修改过后的值:&quot;+newStringValue); 7.getBit(K key, long offset)判断指定的位置ASCII码的bit位是否为1。 boolean bitBoolean = redisTemplate.opsForValue().getBit(&quot;stringValue&quot;,1); boolean bitBoolean = redisTemplate.opsForValue().getBit(&quot;stringValue&quot;,1); 8.size(K key)获取指定字符串的长度。 Long stringValueLength = redisTemplate.opsForValue().size(&quot;stringValue&quot;); Long stringValueLength = redisTemplate.opsForValue().size(&quot;stringValue&quot;); 9.increment(K key, double delta) 以增量的方式将double值存储在变量中。 double stringValueDouble = redisTemplate.opsForValue().increment(&quot;doubleValue&quot;,5); System.out.println(&quot;通过increment(K key, double delta)方法以增量方式存储double值:&quot; + stringValueDouble); 10.increment(K key, long delta)以增量的方式将long值存储在变量中。 double stringValueLong = redisTemplate.opsForValue().increment(&quot;longValue&quot;,6); System.out.println(&quot;通过increment(K key, long delta)方法以增量方式存储long值:&quot; + stringValueLong); 11.setIfAbsent(K key, V value)如果键不存在则新增,存在则不改变已经有的值。 boolean absentBoolean = redisTemplate.opsForValue().setIfAbsent(&quot;absentValue&quot;,&quot;fff&quot;); System.out.println(&quot;通过setIfAbsent(K key, V value)方法判断变量值absentValue是否存在:&quot; + absentBoolean); if(absentBoolean){ String absentValue = redisTemplate.opsForValue().get(&quot;absentValue&quot;)+&quot;&quot;; System.out.print(&quot;,不存在，则新增后的值是:&quot;+absentValue); boolean existBoolean = redisTemplate.opsForValue().setIfAbsent(&quot;absentValue&quot;,&quot;eee&quot;); System.out.print(&quot;,再次调用setIfAbsent(K key, V value)判断absentValue是否存在并重新赋值:&quot; + existBoolean); if(!existBoolean){ absentValue = redisTemplate.opsForValue().get(&quot;absentValue&quot;)+&quot;&quot;; System.out.print(&quot;如果存在,则重新赋值后的absentValue变量的值是:&quot; + absentValue); } } 12.set(K key, V value, long timeout, TimeUnit unit)设置变量值的过期时间。 redisTemplate.opsForValue().set(&quot;timeOutValue&quot;,&quot;timeOut&quot;,5,TimeUnit.SECONDS); String timeOutValue = redisTemplate.opsForValue().get(&quot;timeOutValue&quot;)+&quot;&quot;; System.out.println(&quot;通过set(K key, V value, long timeout, TimeUnit unit)方法设置过期时间，过期之前获取的数据:&quot;+timeOutValue); Thread.sleep(5*1000); timeOutValue = redisTemplate.opsForValue().get(&quot;timeOutValue&quot;)+&quot;&quot;; System.out.print(&quot;,等待10s过后，获取的值:&quot;+timeOutValue); 13.set(K key, V value, long offset) 覆盖从指定位置开始的值。 redisTemplate.opsForValue().set(&quot;absentValue&quot;,&quot;dd&quot;,1); String overrideString = redisTemplate.opsForValue().get(&quot;absentValue&quot;)+&quot;&quot;; System.out.println(&quot;通过set(K key, V value, long offset)方法覆盖部分的值:&quot;+overrideString); 14.multiSet(Map&lt;? extends K,? extends V&gt; map)设置map集合到redis。 Map valueMap = new HashMap(); valueMap.put(&quot;valueMap1&quot;,&quot;map1&quot;); valueMap.put(&quot;valueMap2&quot;,&quot;map2&quot;); valueMap.put(&quot;valueMap3&quot;,&quot;map3&quot;); redisTemplate.opsForValue().multiSet(valueMap); 15.multiGet(Collection keys) 根据集合取出对应的value值。 //根据List集合取出对应的value值 List paraList = new ArrayList(); paraList.add(&quot;valueMap1&quot;); paraList.add(&quot;valueMap2&quot;); paraList.add(&quot;valueMap3&quot;); List&lt;String&gt; valueList = redisTemplate.opsForValue().multiGet(paraList); for (String value : valueList){ System.out.println(&quot;通过multiGet(Collection&lt;K&gt; keys)方法获取map值:&quot; + value); } 16.multiSetIfAbsent(Map&lt;? extends K,? extends V&gt; map)如果对应的map集合名称不存在，则添加，如果存在则不做修改。 Map valueMap = new HashMap(); valueMap.put(&quot;valueMap1&quot;,&quot;map1&quot;); valueMap.put(&quot;valueMap2&quot;,&quot;map2&quot;); valueMap.put(&quot;valueMap3&quot;,&quot;map3&quot;); redisTemplate.opsForValue().multiSetIfAbsent(valueMap); RedisTemplate常用集合-opsForList1.leftPush(K key, V value)在变量左边添加元素值。 redisTemplate.opsForList().leftPush(&quot;list&quot;,&quot;a&quot;); redisTemplate.opsForList().leftPush(&quot;list&quot;,&quot;b&quot;); redisTemplate.opsForList().leftPush(&quot;list&quot;,&quot;c&quot;); 2.index(K key, long index)获取集合指定位置的值。 String listValue = redisTemplate.opsForList().index(&quot;list&quot;,1) + &quot;&quot;; System.out.println(&quot;通过index(K key, long index)方法获取指定位置的值:&quot; + listValue); 3.range(K key, long start, long end)获取指定区间的值。 List&lt;Object&gt; list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过range(K key, long start, long end)方法获取指定范围的集合值:&quot;+list); 4.leftPush(K key, V pivot, V value)把最后一个参数值放到指定集合的第一个出现中间参数的前面，如果中间参数值存在的话。 redisTemplate.opsForList().leftPush(&quot;list&quot;,&quot;a&quot;,&quot;n&quot;); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过leftPush(K key, V pivot, V value)方法把值放到指定参数值前面:&quot; + list); 5.leftPushAll(K key, V… values)向左边批量添加参数元素。 redisTemplate.opsForList().leftPushAll(&quot;list&quot;,&quot;w&quot;,&quot;x&quot;,&quot;y&quot;); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过leftPushAll(K key, V... values)方法批量添加元素:&quot; + list); 6.leftPushAll(K key, Collection values)以集合的方式向左边批量添加元素。 List newList = new ArrayList(); newList.add(&quot;o&quot;); newList.add(&quot;p&quot;); newList.add(&quot;q&quot;); redisTemplate.opsForList().leftPushAll(&quot;list&quot;,newList); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过leftPushAll(K key, Collection&lt;V&gt; values)方法以集合的方式批量添加元素:&quot; + list); 7.leftPushIfPresent(K key, V value) 如果存在集合则添加元素。 redisTemplate.opsForList().leftPushIfPresent(&quot;presentList&quot;,&quot;o&quot;); list = redisTemplate.opsForList().range(&quot;presentList&quot;,0,-1); System.out.println(&quot;通过leftPushIfPresent(K key, V value)方法向已存在的集合添加元素:&quot; + list); 8.rightPush(K key, V value)向集合最右边添加元素。 redisTemplate.opsForList().rightPush(&quot;list&quot;,&quot;w&quot;); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过rightPush(K key, V value)方法向最右边添加元素:&quot; + list); 9.rightPush(K key, V pivot, V value)向集合中第一次出现第二个参数变量元素的右边添加第三个参数变量的元素值。 redisTemplate.opsForList().rightPush(&quot;list&quot;,&quot;w&quot;,&quot;r&quot;); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过rightPush(K key, V pivot, V value)方法向最右边添加元素:&quot; + list); 10.rightPushAll(K key, V… values)向右边批量添加元素。 redisTemplate.opsForList().rightPushAll(&quot;list&quot;,&quot;j&quot;,&quot;k&quot;); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过rightPushAll(K key, V... values)方法向最右边批量添加元素:&quot; + list); 11.rightPushAll(K key, Collection values)以集合方式向右边添加元素。 newList.clear(); newList.add(&quot;g&quot;); newList.add(&quot;h&quot;); redisTemplate.opsForList().rightPushAll(&quot;list&quot;,newList); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过rightPushAll(K key, Collection&lt;V&gt; values)方法向最右边以集合方式批量添加元素:&quot; + list); 12.rightPushIfPresent(K key, V value)向已存在的集合中添加元素。 redisTemplate.opsForList().rightPushIfPresent(&quot;presentList&quot;,&quot;d&quot;); list = redisTemplate.opsForList().range(&quot;presentList&quot;,0,-1); System.out.println(&quot;通过rightPushIfPresent(K key, V value)方法已存在的集合向最右边添加元素:&quot; + list); 13.size(K key)获取集合长度。 long listLength = redisTemplate.opsForList().size(&quot;list&quot;); System.out.println(&quot;通过size(K key)方法获取集合list的长度为:&quot; + listLength); 14.leftPop(K key)移除集合中的左边第一个元素。 Object popValue = redisTemplate.opsForList().leftPop(&quot;list&quot;); System.out.print(&quot;通过leftPop(K key)方法移除的元素是:&quot; + popValue); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;,剩余的元素是:&quot; + list); 15.leftPop(K key, long timeout, TimeUnit unit)移除集合中左边的元素在等待的时间里，如果超过等待的时间仍没有元素则退出。 popValue = redisTemplate.opsForList().leftPop(&quot;presentList&quot;,1, TimeUnit.SECONDS); System.out.print(&quot;通过leftPop(K key, long timeout, TimeUnit unit)方法移除的元素是:&quot; + popValue); list = redisTemplate.opsForList().range(&quot;presentList&quot;,0,-1); System.out.println(&quot;,剩余的元素是:&quot; + list); 16.rightPop(K key)移除集合中右边的元素。 popValue = redisTemplate.opsForList().rightPop(&quot;list&quot;); System.out.print(&quot;通过rightPop(K key)方法移除的元素是:&quot; + popValue); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;,剩余的元素是:&quot; + list); 17.rightPop(K key, long timeout, TimeUnit unit)移除集合中右边的元素在等待的时间里，如果超过等待的时间仍没有元素则退出。 popValue = redisTemplate.opsForList().rightPop(&quot;presentList&quot;,1, TimeUnit.SECONDS); System.out.print(&quot;通过rightPop(K key, long timeout, TimeUnit unit)方法移除的元素是:&quot; + popValue); list = redisTemplate.opsForList().range(&quot;presentList&quot;,0,-1); System.out.println(&quot;,剩余的元素是:&quot; + list); 18.rightPopAndLeftPush(K sourceKey, K destinationKey)移除集合中右边的元素，同时在左边加入一个元素。 popValue = redisTemplate.opsForList().rightPopAndLeftPush(&quot;list&quot;,&quot;12&quot;); System.out.print(&quot;通过rightPopAndLeftPush(K sourceKey, K destinationKey)方法移除的元素是:&quot; + popValue); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;,剩余的元素是:&quot; + list); 19.rightPopAndLeftPush(K sourceKey, K destinationKey, long timeout, TimeUnit unit) 移除集合中右边的元素在等待的时间里，同时在左边添加元素，如果超过等待的时间仍没有元素则退出。 popValue = redisTemplate.opsForList().rightPopAndLeftPush(&quot;presentList&quot;,&quot;13&quot;,1,TimeUnit.SECONDS); System.out.println(&quot;通过rightPopAndLeftPush(K sourceKey, K destinationKey, long timeout, TimeUnit unit)方法移除的元素是:&quot; + popValue); list = redisTemplate.opsForList().range(&quot;presentList&quot;,0,-1); System.out.print(&quot;,剩余的元素是:&quot; + list); 20.set(K key, long index, V value)在集合的指定位置插入元素,如果指定位置已有元素，则覆盖，没有则新增，超过集合下标+n则会报错。 redisTemplate.opsForList().set(&quot;presentList&quot;,3,&quot;15&quot;); list = redisTemplate.opsForList().range(&quot;presentList&quot;,0,-1); System.out.print(&quot;通过set(K key, long index, V value)方法在指定位置添加元素后:&quot; + list); 21.remove(K key, long count, Object value)从存储在键中的列表中删除等于值的元素的第一个计数事件。count&gt; 0：删除等于从左到右移动的值的第一个元素；count&lt; 0：删除等于从右到左移动的值的第一个元素；count = 0：删除等于value的所有元素。 long removeCount = redisTemplate.opsForList().remove(&quot;list&quot;,0,&quot;w&quot;); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过remove(K key, long count, Object value)方法移除元素数量:&quot; + removeCount); System.out.println(&quot;,剩余的元素:&quot; + list); 22.trim(K key, long start, long end)截取集合元素长度，保留长度内的数据。 redisTemplate.opsForList().trim(&quot;list&quot;,0,5); list = redisTemplate.opsForList().range(&quot;list&quot;,0,-1); System.out.println(&quot;通过trim(K key, long start, long end)方法截取后剩余元素:&quot; + list); RedisTemplate常用集合-opsForHash1.put(H key, HK hashKey, HV value)新增hashMap值。 redisTemplate.opsForHash().put(&quot;hashValue&quot;,&quot;map1&quot;,&quot;map1-1&quot;); redisTemplate.opsForHash().put(&quot;hashValue&quot;,&quot;map2&quot;,&quot;map2-2&quot;); 2.values(H key)获取指定变量中的hashMap值。 List&lt;Object&gt; hashList = redisTemplate.opsForHash().values(&quot;hashValue&quot;); System.out.println(&quot;通过values(H key)方法获取变量中的hashMap值:&quot; + hashList); 3.entries(H key)获取变量中的键值对。 Map&lt;Object,Object&gt; map = redisTemplate.opsForHash().entries(&quot;hashValue&quot;); System.out.println(&quot;通过entries(H key)方法获取变量中的键值对:&quot; + map); 4.get(H key, Object hashKey)获取变量中的指定map键是否有值,如果存在该map键则获取值，没有则返回null。 Object mapValue = redisTemplate.opsForHash().get(&quot;hashValue&quot;,&quot;map1&quot;); System.out.println(&quot;通过get(H key, Object hashKey)方法获取map键的值:&quot; + mapValue); 5.hasKey(H key, Object hashKey) 判断变量中是否有指定的map键。 boolean hashKeyBoolean = redisTemplate.opsForHash().hasKey(&quot;hashValue&quot;,&quot;map3&quot;); System.out.println(&quot;通过hasKey(H key, Object hashKey)方法判断变量中是否存在map键:&quot; + hashKeyBoolean); 6.keys(H key)获取变量中的键。 Set&lt;Object&gt; keySet = redisTemplate.opsForHash().keys(&quot;hashValue&quot;); System.out.println(&quot;通过keys(H key)方法获取变量中的键:&quot; + keySet); 7.size(H key)获取变量的长度。 long hashLength = redisTemplate.opsForHash().size(&quot;hashValue&quot;); System.out.println(&quot;通过size(H key)方法获取变量的长度:&quot; + hashLength); 8.increment(H key, HK hashKey, double delta)使变量中的键以double值的大小进行自增长。 double hashIncDouble = redisTemplate.opsForHash().increment(&quot;hashInc&quot;,&quot;map1&quot;,3); System.out.println(&quot;通过increment(H key, HK hashKey, double delta)方法使变量中的键以值的大小进行自增长:&quot; + hashIncDouble); 9.increment(H key, HK hashKey, long delta)使变量中的键以long值的大小进行自增长。 long hashIncLong = redisTemplate.opsForHash().increment(&quot;hashInc&quot;,&quot;map2&quot;,6); System.out.println(&quot;通过increment(H key, HK hashKey, long delta)方法使变量中的键以值的大小进行自增长:&quot; + hashIncLong); 10.multiGet(H key, Collection hashKeys)以集合的方式获取变量中的值。 List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); list.add(&quot;map1&quot;); list.add(&quot;map2&quot;); List mapValueList = redisTemplate.opsForHash().multiGet(&quot;hashValue&quot;,list); System.out.println(&quot;通过multiGet(H key, Collection&lt;HK&gt; hashKeys)方法以集合的方式获取变量中的值:&quot;+mapValueList); 11.putAll(H key, Map&lt;? extends HK,? extends HV&gt; m)以map集合的形式添加键值对。 Map newMap = new HashMap(); newMap.put(&quot;map3&quot;,&quot;map3-3&quot;); newMap.put(&quot;map5&quot;,&quot;map5-5&quot;); redisTemplate.opsForHash().putAll(&quot;hashValue&quot;,newMap); map = redisTemplate.opsForHash().entries(&quot;hashValue&quot;); System.out.println(&quot;通过putAll(H key, Map&lt;? extends HK,? extends HV&gt; m)方法以map集合的形式添加键值对:&quot; + map); 12.putIfAbsent(H key, HK hashKey, HV value)如果变量值存在，在变量中可以添加不存在的的键值对，如果变量不存在，则新增一个变量，同时将键值对添加到该变量。 redisTemplate.opsForHash().putIfAbsent(&quot;hashValue&quot;,&quot;map6&quot;,&quot;map6-6&quot;); map = redisTemplate.opsForHash().entries(&quot;hashValue&quot;); System.out.println(&quot;通过putIfAbsent(H key, HK hashKey, HV value)方法添加不存在于变量中的键值对:&quot; + map); 13.scan(H key, ScanOptions options)匹配获取键值对，ScanOptions.NONE为获取全部键对，ScanOptions.scanOptions().match(&quot;map1&quot;).build() 匹配获取键位map1的键值对,不能模糊匹配。 Cursor&lt;Map.Entry&lt;Object,Object&gt;&gt; cursor = redisTemplate.opsForHash().scan(&quot;hashValue&quot;,ScanOptions.scanOptions().match(&quot;map1&quot;).build()); //Cursor&lt;Map.Entry&lt;Object,Object&gt;&gt; cursor = redisTemplate.opsForHash().scan(&quot;hashValue&quot;,ScanOptions.NONE); while (cursor.hasNext()){ Map.Entry&lt;Object,Object&gt; entry = cursor.next(); System.out.println(&quot;通过scan(H key, ScanOptions options)方法获取匹配键值对:&quot; + entry.getKey() + &quot;----&gt;&quot; + entry.getValue()); } 14.delete(H key, Object… hashKeys)删除变量中的键值对，可以传入多个参数，删除多个键值对。 redisTemplate.opsForHash().delete(&quot;hashValue&quot;,&quot;map1&quot;,&quot;map2&quot;); map = redisTemplate.opsForHash().entries(&quot;hashValue&quot;); System.out.println(&quot;通过delete(H key, Object... hashKeys)方法删除变量中的键值对后剩余的:&quot; + map); RedisTemplate常用集合-opsForSet1.add(K key, V… values)向变量中批量添加值。 redisTemplate.opsForSet().add(&quot;setValue&quot;,&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;B&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;); 2.members(K key) 获取变量中的值。 Set set = redisTemplate.opsForSet().members(&quot;setValue&quot;); System.out.println(&quot;通过members(K key)方法获取变量中的元素值:&quot; + set); 3.size(K key)获取变量中值的长度。 long setLength = redisTemplate.opsForSet().size(&quot;setValue&quot;); System.out.println(&quot;通过size(K key)方法获取变量中元素值的长度:&quot; + setLength); 4.randomMember(K key)随机获取变量中的元素。 Object randomMember = redisTemplate.opsForSet().randomMember(&quot;setValue&quot;); System.out.println(&quot;通过randomMember(K key)方法随机获取变量中的元素:&quot; + randomMember); 5.randomMembers(K key, long count)随机获取变量中指定个数的元素。 List randomMembers = redisTemplate.opsForSet().randomMembers(&quot;setValue&quot;,2); System.out.println(&quot;通过randomMembers(K key, long count)方法随机获取变量中指定个数的元素:&quot; + randomMembers); 6.isMember(K key, Object o)检查给定的元素是否在变量中。 boolean isMember = redisTemplate.opsForSet().isMember(&quot;setValue&quot;,&quot;A&quot;); System.out.println(&quot;通过isMember(K key, Object o)方法检查给定的元素是否在变量中:&quot; + isMember); 7.move(K key, V value, K destKey)转移变量的元素值到目的变量。 boolean isMove = redisTemplate.opsForSet().move(&quot;setValue&quot;,&quot;A&quot;,&quot;destSetValue&quot;); if(isMove){ set = redisTemplate.opsForSet().members(&quot;setValue&quot;); System.out.print(&quot;通过move(K key, V value, K destKey)方法转移变量的元素值到目的变量后的剩余元素:&quot; + set); set = redisTemplate.opsForSet().members(&quot;destSetValue&quot;); System.out.println(&quot;,目的变量中的元素值:&quot; + set); } 8.pop(K key)弹出变量中的元素。 Object popValue = redisTemplate.opsForSet().pop(&quot;setValue&quot;); System.out.print(&quot;通过pop(K key)方法弹出变量中的元素:&quot; + popValue); set = redisTemplate.opsForSet().members(&quot;setValue&quot;); System.out.println(&quot;,剩余元素:&quot; + set) 9.remove(K key, Object… values)批量移除变量中的元素。 long removeCount = redisTemplate.opsForSet().remove(&quot;setValue&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;); System.out.print(&quot;通过remove(K key, Object... values)方法移除变量中的元素个数:&quot; + removeCount); set = redisTemplate.opsForSet().members(&quot;setValue&quot;); System.out.println(&quot;,剩余元素:&quot; + set); 10.scan(K key, ScanOptions options)匹配获取键值对，ScanOptions.NONE为获取全部键值对；ScanOptions.scanOptions().match(&quot;C&quot;).build()匹配获取键位map1的键值对,不能模糊匹配。 //Cursor&lt;Object&gt; cursor = redisTemplate.opsForSet().scan(&quot;setValue&quot;, ScanOptions.NONE); Cursor&lt;Object&gt; cursor = redisTemplate.opsForSet().scan(&quot;setValue&quot;, ScanOptions.scanOptions().match(&quot;C&quot;).build()); while (cursor.hasNext()){ Object object = cursor.next(); System.out.println(&quot;通过scan(K key, ScanOptions options)方法获取匹配的值:&quot; + object); } 11.difference(K key, Collection otherKeys)通过集合求差值。 List list = new ArrayList(); list.add(&quot;destSetValue&quot;); Set differenceSet = redisTemplate.opsForSet().difference(&quot;setValue&quot;,list); System.out.println(&quot;通过difference(K key, Collection&lt;K&gt; otherKeys)方法获取变量中与给定集合中变量不一样的值:&quot; + differenceSet); 12.difference(K key, K otherKey)通过给定的key求2个set变量的差值。 differenceSet = redisTemplate.opsForSet().difference(&quot;setValue&quot;,&quot;destSetValue&quot;); System.out.println(&quot;通过difference(K key, Collection&lt;K&gt; otherKeys)方法获取变量中与给定变量不一样的值:&quot; + differenceSet); 13.differenceAndStore(K key, K otherKey, K destKey)将求出来的差值元素保存。 redisTemplate.opsForSet().differenceAndStore(&quot;setValue&quot;,&quot;destSetValue&quot;,&quot;storeSetValue&quot;); set = redisTemplate.opsForSet().members(&quot;storeSetValue&quot;); System.out.println(&quot;通过differenceAndStore(K key, K otherKey, K destKey)方法将求出来的差值元素保存:&quot; + set); 14.differenceAndStore(K key, Collection otherKeys, K destKey)将求出来的差值元素保存。 redisTemplate.opsForSet().differenceAndStore(&quot;setValue&quot;,list,&quot;storeSetValue&quot;); set = redisTemplate.opsForSet().members(&quot;storeSetValue&quot;); System.out.println(&quot;通过differenceAndStore(K key, Collection&lt;K&gt; otherKeys, K destKey)方法将求出来的差值元素保存:&quot; + set); 15.distinctRandomMembers(K key, long count)获取去重的随机元素。 set = redisTemplate.opsForSet().distinctRandomMembers(&quot;setValue&quot;,2); System.out.println(&quot;通过distinctRandomMembers(K key, long count)方法获取去重的随机元素:&quot; + set); 16.intersect(K key, K otherKey)获取2个变量中的交集。 set = redisTemplate.opsForSet().intersect(&quot;setValue&quot;,&quot;destSetValue&quot;); System.out.println(&quot;通过intersect(K key, K otherKey)方法获取交集元素:&quot; + set); 17.intersect(K key, Collection otherKeys)获取多个变量之间的交集。 set = redisTemplate.opsForSet().intersect(&quot;setValue&quot;,list); System.out.println(&quot;通过intersect(K key, Collection&lt;K&gt; otherKeys)方法获取交集元素:&quot; + set); 18.intersectAndStore(K key, K otherKey, K destKey)获取2个变量交集后保存到最后一个参数上。 redisTemplate.opsForSet().intersectAndStore(&quot;setValue&quot;,&quot;destSetValue&quot;,&quot;intersectValue&quot;); set = redisTemplate.opsForSet().members(&quot;intersectValue&quot;); System.out.println(&quot;通过intersectAndStore(K key, K otherKey, K destKey)方法将求出来的交集元素保存:&quot; + set); 19.intersectAndStore(K key, Collection otherKeys, K destKey)获取多个变量的交集并保存到最后一个参数上。 redisTemplate.opsForSet().intersectAndStore(&quot;setValue&quot;,list,&quot;intersectListValue&quot;); set = redisTemplate.opsForSet().members(&quot;intersectListValue&quot;); System.out.println(&quot;通过intersectAndStore(K key, Collection&lt;K&gt; otherKeys, K destKey)方法将求出来的交集元素保存:&quot; + set); 20.union(K key, K otherKey)获取2个变量的合集。 set = redisTemplate.opsForSet().union(&quot;setValue&quot;,&quot;destSetValue&quot;); System.out.println(&quot;通过union(K key, K otherKey)方法获取2个变量的合集元素:&quot; + set); 21.union(K key, Collection otherKeys)获取多个变量的合集。 set = redisTemplate.opsForSet().union(&quot;setValue&quot;,list); System.out.println(&quot;通过union(K key, Collection&lt;K&gt; otherKeys)方法获取多个变量的合集元素:&quot; + set); 22.unionAndStore(K key, K otherKey, K destKey)获取2个变量合集后保存到最后一个参数上。 redisTemplate.opsForSet().unionAndStore(&quot;setValue&quot;,&quot;destSetValue&quot;,&quot;unionValue&quot;); set = redisTemplate.opsForSet().members(&quot;unionValue&quot;); System.out.println(&quot;通过unionAndStore(K key, K otherKey, K destKey)方法将求出来的交集元素保存:&quot; + set); 23.unionAndStore(K key, Collection otherKeys, K destKey)获取多个变量的合集并保存到最后一个参数上。 redisTemplate.opsForSet().unionAndStore(&quot;setValue&quot;,list,&quot;unionListValue&quot;); set = redisTemplate.opsForSet().members(&quot;unionListValue&quot;); System.out.println(&quot;通过unionAndStore(K key, Collection&lt;K&gt; otherKeys, K destKey)方法将求出来的交集元素保存:&quot; + set); RedisTemplate常用集合-opsForZSet1.add(K key, V value, double score)添加元素到变量中同时指定元素的分值。 redisTemplate.opsForZSet().add(&quot;zSetValue&quot;,&quot;A&quot;,1); redisTemplate.opsForZSet().add(&quot;zSetValue&quot;,&quot;B&quot;,3); redisTemplate.opsForZSet().add(&quot;zSetValue&quot;,&quot;C&quot;,2); redisTemplate.opsForZSet().add(&quot;zSetValue&quot;,&quot;D&quot;,5); 2.range(K key, long start, long end)获取变量指定区间的元素。 Set zSetValue = redisTemplate.opsForZSet().range(&quot;zSetValue&quot;,0,-1); System.out.println(&quot;通过range(K key, long start, long end)方法获取指定区间的元素:&quot; + zSetValue); 3.rangeByLex(K key, RedisZSetCommands.Range range) 用于获取满足非score的排序取值。这个排序只有在有相同分数的情况下才能使用，如果有不同的分数则返回值不确定。 RedisZSetCommands.Range range = new RedisZSetCommands.Range(); //range.gt(&quot;A&quot;); range.lt(&quot;D&quot;); zSetValue = redisTemplate.opsForZSet().rangeByLex(&quot;zSetValue&quot;, range); System.out.println(&quot;通过rangeByLex(K key, RedisZSetCommands.Range range)方法获取满足非score的排序取值元素:&quot; + zSetValue); 4.rangeByLex(K key, RedisZSetCommands.Range range, RedisZSetCommands.Limit limit)用于获取满足非score的设置下标开始的长度排序取值。 RedisZSetCommands.Limit limit = new RedisZSetCommands.Limit(); limit.count(2); //起始下标为0 limit.offset(1); zSetValue = redisTemplate.opsForZSet().rangeByLex(&quot;zSetValue&quot;, range,limit); System.out.println(&quot;通过rangeByLex(K key, RedisZSetCommands.Range range, RedisZSetCommands.Limit limit)方法获取满足非score的排序取值元素:&quot; + zSetValue); 5.add(K key, Set&lt;ZSetOperations.TypedTuple&gt; tuples)通过TypedTuple方式新增数据。 ZSetOperations.TypedTuple&lt;Object&gt; typedTuple1 = new DefaultTypedTuple&lt;Object&gt;(&quot;E&quot;,6.0); ZSetOperations.TypedTuple&lt;Object&gt; typedTuple2 = new DefaultTypedTuple&lt;Object&gt;(&quot;F&quot;,7.0); ZSetOperations.TypedTuple&lt;Object&gt; typedTuple3 = new DefaultTypedTuple&lt;Object&gt;(&quot;G&quot;,5.0); Set&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; typedTupleSet = new HashSet&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt;(); typedTupleSet.add(typedTuple1); typedTupleSet.add(typedTuple2); typedTupleSet.add(typedTuple3); redisTemplate.opsForZSet().add(&quot;typedTupleSet&quot;,typedTupleSet); zSetValue = redisTemplate.opsForZSet().range(&quot;typedTupleSet&quot;,0,-1); System.out.println(&quot;通过add(K key, Set&lt;ZSetOperations.TypedTuple&lt;V&gt;&gt; tuples)方法添加元素:&quot; + zSetValue); 6.rangeByScore(K key, double min, double max)根据设置的score获取区间值。 zSetValue = redisTemplate.opsForZSet().rangeByScore(&quot;zSetValue&quot;,1,2); System.out.println(&quot;通过rangeByScore(K key, double min, double max)方法根据设置的score获取区间值:&quot; + zSetValue); 7.rangeByScore(K key, double min, double max,long offset, long count)根据设置的score获取区间值从给定下标和给定长度获取最终值。 zSetValue = redisTemplate.opsForZSet().rangeByScore(&quot;zSetValue&quot;,1,5,1,3); System.out.println(&quot;通过rangeByScore(K key, double min, double max, long offset, long count)方法根据设置的score获取区间值:&quot; + zSetValue); 8.rangeWithScores(K key, long start, long end) 获取RedisZSetCommands.Tuples的区间值。 Set&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; typedTupleSet = redisTemplate.opsForZSet().rangeWithScores(&quot;typedTupleSet&quot;,1,3); Iterator&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; iterator = typedTupleSet.iterator(); while (iterator.hasNext()){ ZSetOperations.TypedTuple&lt;Object&gt; typedTuple = iterator.next(); Object value = typedTuple.getValue(); double score = typedTuple.getScore(); System.out.println(&quot;通过rangeWithScores(K key, long start, long end)方法获取RedisZSetCommands.Tuples的区间值:&quot; + value + &quot;----&gt;&quot; + score ); } 9.rangeByScoreWithScores(K key, double min, double max)获取RedisZSetCommands.Tuples的区间值通过分值。 Set&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; typedTupleSet = redisTemplate.opsForZSet().rangeByScoreWithScores(&quot;typedTupleSet&quot;,5,8); iterator = typedTupleSet.iterator(); while (iterator.hasNext()){ ZSetOperations.TypedTuple&lt;Object&gt; typedTuple = iterator.next(); Object value = typedTuple.getValue(); double score = typedTuple.getScore(); System.out.println(&quot;通过rangeByScoreWithScores(K key, double min, double max)方法获取RedisZSetCommands.Tuples的区间值通过分值:&quot; + value + &quot;----&gt;&quot; + score ); } 10.rangeByScoreWithScores(K key, double min, double max, long offset, long count)获取RedisZSetCommands.Tuples的区间值从给定下标和给定长度获取最终值通过分值。 Set&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; typedTupleSet = redisTemplate.opsForZSet().rangeByScoreWithScores(&quot;typedTupleSet&quot;,5,8,1,1); iterator = typedTupleSet.iterator(); while (iterator.hasNext()){ ZSetOperations.TypedTuple&lt;Object&gt; typedTuple = iterator.next(); Object value = typedTuple.getValue(); double score = typedTuple.getScore(); System.out.println(&quot;通过rangeByScoreWithScores(K key, double min, double max, long offset, long count)方法获取RedisZSetCommands.Tuples的区间值从给定下标和给定长度获取最终值通过分值:&quot; + value + &quot;----&gt;&quot; + score ); } 11.count(K key, double min, double max)获取区间值的个数。 long count = redisTemplate.opsForZSet().count(&quot;zSetValue&quot;,1,5); System.out.println(&quot;通过count(K key, double min, double max)方法获取区间值的个数:&quot; + count); 12.rank(K key, Object o)获取变量中元素的索引,下标开始位置为0。 long index = redisTemplate.opsForZSet().rank(&quot;zSetValue&quot;,&quot;B&quot;); System.out.println(&quot;通过rank(K key, Object o)方法获取变量中元素的索引:&quot; + index); 13.scan(K key, ScanOptions options)匹配获取键值对，ScanOptions.NONE为获取全部键值对；ScanOptions.scanOptions().match(&quot;C&quot;).build()匹配获取键位map1的键值对,不能模糊匹配。 //Cursor&lt;Object&gt; cursor = redisTemplate.opsForSet().scan(&quot;setValue&quot;, ScanOptions.NONE); Cursor&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; cursor = redisTemplate.opsForZSet().scan(&quot;zSetValue&quot;, ScanOptions.NONE); while (cursor.hasNext()){ ZSetOperations.TypedTuple&lt;Object&gt; typedTuple = cursor.next(); System.out.println(&quot;通过scan(K key, ScanOptions options)方法获取匹配元素:&quot; + typedTuple.getValue() + &quot;---&gt;&quot; + typedTuple.getScore()); } 14.score(K key, Object o) 获取元素的分值。 double score = redisTemplate.opsForZSet().score(&quot;zSetValue&quot;,&quot;B&quot;); System.out.println(&quot;通过score(K key, Object o)方法获取元素的分值:&quot; + score); 15.zCard(K key)获取变量中元素的个数。 long zCard = redisTemplate.opsForZSet().zCard(&quot;zSetValue&quot;); System.out.println(&quot;通过zCard(K key)方法获取变量的长度:&quot; + zCard); 16.incrementScore(K key, V value, double delta)修改变量中的元素的分值。 double incrementScore = redisTemplate.opsForZSet().incrementScore(&quot;zSetValue&quot;,&quot;C&quot;,5); System.out.print(&quot;通过incrementScore(K key, V value, double delta)方法修改变量中的元素的分值:&quot; + incrementScore); score = redisTemplate.opsForZSet().score(&quot;zSetValue&quot;,&quot;C&quot;); System.out.print(&quot;,修改后获取元素的分值:&quot; + score); zSetValue = redisTemplate.opsForZSet().range(&quot;zSetValue&quot;,0,-1); System.out.println(&quot;，修改后排序的元素:&quot; + zSetValue); 17.reverseRange(K key, long start, long end)索引倒序排列指定区间元素。 zSetValue = redisTemplate.opsForZSet().reverseRange(&quot;zSetValue&quot;,0,-1); System.out.println(&quot;通过reverseRange(K key, long start, long end)方法倒序排列元素:&quot; + zSetValue); 18.reverseRangeByScore(K key, double min, double max)倒序排列指定分值区间元素。 zSetValue = redisTemplate.opsForZSet().reverseRangeByScore(&quot;zSetValue&quot;,1,5); System.out.println(&quot;通过reverseRangeByScore(K key, double min, double max)方法倒序排列指定分值区间元素:&quot; + zSetValue); 19.reverseRangeByScore(K key, double min, double max, long offset, long count)倒序排列从给定下标和给定长度分值区间元素。 zSetValue = redisTemplate.opsForZSet().reverseRangeByScore(&quot;zSetValue&quot;,1,5,1,2); System.out.println(&quot;通过reverseRangeByScore(K key, double min, double max, long offset, long count)方法倒序排列从给定下标和给定长度分值区间元素:&quot; + zSetValue); 20.reverseRangeByScoreWithScores(K key, double min, double max)倒序排序获取RedisZSetCommands.Tuples的分值区间值。 Set&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; typedTupleSet = redisTemplate.opsForZSet().reverseRangeByScoreWithScores(&quot;zSetValue&quot;,1,5); iterator = typedTupleSet.iterator(); while (iterator.hasNext()){ ZSetOperations.TypedTuple&lt;Object&gt; typedTuple = iterator.next(); Object value = typedTuple.getValue(); double score1 = typedTuple.getScore(); System.out.println(&quot;通过reverseRangeByScoreWithScores(K key, double min, double max)方法倒序排序获取RedisZSetCommands.Tuples的区间值:&quot; + value + &quot;----&gt;&quot; + score1 ); } 21.reverseRangeByScoreWithScores(K key, double min, double max, long offset, long count)倒序排序获取RedisZSetCommands.Tuples的从给定下标和给定长度分值区间值。 Set&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; typedTupleSet = redisTemplate.opsForZSet().reverseRangeByScoreWithScores(&quot;zSetValue&quot;,1,5,1,2); iterator = typedTupleSet.iterator(); while (iterator.hasNext()){ ZSetOperations.TypedTuple&lt;Object&gt; typedTuple = iterator.next(); Object value = typedTuple.getValue(); double score1 = typedTuple.getScore(); System.out.println(&quot;通过reverseRangeByScoreWithScores(K key, double min, double max, long offset, long count)方法倒序排序获取RedisZSetCommands.Tuples的从给定下标和给定长度区间值:&quot; + value + &quot;----&gt;&quot; + score1 ); } 22.reverseRangeWithScores(K key, long start, long end)索引倒序排列区间值。 Set&lt;ZSetOperations.TypedTuple&lt;Object&gt;&gt; typedTupleSet = redisTemplate.opsForZSet().reverseRangeWithScores(&quot;zSetValue&quot;,1,5); iterator = typedTupleSet.iterator(); while (iterator.hasNext()){ ZSetOperations.TypedTuple&lt;Object&gt; typedTuple = iterator.next(); Object value = typedTuple.getValue(); double score1 = typedTuple.getScore(); System.out.println(&quot;通过reverseRangeWithScores(K key, long start, long end)方法索引倒序排列区间值:&quot; + value + &quot;-----&gt;&quot; + score1); } 23.reverseRank(K key, Object o)获取倒序排列的索引值。 long reverseRank = redisTemplate.opsForZSet().reverseRank(&quot;zSetValue&quot;,&quot;B&quot;); System.out.println(&quot;通过reverseRank(K key, Object o)获取倒序排列的索引值:&quot; + reverseRank); 24.intersectAndStore(K key, K otherKey, K destKey)获取2个变量的交集存放到第3个变量里面。 redisTemplate.opsForZSet().intersectAndStore(&quot;zSetValue&quot;,&quot;typedTupleSet&quot;,&quot;intersectSet&quot;); zSetValue = redisTemplate.opsForZSet().range(&quot;intersectSet&quot;,0,-1); System.out.println(&quot;通过intersectAndStore(K key, K otherKey, K destKey)方法获取2个变量的交集存放到第3个变量里面:&quot; + zSetValue); 25.intersectAndStore(K key, Collection otherKeys, K destKey)获取多个变量的交集存放到第3个变量里面。 List list = new ArrayList(); list.add(&quot;typedTupleSet&quot;); redisTemplate.opsForZSet().intersectAndStore(&quot;zSetValue&quot;,list,&quot;intersectListSet&quot;); zSetValue = redisTemplate.opsForZSet().range(&quot;intersectListSet&quot;,0,-1); System.out.println(&quot;通过intersectAndStore(K key, Collection&lt;K&gt; otherKeys, K destKey)方法获取多个变量的交集存放到第3个变量里面:&quot; + zSetValue); 26.unionAndStore(K key, K otherKey, K destKey)获取2个变量的合集存放到第3个变量里面。 redisTemplate.opsForZSet().unionAndStore(&quot;zSetValue&quot;,&quot;typedTupleSet&quot;,&quot;unionSet&quot;); zSetValue = redisTemplate.opsForZSet().range(&quot;unionSet&quot;,0,-1); System.out.println(&quot;通过unionAndStore(K key, K otherKey, K destKey)方法获取2个变量的交集存放到第3个变量里面:&quot; + zSetValue); 27.unionAndStore(K key, Collection otherKeys, K destKey)获取多个变量的合集存放到第3个变量里面。 redisTemplate.opsForZSet().unionAndStore(&quot;zSetValue&quot;,list,&quot;unionListSet&quot;); zSetValue = redisTemplate.opsForZSet().range(&quot;unionListSet&quot;,0,-1); System.out.println(&quot;通过unionAndStore(K key, Collection&lt;K&gt; otherKeys, K destKey)方法获取多个变量的交集存放到第3个变量里面:&quot; + zSetValue); 28.remove(K key, Object… values)批量移除元素根据元素值。 long removeCount = redisTemplate.opsForZSet().remove(&quot;unionListSet&quot;,&quot;A&quot;,&quot;B&quot;); zSetValue = redisTemplate.opsForZSet().range(&quot;unionListSet&quot;,0,-1); System.out.print(&quot;通过remove(K key, Object... values)方法移除元素的个数:&quot; + removeCount); System.out.println(&quot;,移除后剩余的元素:&quot; + zSetValue); 29.removeRangeByScore(K key, double min, double max)根据分值移除区间元素。 removeCount = redisTemplate.opsForZSet().removeRangeByScore(&quot;unionListSet&quot;,3,5); zSetValue = redisTemplate.opsForZSet().range(&quot;unionListSet&quot;,0,-1); System.out.print(&quot;通过removeRangeByScore(K key, double min, double max)方法移除元素的个数:&quot; + removeCount); System.out.println(&quot;,移除后剩余的元素:&quot; + zSetValue); 30.removeRange(K key, long start, long end)根据索引值移除区间元素。 removeCount = redisTemplate.opsForZSet().removeRange(&quot;unionListSet&quot;,3,5); zSetValue = redisTemplate.opsForZSet().range(&quot;unionListSet&quot;,0,-1); System.out.print(&quot;通过removeRange(K key, long start, long end)方法移除元素的个数:&quot; + removeCount); System.out.println(&quot;,移除后剩余的元素:&quot; + zSetValue); RedisTemplate实践一参赛号的自增，利用自增获取参赛号的值，写入对象存库。 //参赛号为空，且起始值大于-1 if (StringUtils.isEmpty(enrollMatch.getSeq()) &amp;&amp; match.getSeqStart() &gt; -1) { //第一次参赛证生成 //判断redis里面是否有对应的比赛的key,如果没有， //执行redisTemplate.opsForHash().increment(&quot;match_seq&quot;, match.get_id(), match.getSeqStart()); //在起始参赛号自增1。 //redisTemplate.opsForHash().hasKey(&quot;match_seq&quot;, match.get_id()) if (!redisTemplate.opsForHash().hasKey(&quot;match_seq&quot;, match.get_id())) redisTemplate.opsForHash().increment(&quot;match_seq&quot;, match.get_id(), match.getSeqStart()); //非第一次 long num = redisTemplate.opsForHash().increment(&quot;match_seq&quot;, match.get_id(), 1); String numStr = &quot;&quot; + num; while (numStr.length() &lt; 5) // 凑够长度5 numStr = &quot;0&quot; + numStr; String seq = match.getSeqPrefix() + numStr; } 参赛证分配 数据结构：系统可以针对比赛设置赛区；赛区里设置考场，如第一考场，第二考场等；考场里设置有座位号，每个考场座位号从1开始，座位号个数可以系统设置。 需求：给每个参赛选手分配座位号。 实现过程： //查询所有需要分配考场座位的记录。根据比赛阶段（复赛，线下形式），已交报名费，已选赛点，未分配考场。 List&lt;MenrollPhase&gt; menrollPhaseList = mongoTemplate.find(Query.query(Criteria.where(&quot;phaseId&quot;).is(phaseId).and(&quot;state&quot;).is(2).and(&quot;matchPlaceId&quot;).ne(null).ne(&quot;&quot;).and(&quot;matchPlaceRoomId&quot;).in(null,&quot;&quot;)).with(new Sort(&quot;_id&quot;, &quot;ASC&quot;)), MenrollPhase.class); //查询比赛是否设置赛区、需要分配考场的记录不为空且size大于0 if(mphase.getMatchPlaceIds()!=null&amp;&amp;menrollPhaseList!=null&amp;&amp;menrollPhaseList.size()&gt;0){ lineNumber=menrollPhaseList.size();//要生成参赛证总数 //查询该比赛所有赛区 List&lt;MmatchPlace&gt; mmatchPlaceList = mongoTemplate.find(Query.query(Criteria.where(&quot;_id&quot;).in(mphase.getMatchPlaceIds())), MmatchPlace.class); if(mmatchPlaceList!=null&amp;&amp;mmatchPlaceList.size()&gt;0){ ListOperations&lt;String, Object&gt; lo = redisTemplate.opsForList(); //循环遍历赛区，将考场设置到赛区中 for(MmatchPlace mmatchPlace : mmatchPlaceList){ List&lt;MmatchPlaceRoom&gt; mmatchPlaceRoomList = mongoTemplate.find(Query.query(Criteria.where(&quot;placeId&quot;).in(mmatchPlace.get_id())).with(new Sort(&quot;order&quot;, &quot;ASC&quot;)), MmatchPlaceRoom.class); mmatchPlace.setMmatchPlaceRooms(mmatchPlaceRoomList); } //参赛证模板 String[] temp = mphase.getCertTpl().split(&quot;\\|&quot;); //文件名 String fileName = temp[2]; String quchu=null; //加锁，分布式环境下只能有一个线程去考场获取座位号 boolean groupAbsent = redisTemplate.opsForValue().setIfAbsent(&quot;placeRoom_&quot; + mphase.get_id(), &quot;roomsuo&quot;); //groupAbsent为true 可以执行当前代码 if(groupAbsent){ //遍历赛区 for(MmatchPlace mmatchPlace : mmatchPlaceList){ //比赛和赛区id共同组成key String mmatchPlaceIdKey = &quot;matchPlace_&quot; +mphase.get_id()+mmatchPlace.get_id(); //首次分配 if(!redisTemplate.hasKey(mmatchPlaceIdKey)&amp;&amp;mmatchPlace.getMmatchPlaceRooms()!=null&amp;&amp;mmatchPlace.getMmatchPlaceRooms().size()&gt;0){ //遍历赛区下的考场，考场id和座位号共同组成value，赛区为key,考场和座位为value中间以“，”隔开，存入 //redis。 for(MmatchPlaceRoom mmatchPlaceRoom : mmatchPlace.getMmatchPlaceRooms()){ for(int i=0;i&lt;mmatchPlaceRoom.getCounts();i++){ String placeValue = mmatchPlaceRoom.get_id()+&quot;,&quot;+(i+1); lo.leftPush(mmatchPlaceIdKey, placeValue);//存入redis } } } } checkStatus=1;//准备考场已经结束 //开始分考场 for(MenrollPhase menrollPhase : menrollPhaseList){ if(!StringUtils.isEmpty(menrollPhase.getMatchPlaceId())&amp;&amp;StringUtils.isEmpty(menrollPhase.getMatchPlaceRoomId())){ for(MmatchPlace mmatchPlace : mmatchPlaceList){ if(mmatchPlace.get_id().equals(menrollPhase.getMatchPlaceId())){ //取出分配好的座位 quchu = (String) lo.rightPop(&quot;matchPlace_&quot; +mphase.get_id()+mmatchPlace.get_id()); //redis里面没有说明已经取出了。 if(StringUtils.isEmpty(quchu)){ continue; } String[] tempNum = quchu.split(&quot;,&quot;); String roomId = tempNum[0];//考场Id String numBer = tempNum[1];//座位号 for(MmatchPlaceRoom mmatchPlaceRoom : mmatchPlace.getMmatchPlaceRooms()){ if(mmatchPlaceRoom.get_id().equals(roomId)){ //将取出的考场和座位号存入对象写入数据库 menrollPhase.setMatchPlaceRoomId(mmatchPlaceRoom.get_id()); menrollPhase.setMatchPlaceRoomNum(Integer.valueOf(numBer)); mongoTemplate.save(menrollPhase); menrollPhase.setMatchPlace(mmatchPlace); menrollPhase.setMatchPlaceRoom(mmatchPlaceRoom); } } } } } } } //删除锁 redisTemplate.delete(&quot;placeRoom_&quot; + mphase.get_id()); //开始生成参赛证 try{ caseCount=0; this.getDetails(menrollPhaseList); //方法一：使用Windows系统字体(TrueType) BaseFont baseFont = BaseFont.createFont(path+&quot;WEB-INF/template/SIMSUN.TTC,1&quot;,BaseFont.IDENTITY_H,BaseFont.NOT_EMBEDDED); for(MenrollPhase menrollPhase : menrollPhaseList){ if(!StringUtils.isEmpty(menrollPhase.getMatchPlaceId()) &amp;&amp;!StringUtils.isEmpty(menrollPhase.getMatchPlaceRoomId()) &amp;&amp;menrollPhase.getMatchPlaceRoomNum()!=0){ boolean isOk = checkedCarState(menrollPhase); if(!isOk){ continue; } PdfReader reader = new PdfReader(fileName); ByteArrayOutputStream bos = new ByteArrayOutputStream(); PdfStamper ps = new PdfStamper(reader, bos); AcroFields fields = ps.getAcroFields(); fields.addSubstitutionFont(baseFont); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); fillMap(map,menrollPhase); fillData(fields, map); ps.setFormFlattening(true); ps.close(); byte[] bytes = bos.toByteArray(); String base = Base64Util.encode(bytes).trim(); redisTemplate.opsForHash().put(&quot;cansai_&quot;+phaseId,menrollPhase.get_id(), base); caseCount++; } } contectStatus=1;//参赛证生成结束 }catch (IOException|DocumentException e) { e.printStackTrace(); } } } Redis分布式锁解决抢购问题先新建一个RedisLock类： public class RedisService { @Autowired private RedisTemplate stringRedisTemplate; /*** * 加锁 * @param key * @param value 当前时间+超时时间 * @return 锁住返回true */ public boolean lock(String key,String value){ if(stringRedisTemplate.opsForValue().setIfAbsent(key,value)){//setNX 返回boolean return true; } //如果锁超时 *** String currentValue = stringRedisTemplate.opsForValue().get(key); if(!StringUtils.isEmpty(currentValue) &amp;&amp; Long.parseLong(currentValue)&lt;System.currentTimeMillis()){ //获取上一个锁的时间 String oldvalue = stringRedisTemplate.opsForValue().getAndSet(key,value); if(!StringUtils.isEmpty(oldvalue)&amp;&amp;oldvalue.equals(currentValue)){ return true; } } return false; } /*** * 解锁 * @param key * @param value * @return */ public void unlock(String key,String value){ try { String currentValue = stringRedisTemplate.opsForValue().get(key); if(!StringUtils.isEmpty(currentValue)&amp;&amp;currentValue.equals(value)){ stringRedisTemplate.opsForValue().getOperations().delete(key); } } catch (Exception e) { log.error(&quot;解锁异常&quot;); } } } 首先，锁的value值是当前时间加上过期时间的时间戳，Long类型。首先看到用setiFAbsent方法也就是对应的SETNX，在没有线程获得锁的情况下可以直接拿到锁，并返回true也就是加锁，最后没有获得锁的线程会返回false。 最重要的是中间对于锁超时的处理，如果没有这段代码，当秒杀方法发生异常的时候，后续的线程都无法得到锁，也就陷入了一个死锁的情况。我们可以假设CurrentValue为A，并且在执行过程中抛出了异常，这时进入了两个value为B的线程来争夺这个锁，也就是走到了注释*的地方。currentValue==A，这时某一个线程执行到了getAndSet(key,value)函数(某一时刻一定只有一个线程执行这个方法，其他要等待)。这时oldvalue也就是之前的value等于A，在方法执行过后，oldvalue会被设置为当前的value也就是B。这时继续执行，由于oldValue==currentValue所以该线程获取到锁。而另一个线程获取的oldvalue是B，而currentValue是A，所以他就获取不到锁啦。 业务代码： private static final int TIMEOUT= 10*1000; @Transactional public void orderProductMockDiffUser(String productId){ long time = System.currentTimeMillions()+TIMEOUT; if(!redislock.lock(productId,String.valueOf(time)){ throw new SellException(101,&quot;换个姿势再试试&quot;) } //1.查库存 int stockNum = stock.get(productId); if(stocknum == 0){ throw new SellException(ProductStatusEnum.STOCK_EMPTY); //这里抛出的异常要是运行时异常，否则无法进行数据回滚，这也是spring中比较基础的 }else{ //2.下单 orders.put(KeyUtil.genUniqueKey(),productId);//生成随机用户id模拟高并发 sotckNum = stockNum-1; try{ Thread.sleep(100); } catch (InterruptedExcption e){ e.printStackTrace(); } stock.put(productId,stockNum); } redisLock.unlock(productId,String.valueOf(time)); }]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP概述和URL]]></title>
    <url>%2F2018%2F12%2F10%2Fhttps%2F</url>
    <content type="text"><![CDATA[引言HTTP协议，一个熟悉又陌生的应用协议。熟悉的是它在各种计算机网络教材中必然会被提及的应用层协议，稍微有点计算机知识的人基本上都会听过这个协议。它是互联网的基础，可以这么说，没有HTTP协议，就没有当前互联网的蓬勃发展。然而，笔者对它又是陌生的。因为虽然参与开发了不少Web应用，但很少直接与HTTP协议直接打交道，因为Web容器隐藏了很多HTTP协议的细节（譬如：请求协议头的解析，响应报文的生成），使得上层开发不用关心HTTP即可以开发出可用的Web应用。不应该仅仅满足可用，还应该让应用更加高效，这促使我不得不去研究HTTP协议。私以为，如果想进一步提升自己的技术能力，必须要深入到协议层级别，一些平时感觉到莫名其妙的问题就可以迎刃而解。 最近阅读了《HTTP权威指南》一书，本文包含书中的核心要点、个人延伸以及个人操作实践。由于本书是2009年出版，距现在已经有6年之久，期间，HTTP协议本身也不断地发展。所以，一些老旧的知识点将不会出现在本文中。希望通过不断阅读和实践，并且记录下来，能够加深对HTTP协议本身的理解！ HTTP协议概述本节介绍一些HTTP的基础知识，先对HTTP有一个宏观上的了解。 HTTP协议是什么平常老看到TCP/IP协议、FTP协议，XX协议……不禁要问，到底什么是协议？查阅了百度百科，上面是这么定义的： 基本解释：共同计议；协商法律范畴：协议是指两个或两个以上实体为了开展某项活动，经过协商后双方达成的一致意见。 两个要点： 1. 两个及两个以上参与者。也就是说，如果只有一方参与，根本就不会涉及到协议。 2. 协商一致。也就是说，所有参与方都必须同意并且遵守，才能使得活动能正常运行下去。 上面讲的一般意义上的协议，在计算机领域中，我们讲的协议一般是指通信协议，它仍然遵循上面的要点。首先，通信必然涉及到多方参与；其次，如果有一方不遵守协议，则根本没法进行有效通信。 下面来看看啥是HTTP协议，百度百科：超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。1960年美国人Ted Nelson构思了一种通过计算机处理文本信息的方法，并称之为超文本（hypertext）,这成为了HTTP超文本传输协议标准架构的发展根基。Ted Nelson组织协调万维网协会（World Wide Web Consortium）和互联网工程工作小组（Internet Engineering Task Force ）共同合作研究，最终发布了一系列的RFC，其中著名的RFC 2616定义了HTTP 1.1。 HTTP协议的特点下面从这篇博客中转过来的HTTP协议的特点，这篇文章对HTTP做了很全面的介绍，值得一读。 支持客户/服务器模式。支持基本认证和安全认证。 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。 灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 HTTP 0.9和1.0使用非持续连接（无连接性）：限制每次连接只处理一个请求，服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。HTTP 1.1使用持续连接：不必为每个web对象创建一个新的连接，一个连接可以传送多个对象。 无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大 特别直接说明的是HTTP的连接性。在我以前的认识里，HTTP最重要的特性是无连接性和无状态性。所谓无连接性，是指每一次请求都使用新的连接，请求完毕后连接关闭。这样做最大的好处时，最大程度上减少空闲连接占用服务端资源，这在系统资源比较昂贵、页面比较简单、仅传输静态页面的年代确实是非常合理的。但是，随着电商、视频等富媒体Web应用的兴起，HTTP的无连接性制约了系统的性能。一个Web应用动辄上百张图片，每一张图片都要占用一个网络连接。要知道，每新建一个连接都需要在TCP协议层进行“三次握手”，效率非常低下。随着在HTTP v1.1版本中默认采用Keep-Alive，多个请求可以使用同一个连接，HTTP的无连接性描述已经变得不准确了。 版本变化 HTTP/0.9 已过时。只接受 GET 一种请求方法，没有在通讯中指定版本号，且不支持请求头。由于该版本不支持 POST 方法，所以客户端无法向服务器传递太多信息。 HTTP/1.0 这是第一个在通讯中指定版本号的HTTP 协议版本，至今仍被广泛采用，特别是在代理服务器中。 HTTP/1.1 当前版本。持久连接被默认采用，并能很好地配合代理服务器工作。还支持以管道方式同时发送多个请求，以便降低线路负载，提高传输速度。 URL与资源什么是URLURL（Uniform Resource Location, 统一资源定位符）是Internet上的所有资源的标准化名称。可以把Internet看做一个巨大的正在扩张的城市，里面充满了各种可看的东西，可做的事情。我们需要为这个城市里面的所有景点和服务起一个名字，所有的名字必须在符合统一的标准，这样才能使得我们方便地使用这座城市的宝藏。URL就是其中一类重要的资源命名方式。URL指向每一条电子信息，告诉你它位于何处，如何与之进行交互。URL必须是唯一的，也就是说，一个URL只能对应唯一的资源。 URI、URL和URN说起URL，就必然要提URI和URN。那么它们之间到底有什么联系和区别呢？首先来看看URI和URN是什么。 URI：即Uniform Resource Identifier，统一资源标识符。它是一个通用的概念，理论上，能保证资源全局唯一性的标识符都可以叫做URI； URN：即Uniform Resource Name,统一资源名称。这样的资源名是与资源具体的位置无关的。 URI、URL、URN是相互关联的。URL和URN都是URI的子集，按照集合论的观点，它们之间的关系见下图。 也就说，任何的URL都可以是URI，反之不然。URL是与资源所处的位置密切相关的，如果资源挪动位置，则必然导致URL跟着一起变化。想象一下，如果资源换了位置（虽然这极少发生），则原来开发的软件就会失效。URN主要就是为了解决这个问题而提出来的。它通过给资源命名而不是定位来唯一地确定资源。 假定现在要给我起个独一无二的名称，我可以自己命名为此时我所处的地理位置,假设是(东经36度，北纬36度)。好了，现在请叫我(东经36度，北纬36度)，通过地图肯定可以找到我。然而，我并不是时时刻刻都呆着这里的，因为我是个活人。当我移动位置之后，通过原来的位置定位到的已然不是我了，这就很麻烦了。如果用我的姓名来找我（假设是独一无二的），则无论何时何地找到的都是我了。 目前来说，URN貌似还没能到实用阶段，看上去用处也不大。一则资源位置一般不会发生变动；二则URL已经完全普及，为啥要抛弃已经用得溜溜的东西呢？！至少目前还没有理由这么干。所以，在《HTTP权威指南》一书中，并没有将URI和URL区分开来，所有的URI都可以看做是URL。 URL的完整语法在Web应用中，URL通常是由3各部分构成，以URL地址http://www.joes.com/seanonal/index.html为例来进行说明。 第一部分(http)是方案（schema），可以告知客户端怎样访问资源。 第二部分(www.joes.com)是服务器的位置，告知客户端资源位于何处。 第三部分（/seanonal/index.html）是资源路径。 上面的URL地址只是众多格式的URL地址中的一种，实际上，URL还可以通过HTTP以外的其他协议来进行访问。比如个人E-mail账户： ikangbow@outlook.com 或者是通过FTP协议获取文件： ftp://ftp.xxx.com/file.xls 咋一看，这些URL的格式都不太一样，这是不是意味着每种不同的URL方案会有完全不同的语法呢？其实不然。大多数URL语法都建立在9个部分构成的通用格式上： schema://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;path&gt;;&lt;params&gt;?&lt;query&gt;#&lt;frag&gt;&lt;/pre&gt; 每一部分的的意义如下： schema：获取资源时使用何种协议 user：访问资源时需要的用户名 password：访问资源时与用户名配套的密码 host：资源所在的服务器地址 port：服务器所监听的端口 path：服务器上的本地资源路径，用/分隔 params：指定输入参数，用key/value表示 query：资源查询参数 frag：资源内部的片段名字 URL通常会由其中的某些部分组合而成，下面是一些URL典型示例： http://www.joes.com/seanonal/index.html file:///D:/relativeUrl.html ftp://username:password@ftp.xxx.com/file.xls http://www.joes.com/seanonal/index.html;type=d http://www.joes.com/seanonal/goods.html?item=45454 http://www.joes.com/seanonal/goods.html?item=45454#name 相对URL 绝对URL和相对URL是URL的两种不同的表现形式，前面所有的URL示例都是绝对URL，绝对URL包含访问资源所需的全部资源。下面是一个简单的HTML页面代码，其中的page1就是一个包含相对URL的链接。 相对URL是不完整的，要获取资源的全部信息，还要依赖称为基础（base）的URL。基础URL通常来自以下地方： 第一，在资源中显示提供。在HTML文档中，可以由标签定义一个基础URL。下面的代码定义了&lt;base href=”http://www.demo.com/base/“ &gt; ，于是，page2实际的绝对地址就是：http://www.demo.com/base/page2.html。 第二，所属资源的URL作为基础URL。还是以上面的代码为例。假定page1.html的绝对URL是http://www.demo.com/page1.html，那么page1.html属于http://www.demo.com/资源下的。直接用这个地址作为基础URL，则page2的绝对URL是：http://www.demo.com/page2.html。 第三，没有基础URL。注：这里没有看懂，先列在这里…… #URL编码 合格的URL应该满足下面的要求： 可移植性：作为统一的命名，应该要能够通过不同的协议来传送资源。不同的协议可能会有特定的保留字符，在不同的协议中传输时，不应该因为这些特殊字符而丢失信息。 可读性：不可见的、不可打印的（比如空格）字符不应该出现在URL中。 完整性：可能需要通用字符外的二进制数据或字符，因此需要一种转义机制，将不安全的字符编码为安全字符。 从历史来看，计算机应用程序都是用US-ASCII字符集^footnote。由于其历史悠久，所以可移植性很好。但是它不支持数百种非罗马语言中的字符。这就需要一套转义编码机制，用US-ASCII字符集来对任意字符进行编码。目前设计的转义表示法是用一个“%”，后面跟着两个表示ASCII码的十六进制数。下面是一些编码示例： 从历史来看，计算机应用程序都是用US-ASCII字符集^footnote。由于其历史悠久，所以可移植性很好。但是它不支持数百种非罗马语言中的字符。这就需要一套转义编码机制，用US-ASCII字符集来对任意字符进行编码。目前设计的转义表示法是用一个“%”，后面跟着两个表示ASCII码的十六进制数。下面是一些编码示例： #总结本文是《HTTP权威指南》学习笔记的第一篇，介绍一些HTTP中的基本概念和概述。重点介绍了URL（统一资源定位符）这一种最重要的Web资源命名方式，将其与URI、URN这类经常混淆的概念进行的比较。概要说明了URL的语法格式、相对URL和URL编码。首先对HTTP有一个整体上的认识，接下来要写的是HTTP中的重要细节内容，与日常的开发密切相关。]]></content>
      <categories>
        <category>https</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
</search>
